{
  "timestamp": "2025-12-05T16:49:24.829536",
  "query": "machine learning for document classification",
  "retrieval": {
    "num_documents": 3,
    "documents": [
      {
        "text": "credential handling errors •C21:Misuse or incorrect assumptions about external APIs •C22:Mishandling of complex data structures and relationships •C23:Exception handling and reporting deficiencies •C29:Overconfident but incorrect assertions •C30:Limited breadth in exploration or search A.2 DISTILLEDCLASSIFIERRESULTS Since the performance of the distilled classifier determinantly impacts the quality and consistency of the constructed graph, we also carry out extensive experiments to improve its performance. We attempted to fine-tune various models with different approaches, and record all results in Table 2. From the results, the general observation is that: (1) doing global fine-tuning results in similar performance as doing LoRA fine-tuning; and (2) Using a larger model significantly improves the performance. Our Wikipedia training corpus consist of 350k code pairs generated from English Wikiepdia, using the same embedding plus taking top30%highest cosine similarities pipeline as inLOGOS. On this corpus, we observe a serious label imbalance issue. The pairs labeled “mutual” account for only 13 approximately10%of those with other labels. Therefore, we also attempted the label smoothing technique and the oversampling-based balancing technique. Results show that the balancing tech- nique is effective in our label-imbalance case. Model Acc F1 Macro F1 Micro Balanced Acc MiniLM-L12 (33M) 0.536 0.507 0.536 0.626 Roberta-focal-loss 0.722 0.654 0.722 0.734 Modern-Bert-full 0.723 0.665 0.723 0.694 Roberta-large-full 0.740 0.674 0.740 0.735 Roberta-MNLI-full 0.730 0.664 0.730 0.733 Roberta-large-LoRA 0.719 0.649 0.728 0.728 Qwen3-0.6B 0.766 0.711 0.766 0.715 Qwen3-4B-it 0.803 0.747 0.802 0.742 Qwen3-4B-base 0.804 0.745 0.804 0.759 Qwen3-4B-balanced 0.814 0.815 0.814 0.812 Table 2: Model performance comparison across Accuracy, F1 Macro, F1 Micro, and Balanced Accuracy. All models are trained on a single A100 GPU with using theHuggingFace AutoModelforClassificationframework with the following hyperparameters: Label smoothing was applied with a factor of0.1. Whenever applicable, focal loss gamma was set to2.0. A weighted sampler was used for class balancing. Training was performed for10epochs with a learning rate of2×10−4, per-device training batch size of256, and per-device evaluation batch size of512. The LoRA configuration employed a rankr= 16,α= 32, dropout rate of0.05, bias set tonone, and targeted modules includedq proj,k proj,v proj, ando proj, withscore modules preserved. A.3 CODEFREQUENCIES PERDATASET ANDQUESTION A.3.1 PAPERABSTRACTSDATASETQUESTIONS ANDTOP25 FREQUENTCODES: •Q1:What are the problem framings and research gaps identified within the UIST abstracts? This question aims to identify the common ways that authors in the UIST community frame their research. It explores the recurring themes and narratives used to establish the signif- icance of their work, such as addressing technological limitations, filling gaps in existing research, or enabling new forms of interaction. By looking at the collection of abstracts, you can identify the shared understandings of what constitutes a ”problem” in this research community. Bridging physical-digital. . .Enabling novel interactio. . . User-centered evaluation . . .Development of novel inpu. . . Enabling seamless connect. . .Empirical validation of i. . .User study validation of . . . User study validation of . . . Innovating novel interact. . .Enhancing user experience. . . Enabling dynamic spatial . . . Bridging physical and dig. . . Overcoming traditional in. . . Facilitating seamless phy. . .Enabling real-time feedba. . . Bridging physical-digital. .",
        "source": "2509.24294v1.pdf",
        "chunk_index": 15,
        "distance": 0.6105550527572632,
        "similarity_score": 0.3894449472427368
      },
      {
        "text": "[24]. Although it does effectively minimize irrelevant con- textual features, its stringent filtering process can result in over-filtering for some query-document pairs [9]. This indicates that an adjustment to this threshold or the application of a domain-based cross-encoder would yield more relevant verified docu- ments without any sacrifice to the verification principle [2]. –Beyond document-level relevance, a key area for future development is assurance of collective coherence and direct relevance of the returned context [12]. This could come in the form of an additional post-prioritization filtering step or more sophisticated coherence scoring to ensure that the overall context block exposed to the LLM is well- aligned to the query intent and has little chance of including tangentially related or distracting content [10] [11] [6]. – BM25-like Proxy Limitations: Our rudimentary keyword-overlap-based BM25-like implementation (using BM25Okapi), although sufficient for proving the fallback mech- anism, does not have the maturity and ranking capability of an actual BM25 imple- mentation (e.g., full BM25Okapi) [8] [9]. This can produce the retrieval of keyword- matching but semantically distant documents during fallback, which are subsequently handed over to the LLM [8] [9]. Adapting a more sophisticated BM25 would further optimize this phase [9]. These findings do not diminish the architectural significance of MeVe but rather high- light the requirement for ideally tunable modules and an appropriate quality knowledge repository in order to attain high answer accuracy and hallucination resistance levels in practical applications [16]. The minimal overhead of latency attained (1.22 seconds on average for MeVe and 1.12 seconds for Standard RAG) is justified given the efficiency and controllability gains inherent in the framework. 7 Conclusion We introduced MeVe, a modular architectural framework for memory verification and context management in LLMs [4] [19]. By decomposing the monolithic RAG pipeline into tunable, specialized modules, MeVe provides fine-grained control over context quality and efficiency [2] [17]. Our empirical results on a Wikipedia subset as a proof-of-concept empir- ically validate this approach via appreciable improvements in context efficiency compared to baseline retrieval methods [21]. Ultimately, MeVe is not just a complement to RAG but a reinvention of memory interaction in LLM systems [1] [2]. While RAG considers retrieval a monolithic operation, MeVe considers it an evolving and modular process with distinct phases for verification, fallback, and budget management [4] [19]. Separation of concerns allows for more efficiency, manageability, and comprehensibility in memory utilization, a capability that is essential to the construction of future agentic systems and long-context applications [5] [6]. We have demonstrated that the reliability of future AI systems hinges not only on the expansion of context windows but also on the design of intelligent and auditable memory control structures [7] [14]. MeVe offers a systematic and validated solution to this challenge. A Appendix A: Methodological Specifications This appendix provides detailed information pertaining to the hardware and software con- figuration, model numbers, dataset, and hyperparameters utilized in the empirical analysis of the MeVe framework. A.1 Technical Framework and Software Infrastructure The tests were conducted on a system featuring a NVIDIA GeForce RTX 3060 GPU (6 GB GDDR6 VRAM). CPU specifications",
        "source": "2509.01514v1.pdf",
        "chunk_index": 10,
        "distance": 0.6554869413375854,
        "similarity_score": 0.34451305866241455
      },
      {
        "text": "70%) and testing ( 30%) subsets, and finetune BERT-level models, includ- ing BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019). In addition, to investigate how pow- erful embeddings help the task, we try stronger embedding models, Gemini, and utilize its embed- dings to train SVM and MLP for the classification. Based on Gemini embedding, we also try KNN method to see how it performs. As shown in Table 4, GPT-4.1 obtains the best results, with the highestModel Accuracy Cohen’sκ GPT-4.1 (Ex+Guide) 0.805 0.764 BERT0.732 0.671 RoBERTa0.730 0.670 SVM-Gemini0.704 0.632 MLP-Gemini0.684 0.613 KNN-Gemini0.587 0.490 Table 4: Overall sentence-level accuracy and Cohen’s κ for each model on the 30% test subset. GPT-4.1 obtains the best results, while for training-based methods, the BERT model obtains the best performance. accuracy ( 0.805 ) and the highest κ(0.764 ). The detailed confusion matrices on sentence-level ac- curacy on both GPT-4.1 and BERT are shown in Table 3. According to the confusion matrices, we can find the highest values aside from the diagonal areAnalyze-Verify,Implement-Verify, andVerify- Implement, which represent scenarios where even the best model can not perform well. 5 Conclusion In this work, we bridged a gap between cognitive science and artificial intelligence by using Schoen- feld’s Episode Theory to analyze the reasoning of Large Reasoning Models. We demonstrated that a framework designed for human problem-solving can effectively decode machine-generated thought processes, revealing a structured, episodic nature in how LRMs tackle mathematical challenges. The core of our contribution is a novel, large-scale an- notated corpus and a reusable analytical protocol, which we have made publicly available to foster further research. This research not only offers ini- tial insights into the thinking patterns of current models but also establishes a foundational method- ology for future investigations. Limitations The main limitation of this version of the paper is the scope. Currently, only SAT math data is taken into account and labeled. To enhance the dataset’s size and complexity, future iterations should incor- porate data from additional sources. For example, since the SAT is designed as a college admission test in the U.S., the overall difficulty level is rel- atively moderate. As a next step, we plan to in- clude other datasets, which are derived from math- ematical Olympiad competitions and features more challenging items. Besides, the accuracy for auto- matic annotation is not very high, further efforts are needed to improve the accuracy. References Hyung Won Chung, Le Hou, S. Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Web- son, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Dasha Val- ter, Sharan Narang, Gaurav Mishra, and 13 others. 2022. Scaling instruction-finetuned language models. ArXiv, abs/2210.11416. Ellie Darlington. 2013. The use of bloom’s taxonomy in advanced mathematics questions. In Proceedings oftheBritish Society forResearch into Learning Mathematics, volume 33, pages 7–12. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, and etc. 2025. Deepseek-r1: Incentiviz- ing reasoning capability in llms via reinforcement",
        "source": "2509.14662v1.pdf",
        "chunk_index": 6,
        "distance": 0.6557208299636841,
        "similarity_score": 0.3442791700363159
      }
    ],
    "statistics": {
      "num_retrieved": 3,
      "num_unique_sources": 3,
      "avg_similarity_score": 0.3594123919804891,
      "retrieval_time_ms": 200.0880241394043
    }
  },
  "explanation": "\n[Paper 1 - 2509.24294v1.pdf]\nRelevance: Moderately Relevant\nExplanation: Although this paper does not directly focus on machine learning for document classification, it discusses the importance of improving a distilled classifier's performance to construct high-quality and consistent graphs. This relates to the research question since machine learning techniques are essential in developing such classifiers.\nKey Concepts: distilled classifier, graph construction, performance improvement\n\n[Paper 2 - 2509.01514v1.pdf]\nRelevance: Moderately Relevant\nExplanation: This paper examines a method to minimize irrelevant contextual features in document classification while addressing the risk of over-filtering some query-document pairs. The study suggests adjusting thresholds or applying domain-based cross-encoders, which are relevant techniques for improving machine learning models used in document classification tasks.\nKey Concepts: document-level relevance, filtering, contextual features, threshold adjustment, domain-based cross-encoder\n\n[Paper 3 - 2509.14662v1.pdf]\nRelevance: Highly Relevant\nExplanation: This paper investigates the application of various machine learning models and techniques for document classification. The authors compare different pre-trained language models, fine-tuning methods, and embedding models to improve classification accuracy. These are central topics in the research question, making this paper directly relevant.\nKey Concepts: machine learning models, pre-trained language models, fine-tuning, embedding models, classification accuracy"
}