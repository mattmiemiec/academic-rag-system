{
  "filename": "2509.23661v1.pdf",
  "total_chunks": 14,
  "text_length": 50984,
  "chunks": [
    "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training LLaVA-OneVision Community Contributors Abstract We present LLaVA-OneVision-1.5, a novel family of Large Multimodal Models (LMMs) that achieve state-of-the-art performance with significantly reduced computational and financial costs. Different from the existing works, LLaVA-OneVision-1.5 provides an open, efficient, and reproducible framework for building high-quality vision-language models entirely from scratch. The LLaVA-OneVision-1.5 release comprises three primary components:(1) Large-Scale Curated Datasets:We construct an 85M concept-balanced pretraining dataset LLaVA-OneVision-1.5-Mid- Traning and a meticulously curated 22M instruction dataset LLaVA-OneVision-1.5-Instruct.(2) Efficient Training Framework:We develop a complete end-to-end efficient training framework leveraginganofflineparalleldatapackingstrategytofacilitatethetrainingofLLaVA-OneVision-1.5 within a$16,000budget.(3) State-of-the-art Performance:Experimental results demonstrate that LLaVA-OneVision-1.5 yields exceptionally competitive performance across a broad range of downstream tasks. Specifically, LLaVA-OneVision-1.5-8B outperforms Qwen2.5-VL-7B on 18 of 27 benchmarks, and LLaVA-OneVision-1.5-4B surpasses Qwen2.5-VL-3B on all 27 benchmarks. We anticipate releasing LLaVA-OneVision-1.5-RL shortly and encourage the community to await further updates. Codehttps:/github.com/EvolvingLMMs-Lab/LLaVA-OneVision-1.5 Modelshttps://huggingface.co/lmms-lab/LLaVA-OneVision-1.5-8B-Instruct Pretrain datahttps://huggingface.co/datasets/lmms-lab/LLaVA-One-Vision-1.5-Mid-Training-85M Instruct datahttps://huggingface.co/datasets/lmms-lab/LLaVA-OneVision-1.5-Insturct-Data Figure 1Performance of LLava-OV-1.5-8B across multiple benchmarks.arXiv:2509.23661v1 [cs.CV] 28 Sep 2025 1 Introduction Recent advancements in Large Multimodal Models (LMMs) have demonstrated remarkable ca- pabilities in multimodal understanding and reasoning (Comanici et al., 2025; Guo et al., 2025; Zhu et al., 2025). These developments enable artificial intelligence applications to effectively comprehend and analyze images, charts, and PDF documents. However, the most performant modelsremainproprietary, withneithertheirtrainingdatanorsourcecodepubliclyavailable. Con- sequently, the broader research community lacks crucial insights into how such high-performing LMMs can be built from scratch. To reduce barriers for community development, several research efforts have attempted to re- produce the capabilities of proprietary models using open architectures. Early efforts such as LLaVA (Liu et al., 2023), LLaVA-Next (Liu et al., 2024b), and LLaVA-OneVision (Li et al., 2025a) provided fully open training data and code, but their performance now falls substantially behind that of current state-of-the-art models (Bai et al., 2025; Zhu et al., 2025). More recent works have pushed the boundary: Molmo (Deitke et al., 2025) released model weights, datasets, and source code, enabling the community to train LMMs from scratch. Through careful architectural choices, a refined training pipeline, and high-quality data, Molmo achieves near-parity with GPT- 4V on both academic benchmarks and user preference evaluations. Open-Qwen2VL (Wang et al., 2025) introduces a 2B-parameter model pre-trained on only 0.36% of 1.4T multimodal tokens in Qwen2-VL, while outperforming Qwen2-VL-2B across various multimodal benchmarks. Despite these advances, the performance gap between open-source and proprietary models continues to widen as the field of LMMs rapidly evolves. Current open-source models are still constrained by substantial computational demands and suboptimal training efficiency. To overcome the aforementioned limitations, we introduce LLaVA-OneVision-1.5, a fully open- source family of LMMs, extending the LLaVA series (Li et al., 2025a) to achieve superior perfor- mance with limited computational cost. Specifically, LLaVA-OneVision-1.5 adopts RICE-ViT (Xie et al., 2025) as the vision encoder, enabling native-resolution adaptation and fine-grained visual understanding based on stronger region-level semantic representation. Building upon LLaVA- OneVision (Li et al., 2025a), LLaVA-OneVision-1.5 adopts a three-stage training pipeline: (Stage-1) Language-Image Alignment, (Stage-1.5) High-Quality Knowledge Learning, and (Stage-2) Visual Instruction Tuning. Notably, we find that simply scaling data at the mid-training stage alone can produce state-of-the-art LMMs, eliminating",
    "understanding based on stronger region-level semantic representation. Building upon LLaVA- OneVision (Li et al., 2025a), LLaVA-OneVision-1.5 adopts a three-stage training pipeline: (Stage-1) Language-Image Alignment, (Stage-1.5) High-Quality Knowledge Learning, and (Stage-2) Visual Instruction Tuning. Notably, we find that simply scaling data at the mid-training stage alone can produce state-of-the-art LMMs, eliminating the need for complex training paradigms. To foster open research, we release all assets to the community, including LLaVA-OneVision-1.5- Mid-Traning and LLaVA-OneVision-1.5-Instruct datasets, the training framework, and model checkpoints (LLaVA-OneVision-1.5-Base and LLaVA-OneVision-1.5-Instruct). In summary, our contributions are as follows: ‚Ä¢LargeMultimodalModels.WeproposeLLaVA-OneVision-1.5 afamilyoffullyopen-sourcelarge multimodal models that achieve superior performance across multiple multimodal benchmarks compared to Qwen2.5-VL. ‚Ä¢Large-Scale Datasets.We construct an 85M concept-balanced pre-training dataset LLaVA- OneVision-1.5-Mid-Traningandameticulouslycurated22MinstructiondatasetLLaVA-OneVision- 1.5-Instruct. ‚Ä¢Efficienct Training Framework.We develop a complete end-to-end training framework that employs an offline parallel data packing strategy to optimize cost-effectiveness, enabling the training of LLaVA-OneVision-1.5 within a$16,000compute budget. ‚Ä¢Open-Source Release.We release all assets to the public including LLaVA-OneVision-1.5- 2 Figure 2Overall architecture of LLaVA-OneVision-1.5. The framework integrates a pre-trained vision encoder with a language model decoder. The vision encoder adopts 2D RoPE for native- resolutionprocessingandincorporatesregion-awareattentiontoenhancelocalsemanticmodeling. During pretraining, both object regions and OCR regions are jointly modeled to inject fine- grained text understanding capability. A lightweight projector maps visual features into the LLM embedding space, and the [CLS]token is preserved to retain global semantic capacity during multimodal alignment. Mid-Traning and LLaVA-OneVision-1.5-Instruct dataset, training framework, and the model checkpoints (LLaVA-OneVision-1.5-Base and LLaVA-OneVision-1.5-Instruct). 2 Architecture 2.1 Overall Architecture The overall architecture of LLaVA-OneVision-1.5 is illustrated in Fig. 2. LLaVA-OneVision-1.5 retains the ‚ÄúViT‚ÄìMLP‚ÄìLLM‚Äù paradigm of the LLaVA series, comprising three core modules: ‚Ä¢Vision Encoder: The vision encoder is responsible for extracting rich and semantically meaningful visual representations from input images, which serve as the foundation for multimodal alignment and downstream reasoning. Unlike previous works (Wang et al., 2024b; Bai et al., 2025) that adopt SigLIP (Zhai et al., 2023) or DFN (Fang et al., 2023), LLaVA-OneVision-1.5 integrates our recently proposed cluster discrimination model RICE- ViT (Xie et al., 2025) to improve region-aware visual and OCR capabilities. ‚Ä¢Projector: The projector bridges the modality gap between the vision encoder and the large language model by mapping visual embeddings into the text embedding space of the LLM. Following Qwen2.5-VL (Bai et al., 2025), we first group spatially adjacent sets of four patch features, which are then concatenated and passed through a two-layer multi-layer perceptron to map them into the text embedding space of the LLM. 3 ‚Ä¢Large Language Model: The large language model acts as the reasoning and generation core of the architecture. After receiving the projected multimodal embeddings, the LLM integrates visual information with linguistic context to perform complex reasoning, instruc- tion following, and natural language generation. The LLaVA-OneVision-1.5 series utilize Qwen3 (Team, 2025) as the language backbone to significantly enhance performance on downstream tasks. This modular design follows the LLaVA framework but incorporates more efficient training recipes and carefully selected encoders, enabling superior cost-effectiveness and scalability. 2.2 Vision Encoder via Region-Aware Cluster Discrimination Fine-grained visual semantics are essential for dense prediction tasks such as grounding, OCR, and segmentation. Although large-scale vision‚Äìlanguage contrastive",
    "downstream tasks. This modular design follows the LLaVA framework but incorporates more efficient training recipes and carefully selected encoders, enabling superior cost-effectiveness and scalability. 2.2 Vision Encoder via Region-Aware Cluster Discrimination Fine-grained visual semantics are essential for dense prediction tasks such as grounding, OCR, and segmentation. Although large-scale vision‚Äìlanguage contrastive models like CLIP (Radford et al., 2021) and SigLIP (Zhai et al., 2023) demonstrate strong performance through global vision- language alignment, they fail to capture the similarity structure of training data or the local region- level semantics within images. This shortcoming stems from instance-wise contrastive learning, which treats all instances as negatives regardless of their semantic similarity and represents each instance solely with a single global embedding. Toovercometheselimitations,ourLLaVA-OneVision-1.5leveragestheRICE-ViT(Xieetal.,2025)as itsvisionencoder, enablingprecisemultimodalalignmentandenrichedregion-levelrepresentation. RICE-ViT enhances both object-centric and OCR capabilities by introducing a unified region cluster discrimination loss, trained on 450M images and 2.4B candidate regions. Its design combines a region-awareattentionmechanismforlocalsemanticmodelingwith2Drotarypositionalencoding, which naturally supports variable input resolutions without requiring resolution-specific fine- tuning, unlike models such as Qwen2-VL (Wang et al., 2024b) and InternVL 2.5 (Chen et al., 2024b). We integrate this pretrained encoder with a language model through joint training, yielding a streamlined multimodal pipeline. Compared to SigLIP2 (Tschannen et al., 2025), which depends on multiple specialized losses (SILC, TIPS, LocCa, and Sigmoid), our method adopts a single cluster discrimination loss that simultaneously strengthens general understanding, OCR, and localization. This unified formulation provides an elegant, computationally efficient solution that matches SigLIP2‚Äôs performance while substantially reducing architectural complexity and training overhead. 3 Data 3.1 Pre-Training Dataset We use the LLaVA-1.5 558K (Liu et al., 2024a) to align the visual features into the word embedding spaceofLLMs. Afterthat, LLaVA-OneVision-1.5isunderpinnedbyalarge-scalemultimodaldataset LLaVA-OneVision-1.5-Mid-Traning, which contains 85 million high-quality image-text pairs (20M in Chinese and 65M in English). The data of LLaVA-OneVision-1.5-Mid-Traning are from a wide range of sources: COYO-700M (Byeon et al., 2022), Obelics (Lauren√ßon et al., 2023), DataComp- 1B (Gadre et al., 2023), LAION-CN (Zhang et al., 2022), ImageNet-21K (Russakovsky et al., 2015), SAM-1B (Kirillov et al., 2023), MINT (Wang et al., 2024c), and Zero250M (Xie et al., 2023). To enrich the diversity of our pretraining data, we introduce a concept-balanced sampling strategy inspired by MetaCLIP (Xu et al., 2024). Unlike MetaCLIP, which depends on raw captions for concept matching and struggles with caption-free or interleaved datasets (e.g., SAM-1B, ImageNet- 4 (a) (b) (c) Figure 3(a) The vocabulary coverage proportion in the LLaVA-OneVision-1.5-Mid-Traning dataset before and after concept balancing. (b) Distribution of data sources within the LLaVA-OneVision- 1.5-Mid-Traning dataset. (c) Distribution of data sources within the LLaVA-OneVision-1.5-Instruct. 21K,andObelics), ourmethodreducesrelianceoncaptionquality, suchasthebriefandincomplete annotations common in COYO-700M. Instead, we adopt a feature-based matching approach that coarsely groups image sources. Specifically, using the pretrained MetaCLIP-H/14-Full-CC2.5B encoders (Xu et al., 2024), we project both images and MetaCLIP‚Äôs 500K concept entries into a shared embedding space. Since MetaCLIP embeddings are already concept-balanced, this enables effective similarity-based concept induction: for each image, we retrieve its top-K nearest concept embeddings to construct refined pseudo-captions that enhance semantic alignment. Top-K Concept Assignment and Balance Sampling.Given an image set ‚Ñê={ùëñ 0, ùëñ1, ..., ùëñ ùëÅ}and a concept vocabulary set ùí±={ùë£",
    "Since MetaCLIP embeddings are already concept-balanced, this enables effective similarity-based concept induction: for each image, we retrieve its top-K nearest concept embeddings to construct refined pseudo-captions that enhance semantic alignment. Top-K Concept Assignment and Balance Sampling.Given an image set ‚Ñê={ùëñ 0, ùëñ1, ..., ùëñ ùëÅ}and a concept vocabulary set ùí±={ùë£ 0, ùë£1, ...ùë£ ùëÄ}, we first utilize the image encoder Œ¶ùë£and text encoder Œ¶ùë°to extract the image embeddings ‚Ñ∞ùëñ={Œ¶ ùë£(Ô∏Äùëñ)Ô∏Ä, ùëñ‚ààùêº} and concept embeddings ‚Ñ∞ùë°={Œ¶ ùë°(Ô∏Äùë£)Ô∏Ä, ùë£‚ààùëâ} . Then we assign each image with the top- ùëònearest concepts based on the cosine similarity of the L2-normalized image and concept embeddings. After that, following MetaCLIP (Xu et al., 2024), we weight each image by the inverse frequencies of its concepts and then sample images based on the normalized image weights. This inverse-frequency methodology promotes a more balanced concept distribution, without relying on original captions, which may be noisy or missing. This process yields 85M images with balanced concepts. Subsequently, we apply a powerful captioner to produce English and Chinese captions for these images, followed by a validity filter to eliminate duplicates and excessively lengthy outputs. Ultimately, we establish an 85M concept-balanced mid-training dataset. The distribution of data sources of the LLaVA- OneVision-1.5-Mid-Traning dataset is illustrated in Fig. 3(b). 3.2 Instruction Dataset Visual instruction tuning (Liu et al., 2023) is vital for enabling LMMs to understand and follow visual instructions, and its effectiveness hinges on the quality of the instruction datasets. To this end, we construct the LLaVA-OneVision-1.5-Instruct dataset by aggregating a wide range of instruction-tuning datasets from diverse sources. The data are carefully curated to ensure balanced coverage across seven categories: Caption, Chart & Table, Code & Math, Domain-specific, General VQA, Grounding & Counting, OCR, and Science. The resulting corpus comprises 22 million samples, with Fig. 3(c) showing the proportional distribution across categories. 5 Table 1Performance comparison across vision-language models on various benchmarks grouped by task type. All scores are reported as accuracy percentages unless otherwise specified. BenchmarkLLaVA-OV-1.5 8BQwen2.5-VL 7BLLaVA-OV-1.5 4BQwen2.5-VL 3BLLaVA-OV 7B General VQAMMStar 67.762.5 64.955.9 61.7 MMBench en 84.183.4 84.278.0 82.5 MMBench cn 81.081.6 76.974.6 81.4 MME-RealWorld en 62.357.3 61.651.6 57.4 MME-RealWorld cn 56.151.5 49.645.4 54.0 SeedBench image 77.377.5 76.674.8 75.4 CV-Bench 80.880.0 77.271.5 77.9 ScienceQA 95.088.8 93.683.3 90.0 SEED-Bench-2-Plus 69.270.9 68.968.6 64.9 RealWorldQA 68.168.5 67.860.0 66.3 Avg. 74.272.2 72.166.4 71.1 ReasoningMathVista mini 69.668.6 67.960.2 58.5 WeMath 33.633.3 24.918.4 20.9 MathVision 25.622.4 24.221.3 18.5 MMMU val 55.451.3 52.746.4 48.8 MMMU-Pro standard 37.436.3 35.331.1 28.0 MMMU-Pro vision 25.232.8 25.421.3 14.3 Avg. 41.140.8 38.433.1 31.5 OCR & ChartChartQA 86.584.1 87.183.4 80.0 CharXiv DQ 74.169.8 63.858.2 47.6 DocVQA 95.094.9 94.492.7 87.2 OCRBench 82.984.2 80.079.2 62.1 AI2Dw M 84.282.6 83.678.6 81.4 AI2Dw/o M 94.193.4 93.390.7 90.8 InfoVQA 78.481.7 76.175.6 68.8 Avg. 85.084.4 82.679.8 74.0 OthersPixmoCount 62.263.3 52.250.9 49.3 CountBench 88.286.4 79.872.5 78.4 VL-RewardBench 46.749.7 48.242.1 44.5 V* 78.077.0 74.969.6 72.3 Avg. 68.869.1 63.858.8 61.1 4 Training Strategy 4.1 Training Pipeline Following LLaVA-OneVision (Li et al., 2025a), LLaVA-OneVision-1.5 undergoes three distinct learning stages to enable LLM for multimodal capabilities: ‚Ä¢Stage-1: Language-Image Alignment.The stage aims to pretrain the projection layer with the LLaVA-1.5 558K to align the",
    "44.5 V* 78.077.0 74.969.6 72.3 Avg. 68.869.1 63.858.8 61.1 4 Training Strategy 4.1 Training Pipeline Following LLaVA-OneVision (Li et al., 2025a), LLaVA-OneVision-1.5 undergoes three distinct learning stages to enable LLM for multimodal capabilities: ‚Ä¢Stage-1: Language-Image Alignment.The stage aims to pretrain the projection layer with the LLaVA-1.5 558K to align the visual features into the word embedding space of LLMs. ‚Ä¢Stage-1.5: High-Quality Knowledge Learning.Building on the language-image alignment stage, we introduce the high-quality knowledge learning stage to strike a balance between computationalefficiencyandinjectingnewknowledgeintoLMMs. Inthisstage,wetransition tofull-parametertrainingofallmodulesusingtheLLaVA-OneVision-1.5-Mid-Traningdataset. ‚Ä¢Stage-2: Visual Instruction Tuning.To enable LMMs to handle a diverse range of visual tasks with desired responses, we continue full-parameter training with the proposed LLaVA- OneVision-1.5-Instruct as well as the FineVision (Face, 2025) dataset. 6 4.2 Infrastructure Load Balancing via Data Packing.A major source of training inefficiency arises from padding, where batch samples are standardized by adding padding tokens. This results in significant computational overhead and poor GPU utilization, particularly with heterogeneous multimodal data. To mitigate this, we propose an offline parallel data packing method that consolidates multiple shorter samples into packed sequences during preprocessing. Our approach employs hash buckets to handle large-scale data efficiently and leverages multi-threaded, strategy-aware batching to control packing success rate, sample count, and batch composition. Unlike online packing, which operates dynamically at runtime, our method processes entire datasets or large contiguous chunks offline, ensuring uniform output lengths. This yields up to an 11√ócompression ratio on 85 million pretraining samples, substantially improving efficiency. HybridParallelismFramework.WeadoptAIAK-Training-LLM1builtuponMegatron-LM(Shoeybi et al., 2019) as our training framework. Its transformer engine and specialized optimizations enable efficient mid-training of LLaVA-OneVision-1.5-8B with a context length of 8K. By lever- aging distributed optimizer parallelism and uniform recomputation, the mid-training process is conducted at native resolution on 85 million captions using 128√óA800 GPUs over 3.7 days. 5 Experiments 5.1 Overall Performance We use LMMs-Eval Zhang et al. (2024) with the default prompt to evaluate the performance of LLaVA-OneVision-1.5 across multiple benchmarks in four categories of downstream tasks:(1) General Visual Question Answering (VQA): MMStar (Chen et al., 2024a), MMEBench series (Fu et al., 2023), MME-RealWorld series (Zhang et al., 2025), SeedBench (Li et al., 2024b), Seed- Bench-2-Plus (Li et al., 2024a), CV-Bench (Tong et al., 2024), and RealWorldQA (Corp., 2024). (2) MultimodalReasoning: MathVista(Luetal.,2024),WeMath(Qiaoetal.,2025),MathVision(Wang etal.,2024a), MMMU(Yueetal.,2024), andMMMU-Proseries(Yueetal.,2025).(3)OCR&Chart Understanding: ChartQA (Masry et al., 2022), CharXiv (Wang et al., 2024d), DocVQA (Mathew et al., 2021), OCRBench (Liu et al., 2024c), AI2D (Kembhavi et al., 2016), and InfoVQA (Mathew et al., 2022).(4) Others: PixmoCount (Deitke et al., 2025), CountBench (Paiss et al., 2023), VL-RewardBench (Li et al., 2025b), and V*(Wu and Xie, 2024). As shown in Tab. 1, LLaVA- OneVision-1.5-8B surpasses Qwen2.5-VL-7B on 18 of 27 benchmarks and LLaVA-OneVision-1.5-4B surpasses Qwen2.5-VL-3B on 27 of 27 benchmarks. 5.2 General Visual Question Answering As detailed in Tab. 1, we evaluate the general visual question answering capability of LLaVA- OneVision-1.5 across multiple benchmarks, and LLaVA-OneVision-1.5-8B demonstrates superior performance on MMStar (67.7), MMBench en(84.1), MME-RealWorld en(62.3), MME-RealWorld cn (56.1), CV-Bench (80.8), and ScienceQA (95.0). Besides, LLaVA-OneVision-1.5 also presents comparable performance on MMBench cn(81.0), SeedBench image(77.3), SEED-Bench-2-Plus (69.2), and RealWorldQA (68.1). 1AIAK-Training-LLM: Baidu Cloud‚Äôs optimized Megatron-LM",
    "question answering capability of LLaVA- OneVision-1.5 across multiple benchmarks, and LLaVA-OneVision-1.5-8B demonstrates superior performance on MMStar (67.7), MMBench en(84.1), MME-RealWorld en(62.3), MME-RealWorld cn (56.1), CV-Bench (80.8), and ScienceQA (95.0). Besides, LLaVA-OneVision-1.5 also presents comparable performance on MMBench cn(81.0), SeedBench image(77.3), SEED-Bench-2-Plus (69.2), and RealWorldQA (68.1). 1AIAK-Training-LLM: Baidu Cloud‚Äôs optimized Megatron-LM 7 Table 2Comparison of RICE-ViT with other vision encoders using the LLaVA-NeXT framework. All models are evaluated using identical configurations: Qwen2.5-7B as the language model, LLaVA-NeXT training data, and the same training pipeline. To ensure fair comparison, we adopt LLaVA-NeXT‚Äôs tiling strategy (up to 2 √ó2+1 tiles) for handling high-resolution images, as many vision encoders do not support native resolution processing. Model Configuration OCR & Document Understanding General Vision Understanding Method Vision Tower InfoVQA DocVQA ChartQA TextVQA OCRBench OCRBenchV2 LiveXivVQA OCR Avg AI2D MMBEN MMECog MMEPer POPE RealworldQA MMStar Other Avg CLIP ViT-L-14-336px 38.9 75.2 66.5 62.5 52.5 23.0 47.4 52.373.2 74.6 48.0 75.688.8 63.749.0 67.6 MLCD ViT-L-14-336px 43.5 76.5 67.8 61.7 53.1 24.0 48.4 53.677.0 76.4 54.1 79.9 88.7 61.1 51.0 69.7 AIMv2 ViT-L-14-336px 35.4 77.272.7 65.957.2 23.9 47.3 54.275.478.648.3 75.0 88.4 62.2 50.2 68.3 RICE-ViT ViT-L-14-336px 45.279.272.365.957.524.148.956.277.976.654.680.788.563.151.870.5 DFN5B ViT-H-14-378px 38.6 70.9 64.4 59.4 47.3 21.9 46.2 49.873.5 73.4 45.8 76.9 88.6 59.9 49.1 66.7 SigLIP ViT-SO400M-14-384px 41.4 76.7 69.3 64.7 55.4 24.0 48.4 54.376.2 77.0 46.1 79.9 88.863.747.3 68.4 SigLIPv2 ViT-SO400M-14-384px 43.7 79.1 70.2 66.2 58.7 25.4 48.6 56.077.077.1 46.680.4 89.363.452.8 69.5 RICE-ViT ViT-L-14-378px 48.182.675.166.258.825.849.558.076.577.654.179.089.162.951.270.1 SigLIPv2 ViT-SO400M-16-560px 50.2 86.2 77.470.2 62.7 26.552.9 60.977.076.5 53.579.9 89.3 68.2 53.1 71.1 RICE-ViT ViT-L-14-560px 53.287.478.169.060.726.153.061.176.978.656.379.388.965.150.570.8 Qwen-ViT from Qwen2.5-VL 7B ViT-H-14-560px 55.985.8 78.8 73.7 66.2 26.8 53.4 62.978.8 78.462.080.8 88.6 64.2 55.0 72.5 RICE-ViT from OV-1.5 3B ViT-L-14-560px 53.787.181.973.873.330.453.664.880.379.658.682.289.067.356.673.4 5.3 Multimodal Reasoning LLaVA-OneVision-1.5exhibitssuperiormultimodalreasoningcapabilitiescomparedtoQwen2.5-VL. Specifically, LLaVA-OneVision-1.5-4B outperforms Qwen2.5-VL-3B on all evaluated benchmarks, leading in MathVista mini(67.9), WeMath (24.9), MathVision (24.2), MMMU val(52.7), MMMU- Prostandard(35.3), and MMMU-Pro vision(25.4). Notably, LLaVA-OneVision-1.5-4B surpasses LLaVA- OneVision-7B across all benchmarks. Compared with Qwen2.5-VL-7B, LLaVA-OneVision-1.5-8B also demonstrates gains of 1.0%, 0.3%, 3.2%, 4.1%, and 1.1% on MatchVista mini, WeMath, MathVision, MMMU val, and MMMU-Pro standard. 5.4 OCR & Chart Understanding The interpretation of visual data, including documents and charts, requires a sophisticated array of skills from multimodal large language models, ranging from low-level Optical Character Recognition (OCR) to high-level semantic reasoning. To thoroughly evaluate these capabilities, we assess LLaVA-OneVision-1.5 across seven challenging benchmarks. LLaVA-OneVision-1.5-8B demonstrates robust outcomes on ChartQA (86.5), CharXiv DQ(74.1), DocVQA (95.0), AI2D w M (84.2), and AI2D w/o M(94.1). Notably, LLaVA-OneVision-1.5-4B outperforms Qwen2.5-VL-3B on all seven benchmarks. 5.5 Others To further elucidate the capabilities of LLaVA-OneVision-1.5, we extend our evaluation to in- clude PixmoCount, CountBench, VL-RewardBench, and V*. LLaVA-OneVision-1.5-8B records scores of 62.2 on PixmoCount, 88.2 on CountBench, 46.7 on VL-RewardBench, and 78.0 on V*, demonstrating proficiency in counting, visual perception, and visual grounding, on par with Qwen2.5-VL-7B. 8 Figure4Performancecomparisonacrossdifferentmid-trainingdatascalesonvariousbenchmarks. Models initially undergo pre-training on LLaVA-558K and are then subjected to mid-training at different data scales (4M, 85M), followed by fine-tuning using the LLaVA-NeXT (Liu et al., 2024b) SFT framework. 0M denotes native pre-training without the mid-training stage. Figure 5Experimental results using 2M blanced and unbalanced mid-training samples (LLaVA-",
    "Figure4Performancecomparisonacrossdifferentmid-trainingdatascalesonvariousbenchmarks. Models initially undergo pre-training on LLaVA-558K and are then subjected to mid-training at different data scales (4M, 85M), followed by fine-tuning using the LLaVA-NeXT (Liu et al., 2024b) SFT framework. 0M denotes native pre-training without the mid-training stage. Figure 5Experimental results using 2M blanced and unbalanced mid-training samples (LLaVA- NeXT-780k as the SFT data) show that using a balanced mid-training dataset yields consistent improvements over a random sampling strategy. 5.6 Ablation Study 5.6.1 Comparison of Different Vision Encoders InTab.2, weevaluatevariousvisionencoders, CLIP(Radfordetal.,2021), MLCD(Anetal.,2024), AIMv2 (Fini et al., 2025), DFN (Fang et al., 2023), SigLIP (Zhai et al., 2023), SigLIPv2 (Tschannen etal.,2025), andRICE-ViT(Xieetal.,2025), withintheLLaVA-NeXTframework. Ataresolutionof 336 pixels, RICE-ViT surpasses CLIP across all benchmarks, achieving significant gains in InfoVQA (+6.3%) and OCRBench (+5.0%). It also demonstrates significant improvements in document understandingcomparedtoAIMv2. At378pixels,RICE-ViToutperformscomputationallyintensive models such as SigLIPv2 in 9 of 14 benchmarks, notably in InfoVQA (+4.4%), DocVQA (+3.5%), andChartQA(+4.9%). TheseresultspositionRICE-ViTasaleadingvisionencoder,enhancingOCR capabilities and providing robust visual understanding, crucial for applications requiring advanced document analysis and visual reasoning. In addition, we further compare the performance of RICE-ViT with that of Qwen-ViT after incorporating LMM training, where RICE-ViT is derived from LLaVA-OneVision-1.5-3B and Qwen-ViT is derived from Qwen2.5-VL-7B. In the areas of OCR & Document Understanding and General Vision Understanding, RICE-ViT demonstrates average performance improvements of 1.9% and 0.9% compared to Qwen-ViT. 9 Figure6Performancecomparisonofthreedatasets(Merge46M,FineVision,andLLaVA-OneVision- 1.5-Inst-Data) across 16 benchmarks during the SFT phase, demonstrating the superiority of Merge46M on most benchmarks. 5.6.2 Mid-Training Data Scaling As depicted in Fig. 4, we present the performance of three different LMMs trained with various scales of mid-training data across ten distinct benchmarks. Employing LLaVA-558K for language- image alignment and standard LLaVA-Next instruction tuning, our findings indicate that scaling the data volume during the high-quality knowledge learning phase consistently enhances model performance across all benchmarks. These results not only underscore the high quality and scalability of the proposed LLaVA-OneVision-1.5-Mid-Traning dataset but also confirm the efficacy of data scaling in improving the performance of LMMs. 5.6.3 Effectiveness of Concept Balance Fig. 3(a) illustrates that after implementing concept balancing, LLaVA-OneVision-1.5-Mid-Traning exhibits a smoother distribution, thereby enhancing the model‚Äôs capability to assimilate a more comprehensive set of knowledge. To further validate the effect of concept balance, we conducted a comparative analysis of models trained on 2M concept-balanced data versus those trained on 10 2M data obtained through random sampling. As indicated in Fig. 5, the concept-balanced 2M data set demonstrates superior performance in 25 of 27 evaluated downstream benchmarks. 5.6.4 Instruction Data Quality and Scaling To enhance performance across diverse VQA tasks, we compile 124 types of instruction data (LLaVA-OneVision-1.5-Inst-Data) for SFT training. We further scale the model capabilities by deduplicating and merging the recently proposed FineVision dataset (HuggingFaceM4, 2025), resulting in the Merged46M SFT dataset. To maintain consistent training steps, we double the batch size for Merged46M due to its larger scale. Fig. 6 shows performance comparisons on 16 benchmarks during SFT using three datasets: LLaVA-OneVision-1.5-Inst-Data, FineVision, and Merged46M. LLaVA-OneVision-1.5-Inst-Data achieves performance comparable to FineVision, while the Merged46M dataset delivers the best results across nearly all benchmarks. 6 Conclusions In this work, we introduce LLaVA-OneVision-1.5,",
    "for Merged46M due to its larger scale. Fig. 6 shows performance comparisons on 16 benchmarks during SFT using three datasets: LLaVA-OneVision-1.5-Inst-Data, FineVision, and Merged46M. LLaVA-OneVision-1.5-Inst-Data achieves performance comparable to FineVision, while the Merged46M dataset delivers the best results across nearly all benchmarks. 6 Conclusions In this work, we introduce LLaVA-OneVision-1.5, a family of large multimodal models that estab- lishes a new paradigm for constructing high-performance vision-language systems with improved efficiency and reproducibility. We demonstrate the feasibility of training competitive multimodal models from scratch under strict constraints. Our contributions are threefold: a large-scale, curated multimodal dataset; an efficient end-to-end training framework operable under a limited budget; and extensive empirical results demonstrating state-of-the-art performance across diverse benchmarks. The model excels particularly in resource-constrained settings, surpassing strong baselines such as Qwen2.5-VL-7B. This study underscores how open and efficient frameworks can drive progress in multimodal AI, democratizing access to state-of-the-art performance. We envision LLaVA-OneVision-1.5 as a foundational resource that empowers the community to build specialized applications and develop more powerful LMMs across diverse tasks through continued scaling. References Xiang An, Kaicheng Yang, Xiangzi Dai, Ziyong Feng, and Jiankang Deng. Multi-label cluster discrimination for visual representation learning. In ECCV, 2024. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2.5-vl technical report. arXiv:2502.13923, 2025. Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Saehoon Kim. Coyo-700m: Image-text pair dataset. https://github.com/kakaobrain/coyo-dataset , 2022. Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? In NeurIPS, 2024a. Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shen- glong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv:2412.05271, 2024b. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the 11 frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv:2507.06261, 2025. X.AI Corp. Grok-1.5 vision preview: Connecting the digital and physical worlds with our first multimodal model.https://x.ai/blog/grok-1.5v, 2024. Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Moham- madreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open weights and open data for state-of-the-art vision-language models. In CVPR, 2025. Hugging Face. Finevision: Open data is all you need. https://huggingface.co/spaces/Hugg ingFaceM4/FineVision, 2025. Alex Fang, Albin Madappally Jose, Amit Jain, Ludwig Schmidt, Alexander Toshev, and Vaishaal Shankar. Data filtering networks. In ICLR, 2023. Enrico Fini, Mustafa Shukor, Xiujun Li, Philipp Dufter, Michal Klein, David Haldimann, Sai Aitharaju, Victor Guilherme Turrisi da Costa, Louis B√©thune, Zhe Gan, Alexander T Toshev, Marcin Eichner, Moin Nabi, Yinfei Yang, Joshua M. Susskind, and Alaaeldin El-Nouby. Multi- modal autoregressive pre-training of large vision encoders. In CVPR, 2025. Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li,",
    "da Costa, Louis B√©thune, Zhe Gan, Alexander T Toshev, Marcin Eichner, Moin Nabi, Yinfei Yang, Joshua M. Susskind, and Alaaeldin El-Nouby. Multi- modal autoregressive pre-training of large vision encoders. In CVPR, 2025. Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, et al. Mme: A comprehensive evaluation benchmark for multimodal large language models. arXiv:2306.13394, 2023. Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Datacomp: In search of the next generation of multimodal datasets. In NeurIPS, 2023. Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan, Jian Wang, Jianyu Jiang, Jiawei Wang, et al. Seed1. 5-vl technical report. arXiv:2505.07062, 2025. HuggingFaceM4. Finevision on hugging face spaces, 2025. URL https://huggingface.co/spa ces/HuggingFaceM4/FineVision. Accessed: 2025-09-25. AniruddhaKembhavi, MikeSalvato, EricKolve, MinjoonSeo, HannanehHajishirzi, andAliFarhadi. A diagram is worth a dozen images. In ECCV, 2016. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. In ICCV, 2023. Hugo Lauren√ßon, Lucile Saulnier, L√©o Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander Rush, Douwe Kiela, et al. Obelics: An open web-scale filtered dataset of interleaved image-text documents. In NeurIPS, 2023. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. TMLR, 2025a. Bohao Li, Yuying Ge, Yi Chen, Yixiao Ge, Ruimao Zhang, and Ying Shan. Seed-bench-2-plus: Benchmarking multimodal large language models with text-rich visual comprehension. In ICLR, 2024a. Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Bench- marking multimodal llms with generative comprehension. In CVPR, 2024b. 12 Lei Li, Yuancheng Wei, Zhihui Xie, Xuqing Yang, Yifan Song, Peiyi Wang, Chenxin An, Tianyu Liu, Sujian Li, Bill Yuchen Lin, et al. Vl-rewardbench: A challenging benchmark for vision-language generative reward models. In CVPR, 2025b. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS , 2023. HaotianLiu, ChunyuanLi, YuhengLi, andYongJaeLee. Improvedbaselineswithvisualinstruction tuning. In CVPR, 2024a. Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, 2024b. URL https://llava-vl. github.io/blog/2024-01-30-llava-next/. Yuliang Liu, Zhang Li, Mingxin Huang, Biao Yang, Wenwen Yu, Chunyuan Li, Xu-Cheng Yin, Cheng-Lin Liu, Lianwen Jin, and Xiang Bai. Ocrbench: on the hidden mystery of ocr in large multimodal models. Science ChinaInformation Sciences, 2024c. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In ICLR, 2024. AhmedMasry,XuanLongDo,JiaQingTan,ShafiqJoty,andEnamulHoque. Chartqa: Abenchmark for question answering about charts with visual and logical reasoning. In ACL, 2022. Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: A dataset for vqa on document images. In WACV, 2021. Minesh Mathew, Viraj Bagal, Rub√®n Tito, Dimosthenis Karatzas, Ernest Valveny, and CV Jawahar. Infographicvqa. In WACV, 2022. Roni Paiss,",
    "for question answering about charts with visual and logical reasoning. In ACL, 2022. Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: A dataset for vqa on document images. In WACV, 2021. Minesh Mathew, Viraj Bagal, Rub√®n Tito, Dimosthenis Karatzas, Ernest Valveny, and CV Jawahar. Infographicvqa. In WACV, 2022. Roni Paiss, Ariel Ephrat, Omer Tov, Shiran Zada, Inbar Mosseri, Michal Irani, and Tali Dekel. Teaching clip to count to ten. In ICCV, 2023. Runqi Qiao, Qiuna Tan, Guanting Dong, Minhui Wu, Chong Sun, Xiaoshuai Song, Zhuoma GongQue, Shanglin Lei, Zhe Wei, Miaoxuan Zhang, et al. We-math: Does your large multimodal model achieve human-like mathematical reasoning? In ACL, 2025. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, 2021. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. IJCV, 2015. Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv:1909.08053, 2019. Qwen Team. Qwen3 technical report, 2025. Peter Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Adithya Jairam Vedagiri IYER, Sai Charitha Akula, Shusheng Yang, Jihan Yang, Manoj Middepogu, Ziteng Wang, et al. Cambrian-1: A fully open, vision-centric exploration of multimodal llms. In NeurIPS, 2024. 13 Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdul- mohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, Olivier H√©naff, Jeremiah Harmsen, Andreas Steiner, and Xiaohua Zhai. Siglip 2: Multilingual vision-language encoders with improved semantic understanding, localization, and dense features, 2025. Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. In NeurIPS, 2024a. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language model‚Äôs perception of the world at any resolution. arXiv:2409.12191, 2024b. WeizhiWang, YuTian, LinjieYang, Heng Wang, andXifeng Yan. Open-qwen2vl: Compute-efficient pre-training of fully-open multimodal llms on academic resources. In COLM, 2025. Xingyao Wang, Zihan Wang, Jiateng Liu, Yangyi Chen, Lifan Yuan, Hao Peng, and Heng Ji. Mint: Evaluating llms in multi-turn interaction with tools and language feedback. In ICLR, 2024c. Zirui Wang, Mengzhou Xia, Luxi He, Howard Chen, Yitao Liu, Richard Zhu, Kaiqu Liang, Xindi Wu, Haotian Liu, Sadhika Malladi, et al. Charxiv: Charting gaps in realistic chart understanding in multimodal llms. In NeurIPS, 2024d. Penghao Wu and Saining Xie. V*: Guided visual search as a core mechanism in multimodal llms. InCVPR, 2024. Chunyu Xie, Heng Cai, Jincheng Li, Fanjing Kong, Xiaoyu Wu, Jianfei Song, Henrique Morimitsu, Lin Yao, Dexin Wang, Xiangzheng Zhang, et al. Ccmb: A large-scale chinese cross-modal benchmark. In ACMMM, 2023. Yin Xie, Kaicheng Yang, Xiang An, Kun Wu, Yongle Zhao, Weimo Deng, Zimin Ran, Yumeng Wang, Ziyong Feng, Roy Miles, et al. Region-based cluster discrimination for visual representation learning. In ICCV, 2025. Hu Xu,",
    "Yao, Dexin Wang, Xiangzheng Zhang, et al. Ccmb: A large-scale chinese cross-modal benchmark. In ACMMM, 2023. Yin Xie, Kaicheng Yang, Xiang An, Kun Wu, Yongle Zhao, Weimo Deng, Zimin Ran, Yumeng Wang, Ziyong Feng, Roy Miles, et al. Region-based cluster discrimination for visual representation learning. In ICCV, 2025. Hu Xu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao Huang, Russell Howes, Vasu Sharma, Shang-Wen Li, Gargi Ghosh, Luke Zettlemoyer, and Christoph Feichtenhofer. Demystifying clip data. In ICLR, 2024. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In CVPR, 2024. Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Ming Yin, Botao Yu, Ge Zhang, et al. Mmmu-pro: A more robust multi-discipline multimodal understanding benchmark. In ACL, 2025. Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In ICCV, 2023. Jiaxing Zhang, Ruyi Gan, Junjie Wang, Yuxiang Zhang, Lin Zhang, Ping Yang, Xinyu Gao, Ziwei Wu, Xiaoqun Dong, Junqing He, Jianheng Zhuo, Qi Yang, Yongfeng Huang, Xiayu Li, Yanghan Wu, Junyu Lu, Xinyu Zhu, Weifeng Chen, Ting Han, Kunhao Pan, Rui Wang, Hao Wang, Xiaojun Wu, Zhongshen Zeng, and Chongpei Chen. Fengshenbang 1.0: Being the foundation of chinese cognitive intelligence. arXiv:2209.02970, 2022. 14 Kaichen Zhang, Bo Li, Peiyuan Zhang, Fanyi Pu, Joshua Adrian Cahyono, Kairui Hu, Shuai Liu, Yuanhan Zhang, Jingkang Yang, Chunyuan Li, and Ziwei Liu. Lmms-eval: Reality check on the evaluation of large multimodal models, 2024. URLhttps://arxiv.org/abs/2407.12772. Yi-Fan Zhang, Huanyu Zhang, Haochen Tian, Chaoyou Fu, Shuangqing Zhang, Junfei Wu, Feng Li, Kun Wang, Qingsong Wen, Zhang Zhang, et al. Mme-realworld: Could your multimodal llm challenge high-resolution real-world scenarios that are difficult for humans? In ICLR, 2025. Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv:2504.10479, 2025. 15 A LLaVA-OV-1.5 vs. Qwen2.5-VL with Same LLM ToenableafaircomparisonwithQwen2.5-VL,wetrainLLaVA-Onevision-1.5-3BbasedonQwen2.5- 3B-Instruct. As shown in Fig. 7, LLaVA-Onevision-1.5-3B also demonstrates superior performance, achieving better results on 17 out of 27 downstream benchmarks. Figure 7Comparison between LLaVA-OV-1.5-3B and Qwen2.5-VL-3B model on public datasets using the same LLM for both evaluations. Figure 8Original concept distributions across eight common vision datasets used in the LLaVA- OneVision-1.5-Mid-Traning dataset. 16 B Mid-Training Data: Concept Distribution and Top 50 Topics Fig.8presentstherawconceptdistributionsacrosseightcommonvisiondatasetsusedintheLLaVA- OneVision-1.5-Mid-Traning dataset. All sources exhibit a pronounced long -tail bias, indicating that the original data are far from comprehensive. Obelics displays the broadest and most uniform distribution (slowest tail decay), while others, such as ImageNet-21K and SA-1B, cover fewer concepts with higher frequency concentration. To characterize the semantic space of the mid- training mixture, we apply topic modeling to associated texts and extract the 50 most salient topics (Tab. 3). These span diverse domains, including wildlife, apparel, cuisine, interiors, engineering, electronics, healthcare, WebUI, and cultural activities, offering an interpretable summary of data coverage. ThisanalysisconfirmsObelicsasthemostcomprehensivesourceandhighlightscoverage gaps in other datasets, informing subsequent data curation.",
    "training mixture, we apply topic modeling to associated texts and extract the 50 most salient topics (Tab. 3). These span diverse domains, including wildlife, apparel, cuisine, interiors, engineering, electronics, healthcare, WebUI, and cultural activities, offering an interpretable summary of data coverage. ThisanalysisconfirmsObelicsasthemostcomprehensivesourceandhighlightscoverage gaps in other datasets, informing subsequent data curation. Table 3Topic Modeling Results (50 Topics) Concept Related words Wildlife feathers, wildlife, beak, branch, birds, habitat, scales, plumage, claws, feather, behavior, spots, watchers, snout, species, creature, paws, limbs, flight, drawing Jewelry and Aviation jewelry, beads, airplane, aircraft, aviation, plane, sparkle, bracelet, flight, stones, elegance, pendant, accessory, loop, diamonds, gemstones, gemstone, pearl, landing, shine Internet Humor humor, phrase, eco, baby, parents, sustainability, frustration, references, meme, sentiment, surprise, mother, farm, disney, twist, internet, cartoonish, novelty, reaction, drawing Education and Learning slide, educators, learners, bullet, education, learning, class- room, study, math, guide, student, resource, writing, text- book, skills, problem, publisher, question, list, questions Watches and Accessories case, strap, watch, wristwatch, hour, brim, crown, markers, dial, barrel, numerals, grip, accessory, clock, luxury, gun, minute, buckle, rifle, stitching Gaming and Sports games, gamers, baseball, armor, jerseys, fi, franchise, sword, basketball, horror, competition, sport, court, gaming, anime, athleticism, readiness, arena, athletes, flames Certification and Professional- ismstamp, clients, certification, seal, colleagues, stamps, stan- dards, calligraphy, headshot, friendliness, compliance, certifi- cations, profiles, mark, envelope, creases, impression, identi- fication, trustworthiness, german Vehicles and Racing seat, wheel, tires, grille, truck, license, headlights, speed, driver, track, rims, quarter, mirrors, race, seats, racing, train, transportation, three-quarter, windshield Architectural and Mechanical Plansgrooves, architects, drawings, plan, drawing, tire, hub, roller, wheel, tread, plans, coaster, distances, blueprint, 2023, rub- ber, planners, amusement, skateboard, traction Continued on next page 17 Table 3 ‚Äì continued from previous page Concept Related words Ceremonies and Achievement achievement, pride, ceremony, award, trophy, university, cam- pus, awards, skeleton, victory, graduation, bones, success, achievements, alumni, casino, medal, certificate, dinosaur, tuxedo Portraiture and Makeup finger, eyebrows, jawline, cheek, makeup, cheeks, portraiture, index, nails, individual, strands, thumb, beer, cheekbones, brow, bangs, ear, shot, stubble, creases Software and Development software, computer, developers, options, system, user, code, management, file, version, application, menu, fields, input, process, arrows, programming, files, 3d, interfaces Pets and Footwear dog, footwear, owners, cat, shoe, laces, toe, sole, heel, paws, tongue, sneakers, soles, mesh, ankle, rubber, foot, dogs, toes, straps Typography and Design sans, gradients, letter, variations, symbolism, forms, spac- ing, thickness, variation, meaning, bold, alignment, curves, designers, representation, emotions, rectangle, curve, knowl- edge, trademark Cuisine and Cooking cooks, meal, ingredients, dish, freshness, vegetables, cooking, sauce, fruit, slices, pieces, herbs, spoon, meat, cuisine, crispy, slice, rice, fruits, chocolate Home Interiors coffee, lamp, shelf, living, vase, bedroom, cabinet, pillows, comfort, rug, countertop, cup, homeowners, sofa, pillow, cab- inets, counter, flooring, couch, shade Skincare and Packaging skincare, liquid, lid, wine, container, ingredients, tube, sup- plements, screw, premium, luxury, benefits, transparency, labeling, jar, solutions, supplement, powder, oil, spray Childhood and Holidays parents, toy, holiday, baby, gift, toys, polka, kids, christmas, decorations, ribbon, bear, caregivers, fun, charm, decoration, greeting, rabbit, birthday, dot Social Interactions pen, smiles, postures, mid-20th-century, discussion, celebrity, hairstyles, collage, interactions, collaboration, jackets, note- book, -century, fourth,",
    "luxury, benefits, transparency, labeling, jar, solutions, supplement, powder, oil, spray Childhood and Holidays parents, toy, holiday, baby, gift, toys, polka, kids, christmas, decorations, ribbon, bear, caregivers, fun, charm, decoration, greeting, rabbit, birthday, dot Social Interactions pen, smiles, postures, mid-20th-century, discussion, celebrity, hairstyles, collage, interactions, collaboration, jackets, note- book, -century, fourth, gestures, cinema, teamwork, interview, ties, plaid Literature and Romance love, spine, intimacy, works, couple, publisher, affection, novel, romance, proximity, moments, bond, hardcover, 19th- century, pages, edition, poetry, drama, collection, covers Inspirational Quotes quote, self, positivity, inspiration, resilience, philosophy, quotes, hope, help, introspection, journey, phrase, attribu- tion, things, weight, motivation, freedom, contemplation, love, vulnerability Continued on next page 18 Table 3 ‚Äì continued from previous page Concept Related words Waterside Leisure beach, shore, ripples, relaxation, sand, roofs, boat, sea, pool, bridge, shoreline, turquoise, landscapes, boats, river, lake, deck, skyline, trunks, leisure Engineering and Mechanics component, engineers, holes, mechanics, diy, engineering, technicians, screws, hardware, hole, specifications, wires, electronics, manufacturers, steel, manufacturing, manufac- turer, circuit, grip, ends Retail and Pricing price, package, shoppers, 10, barcode, promotion, pack, store, 20, sale, 100, tag, info, shipping, 50, medication, 30, sales, discount, quantity Technology and Innovation innovation, network, lightning, connectivity, bolt, globe, con- nections, nodes, ideas, cloud, gears, communication, net- works, bulb, intelligence, integration, lightbulb, concept, telecommunications, networking Industrial and Manufacturing workers, machine, machinery, manufacturing, storage, safety, fish, facility, factory, workshop, pipes, warehouse, mainte- nance, logistics, task, site, fins, warning, steel, production Trust and Publication trust, shield, reliability, studies, china, stakeholders, clients, partners, publication, translation, institution, publisher, pub- lic, tradition, scholars, formality, stability, organizations, strength, academics Healthcare and Fitness healthcare, fitness, patients, training, wellness, exercise, hos- pital, practice, gym, strength, clinic, muscles, shirtless, sup- port, lab, treatment, stethoscope, muscle, weight, doctor Museum Artifacts motifs, sculpture, bronze, museum, statue, coin, folds, en- gravings, artifacts, skull, scroll, item, carvings, coins, gallery, pedestal, scratches, artifact, elegance, artwork Dining and Hospitality tables, restaurant, caf√©, experience, spot, counter, customers, seating, locals, diners, drinks, shop, establishment, patrons, menu, casual, fixtures, hats, market, drink Real Estate estate, tiles, homeowners, shrubs, bathroom, lawn, railings, bushes, porch, property, driveway, staircase, bricks, houses, panes, yard, homebuyers, neighborhood, chimney, concrete Music and Geography guitar, musicians, concert, instrument, instruments, country, artist, strings, regions, musician, roads, singer, drum, song, locations, geography, land, performers, performances, maps Entomology and Containers lid, antennae, container, insect, butterfly, spots, shell, spines, wing, ridges, candle, storage, rim, insects, hairs, abdomen, entomology, bee, iridescent, observation Continued on next page 19 Table 3 ‚Äì continued from previous page Concept Related words Apparel Design apparel, pocket, neckline, pockets, waist, garment, fit, cuffs, zipper, hip, belt, tag, straps, torso, cotton, crew, seams, hem, hood, knee Crafting and DIY crafters, craft, supplies, crafts, diy, artists, projects, creativ- ity, lab, textiles, textile, laboratory, project, pieces, motifs, stationery, pencil, thread, tactile, workspace Data and Chemistry graph, comparison, values, axis, trends, analysis, chart, x-, analysts, market, chemistry, metrics, chemical, visualization, graphs, 2d, years, investors, axes, groups Electronic Devices devices, smartphone, computer, keyboard, laptop, lens, mon- itor, cable, indicator, case, audio, keys, speaker, tablet, usb, ports, security, workspace, electronics, solutions Recreation and Law police, law, sheet,",
    "Chemistry graph, comparison, values, axis, trends, analysis, chart, x-, analysts, market, chemistry, metrics, chemical, visualization, graphs, 2d, years, investors, axes, groups Electronic Devices devices, smartphone, computer, keyboard, laptop, lens, mon- itor, cable, indicator, case, audio, keys, speaker, tablet, usb, ports, security, workspace, electronics, solutions Recreation and Law police, law, sheet, superhero, notes, golf, enforcement, club, course, officer, officers, cape, notation, copyright, costume, security, piano, personnel, swing, musicians Web and UI navigation, app, user, header, links, webpage, search, smart- phone, screenshot, web, options, link, yuan, tabs, battery, url, post, site, menu, email Space and Music Media moon, earth, exploration, record, wonder, vinyl, planet, pur- ples, astronomy, records, sphere, thirds, crescent, two-thirds, landmasses, herbarium, rocket, oceans, cloud, magic Business and Finance Docu- mentsbullet, info, finance, header, management, money, address, documents, topics, question, headers, description, list, head- ings, job, email, fields, slide, dollar, newspaper Weddings and Traditions wedding, lace, embroidery, dance, festival, dresses, indian, traditions, bouquet, occasion, gown, couple, bride, costumes, couples, weddings, outfits, tradition, headscarf, veil Transportation and Fiction horse, ship, bike, mystery, bicycle, soldiers, handlebars, thriller, hull, motorcycle, crime, smoke, genre, rider, genres, cyclists, drama, horses, war, dramas Botany and Gardening stems, veins, cluster, stem, blooms, bloom, gardeners, soil, gardening, tips, clusters, bark, buds, trunk, stamens, botany, canopy, blossoms, fishing, droplets Vast Landscapes terrain, peaks, landscapes, desert, expanse, slopes, range, formations, vastness, isolation, grandeur, peak, hikers, valley, stones, mist, appeals, earth, slope, awe Biological Diagrams diagram, arrows, biology, research, documentation, anatomy, cell, diagrams, purposes, cells, blood, flow, study, emergency, specimen, scientists, tissue, understanding, measurements, response Continued on next page 20 Table 3 ‚Äì continued from previous page Concept Related words Religious Architecture carvings, grandeur, church, robe, dome, reverence, christian, robes, temple, site, statue, landmark, spires, arches, euro- pean, towers, statues, arch, castle, spire Time and Health Data dates, days, calendar, pm, masks, covid-19, times, info, week, month, ray, pandemic, schedule, daily, 1st, sunday, flyer, dna, 2020, helix Public Speaking speech, bubble, press, speaker, podium, politics, pin, govern- ment,formality,communication, mark,bubbles,microphones, announcement, lapel, u.s., gesture, exclamation, gravity, ges- tures Urban Streetscape signage, mid-morning, -morning, sidewalk, pavement, park- ing, pole, traffic, hotel, poles, store, locals, awning, lot, street- lights, pedestrians, station, residents, storefront, japan C Contributors Contributors ‚Ä¢Xiang An ‚Ä¢Yin Xie ‚Ä¢Kaicheng Yang ‚Ä¢Wenkang Zhang ‚Ä¢Xiuwei Zhao ‚Ä¢Zheng Cheng ‚Ä¢Yirui Wang ‚Ä¢Songcen Xu ‚Ä¢Changrui Chen ‚Ä¢Chunsheng Wu ‚Ä¢Huajie Tan ‚Ä¢Chunyuan Li ‚Ä¢Jing Yang ‚Ä¢Jie Yu ‚Ä¢Xiyao Wang ‚Ä¢Bin Qin ‚Ä¢Yumeng Wang ‚Ä¢Zizhen YanProject Leaders ‚Ä¢Ziyong Feng ‚Ä¢Ziwei Liu ‚Ä¢Bo Li ‚Ä¢Jiankang Deng 21"
  ]
}