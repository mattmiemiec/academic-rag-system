{
  "filename": "2509.08846v1.pdf",
  "total_chunks": 19,
  "text_length": 58550,
  "chunks": [
    "Uncertainty Estimation using Variance-Gated Distributions H. Martin Gillis∗ Faculty of Computer Science Dalhousie University Halifax, NS Canada martin.gillis@dal.caIsaac Xu∗ Faculty of Computer Science Dalhousie University Halifax, NS Canada isaac.xu@dal.caThomas Trappenberg† Faculty of Computer Science Dalhousie University Halifax, NS Canada tt@cs.dal.ca Abstract Evaluation of per-sample uncertainty quantification from neural networks is essen- tial for decision-making involving high-risk applications. A common approach is to use the predictive distribution from Bayesian or approximation models and decompose the corresponding predictive uncertainty into epistemic (model-related) and aleatoric (data-related) components. However, additive decomposition has recently been questioned. In this work, we propose an intuitive framework for uncertainty estimation and decomposition based on the signal-to-noise ratio of class probability distributions across different model predictions. We introduce a variance-gated measure that scales predictions by a confidence factor derived from ensembles. We use this measure to discuss the existence of a collapse in the diversity of committee machines. 1 Introduction High-risk decision-making using machine learning systems for applications such as healthcare, envi- ronment, autonomous driving, and financial, requires reliable uncertainty estimates for individual predictions to avoid undesirable outcomes. Uncertainty quantification (UQ) methods provide an ap- proach to detect when predictions should not be trusted and when abstentions or human interventions are required. The most commonly applied method for quantifying uncertainty is Bayesian model averaging (BMA) approximations, such as Monte Carlo dropout (MCD) [ 1], deep ensembles (DE) [ 2], and more recently, last-layer ensembles (LLE) [ 3–5], by averaging or sampling from a posterior of finite model parameters (Eq. 1). p(y|x,D)=Z wp(y|x,w)p(w| D) dw =E w∼p(w|D) [p(y|x,w)]≈1 MMX m=1p(y|x,w m)(1) An important current debate is how to measure predictive uncertainty, and how this total uncertainty (TU) can be decomposed into aleatoric (AU) and epistemic (EU) uncertainty in order to evaluate if the uncertainty is inherent in the data or due to limitations of a model. A common approach is to use measures based on entropy (Eq. 2). H[p(y|x,D)]|{z} T U(Total Uncertainty)=Ew∼p(w|D) [H(p(y|x,w))]| {z } AU(Aleatoric Uncertainty)+I[p(y,w|x,D)]|{z } EU(Epistemic Uncertainty)(2) Despite the theoretical support from information theory, the additive decomposition of total uncer- tainty has recently come under criticism and is now cautioned for uncertainty estimations [ 6]. Recent ∗Equal contributions. †Corresponding author. Preprint.arXiv:2509.08846v1 [cs.LG] 7 Sep 2025 work by Schweighofer et al. [4]extensively examined the entropy-based approach and showed that it mistakenly assumed the BMA distribution to be equivalent to the true posterior predictive distribution. The authors subsequently introduced an expected pairwise cross-entropy and Kullback–Leibler (KL) divergence of ensemble model predictions as measures for predictive and epistemic uncertainty, which is arguably the current state-of-the-art method for uncertainty quantification. In this study, we offer a complementary framework for measuring and decomposing uncertainty using the signal-to-noise-ratio (SNR) from means ( µ, signal) and standard deviations ( σ, noise) of class probability distributions derived from from BMA of MCD, LLE (multihead) and MCD of LLE (multihead) ensembles. We introduce a variance-gating function that attenuates class probabilities based on their local SNR. After normalization, the rescaled distributions favors more confidently (i.e., reduced uncertainty) predicted regions of the probability space, along with a new predictive uncertainty",
    "from from BMA of MCD, LLE (multihead) and MCD of LLE (multihead) ensembles. We introduce a variance-gating function that attenuates class probabilities based on their local SNR. After normalization, the rescaled distributions favors more confidently (i.e., reduced uncertainty) predicted regions of the probability space, along with a new predictive uncertainty profile. We also introduce a variance-gated measure of uncertainty and show preliminary data that our approach is comparable to Schweighofer et al. [4]. Our experimental results revealed a challenge with committee machines in that after long training there is a collapse in the diversity of model samples. Although this has sometimes been briefly noted in the literature [ 4,7–9], we evaluated this phenomenon with our new measure. 2 Variance-Gated Predictive Distribution In the following, we consider a multiclass classification within a committee machine setting [ 3]. To account for class variation in model disagreement, we introduce variance-gated distributions ˜pm,k that combines the mean predicted probabilities p(y|x,D) with a per-class gating mechanism that is shared across ensemble modelsmsuch that, ˜pm,k=pm(y)·Γ k(y)P jpm(j)·Γ k(j)(3) where Γk(y) = 1−exp[−µ(y)/kσ(y)] andµ(y) denotes the ensemble mean predicted probability for classyandσ(y) is the corresponding ensemble standard deviation, which quantifies model-to-model disagreement. The gating parameter k >0 is a scalar hyperparameter that controls sensitivity to variance. The resulting gate Γk(y)is a bounded, monotonic function in the range [0,1) . Under mild distributional assumptions, kσ(y) may be viewed as a scale factor that reflects the typical deviation of ensemble members from the mean prediction, thus relating the gating function to the population of model divergences. Effectively, hyperparameter kallows a user to define how many ensemble members disagree or deviate from the mean. This provides a used-defined sensitivity adjustment that can reflect the degree of acceptable risks for model predictions. The resulting ˜pm,kis subsequently normalized to ensure validity as a probability distribution. The variance-gated adjustment ensures that predictions with high model disagreement receive lower weight, even if their mean confidence is high. When predictions are consistent (or collapsed) across the ensembles ( kσ(y)≪µ(y) ), we haveΓk(y)→1 and thus ˜pm,k≈p. Conversely, when predictions are inconsistent ( kσ(y)≫µ(y) ); Γk(y)→0 , so the correction reduces confidence such that ˜pm,k(y)→0 . As a result, uncertainty is reduced with increasing k-values since confident and certain predictions are preserved, while ambiguous and uncertain predictions are suppressed. A property that is lacking for the standard entropy-based decomposition measures [6]. We quantify predictive uncertainty using the variance-gated distribution ˜pk(y|x,D) with its entropy H[˜pk]serving as the measure of total uncertainty. We obtain the principled decomposition of predictive uncertainty into aleatoric and epistemic components using variance-gated predictions defined as: H[˜p k(y|x,D)] =E w∼˜p k(w|D) [H(˜p k(y|x,w))] +I[˜p k(y,w|x,D)](4) This framework provides a decomposition that respects ensemble variability and remains consistent with information-theoretic formulations (Eq. 2vs.Eq. 4). Using the standard uncertainty decomposi- tion, epistemic uncertainty is obtained as the difference between total predictive uncertainty and the expected aleatoric uncertainty. 2 3 Variance-Gated Margin Uncertainty The reliable estimation of uncertainty demands methods that goes beyond ensemble means. Various strategies have met this requirement using entropy, including our proposed method that uses a",
    "decomposi- tion, epistemic uncertainty is obtained as the difference between total predictive uncertainty and the expected aleatoric uncertainty. 2 3 Variance-Gated Margin Uncertainty The reliable estimation of uncertainty demands methods that goes beyond ensemble means. Various strategies have met this requirement using entropy, including our proposed method that uses a combination of variance and entropy decomposition. However, entropy is known to overestimate uncertainty when probability values are spread across many classes and underestimate when a model is highly confident between a few options. Therefore, entropy alone is not sufficient to provide adequate information for decision-making. We posit the following question: Can we identify a measure that is sensitive to class separation, incorporate uncertainty awareness, while avoiding the use of an entropy-based framework? We want to identify a metric that 1) maintains epistemic awareness and 2) ideally, provides a user the ability to adjust risk-tolerance. We propose to use the confidence margin of the top-2 mean predictions and corresponding standard deviations. This approach is an extension of the Best-versus-Second Best (BvSB) introduced by Joshi et al. [10], where we incorporate variance along with a user-defined sensitivity hyperparameter k. Let iandjdenote the top-1 and top-2 ranked classes by µ(·), we define a prediction rule ˆyand derive a SNR as: SNR=µ(i)−µ(j) σ(i) +σ(j) +ϵ> k,ˆy=\u001aiifµ(i)−kσ(i)> µ(j) +kσ(j) uncertain otherwise(5) where µ(i)−µ(j) is the probability margin between the two most likely classes and σ(i)+σ(j) +ϵ is the combined predictive variance, with ϵ >0 (e.g., 10−8) to ensures numerical stability. In principle, the SNR can be interpreted as a binary decision boundary between classes iandj, restricted by k. Under mild distribution assumptions, a user-defined threshold value of kcan be applied to reflect the fraction of samples requiring abstentions or human interventions. For example, when k= 1 , only samples with SNR>1 will be considered, all others are uncertain. However, this criterion fails to capture cases where a model outputs ambiguous and uncertain predictions. Such outputs artificially inflates the SNR values, leading to misleading classifications. To address this limitation, we introduce a variance-gated margin uncertainty (GMU), using a gating function that rescales model predictions by incorporating both confidence and variance (i.e., epistemic) information (Eq. 6). Γ(i, j) =\u0014 1−exp\u0012 −µ(i)−µ(j) σ(i) +σ(j) +ϵ\u0013\u0015 ,GMU= 1−µ(i)·Γ(i, j)(6) The GMU functions as a variance-gated uncertainty margin since large values corresponds to high separation of the top-1 and top-2 predictions, while small values capture situations of ambiguous and uncertain predictions. 4 Experiments In the following, we present our preliminary results for the proposed variance-gated framework using standard benchmark datasets. Experiments were performed on MNIST, SVHN, CIFAR10, and CIFAR100, using three ensemble strategies: MCD, LLE, and MCD-LLE hybrids, each with 100 ensemble members. We compared our method against standard entropy-based uncertainty decomposition and the recent information-theoretic approach by Schweighofer et al. [4], including: 1) total entropy (TU); 2) expected entropy (AU); 3) mutual information (EU); 4) expected pairwise cross-entropy (EPCE); 5) expected pairwise Kullback–Leibler divergence (EPKL); and 6) expected pairwise Jensen–Shannon divergence (EPJS). As a representative example, Figure 1 summarizes the decomposition of predictive uncertainty for the CIFAR10 dataset using LLE and",
    "including: 1) total entropy (TU); 2) expected entropy (AU); 3) mutual information (EU); 4) expected pairwise cross-entropy (EPCE); 5) expected pairwise Kullback–Leibler divergence (EPKL); and 6) expected pairwise Jensen–Shannon divergence (EPJS). As a representative example, Figure 1 summarizes the decomposition of predictive uncertainty for the CIFAR10 dataset using LLE and MCD ensembles. Additional experimental details and results for datasets, including a description for the multilabel GMU uncertainty measure, calibrations, and out-of-distribution (OOD) results for the CIFAR10 and SVHN datasets are provided in the Supporting Information. Variance-Gatedvs.Baseline MeasuresFor the MCD ensembles (panels a and b), variance-gated uncertainty estimations and baseline measures agree on which samples are the most uncertain, approximately the first 1500 samples (out of 10000). Finer-grained or risk-adverse control can be obtained by selecting specific measures and/or adjusting the sensitivity hyperparameter k. In this case, the variance-gated distributions produce decompositions that were consistently below non-gated estimations. This confirms that the gating function attenuates predictions for high-variance, 3 (LLE, M=100, D= 2.5 × 10 8) 0 500 1000 1500 2000 2500 Sorted Samples (by score) (a)0123Total UncertaintyTotal Uncertainty for CIFAR10 (MCD, M=100, D= 1.3 × 10 3) GMU TU GTU(k=1) GTU(k=2) GTU(k=3) EPCE GEPCE(k=1) GEPCE(k=2) GEPCE(k=3) 0 500 1000 1500 2000 2500 Sorted Samples (by score) (b)0.00.51.01.52.0Epistemic UncertaintyEpistemic Uncertainty for CIFAR10 (MCD, M=100, D= 1.3 × 10 3) GMU EU GEU(k=1) GEU(k=2) GEU(k=3) EPKL GEPKL(k=1) GEPKL(k=2) GEPKL(k=3) EPJS GEPJS(k=1) GEPJS(k=2) GEPJS(k=3) 0 100 200 300 Epoch (c)0.00.20.40.60.81.0Diversity×10 3Diversity Profile for CIFRA10 0 500 1000 1500 2000 2500 Sorted Samples (by score) (d)0.00.51.01.52.02.5Total UncertaintyTotal Uncertainty GMU TU GTU(k=1) GTU(k=2) GTU(k=3) EPCE GEPCE(k=1) GEPCE(k=2) GEPCE(k=3) 0 500 1000 1500 2000 2500 Sorted Samples (by score) (e)0.00.20.40.60.8Aleatoric UncertaintyAleatoric Uncertainty GMU AU GAU(k=1) GAU(k=2) GAU(k=3) 0 500 1000 1500 2000 2500 Sorted Samples (by score) (f)0.00.20.40.60.8Epistemic UncertaintyEpistemic Uncertainty GMU EU GEU(k=1) GEU(k=2) GEU(k=3) EPKL GEPKL(k=1) GEPKL(k=2) GEPKL(k=3) EPJS GEPJS(k=1) GEPJS(k=2) GEPJS(k=3)Uncertainty Decomposition for the CIFAR10 Dataset (LLE, M=100, D=2.5×108) Figure 1: Uncertainty decomposition and diversity profile results for the CIFAR10 datasets using LLE and MCD ensembles. Panels (a, b, d–f) show the decomposition of total, aleatoric, and epistemic uncertainty using variance-gated distributions (GMU, GTU, GAU, GEU, GEPCE, GEPKL, GEPJS), compared against baseline measures (TU, AU, EU, EPCE, EPKL, EPJS). Panel (c) reports the diversity profile (LLE) across training epochs, quantified as the expectation over samples iand classes c, of the variance across models ( Diversity, D=E i,c[Var M]). Uncertainty measures were normalized (panels a, b, d, f) by the largest metric to allow direct comparison across methods. low-confidence predictions and estimations converge to baselines values with decreasing variance. Our proposed GMU provided simar results, where the design of this measure is to identify ambiguous (i.e., top-2 predictions) cases where ensemble disagreement is high. Therefore, it is quite possible that GMU results with a different set of uncertain samples. This is an active area we are currently investigating. Another important feature of GMU is its computational efficiency, since there are no nested calculations of entropy or pairwise divergences. Diversity CollapseDuring our investigation, we observed that LLE networks had a tendency to collapse. While this did not occur",
    "uncertain samples. This is an active area we are currently investigating. Another important feature of GMU is its computational efficiency, since there are no nested calculations of entropy or pairwise divergences. Diversity CollapseDuring our investigation, we observed that LLE networks had a tendency to collapse. While this did not occur for all experiments, with extended training these networks converge to a single model. We demonstrate this effect in panel (c) with the evolution of ensemble diversity over training epochs. While ensembles initially maintain high variability, diversity declines as training progresses, eventually collapsing to a near-zero variance state. This indicates that despite large ensemble sizes (M=100), models can converge to similar solutions, reducing effective epistemic uncertainty. Our proposed variance-gated distribution specifically highlights this observation where increasing khas no effect on uncertainty estimations (panels d–f). As a result, standard entropy-based and divergence-based methods may underestimate uncertainty once diversity collapses, since they implicitly assume disagreement across models. Our variance-gated framework and GMU make this collapse explicit. With decreasing diversity, gated estimations converge toward baseline predictions, signaling that the epistemic component has eroded. 5 Conclusions We proposed variance-gated distributions as a complementary approach for uncertainty decomposition and estimation. Our results reveal a collapse of ensemble diversity during training, which variance- gated measures make explicit, providing a diagnostic tool for ensemble diversity and when diversity- preserving strategies may be required. 4 References [1]Yarin Gal and Zoubin Ghahramani. Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning. InInternational Conference on Machine Learning (ICML), 2016. doi: 10.48550/arXiv.1506.02142. [2]Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles. InConference on Neural Information Processing Systems (NIPS), 2017. doi: 10.48550/arXiv.1612.01474. [3]H. Martin Gillis, Isaac Xu, Benjamin Misiuk, Craig J. Brown, and Thomas Trappenberg. Last-layer committee machines for uncertainty estimations of benthic imagery, 2025. [4]Kajetan Schweighofer, Lukas Aichberger, Mykyta Ielanskyi, and Sepp Hochreiter. Introducing an Improved Information-Theoretic Measure of Predictive Uncertainty. InNeural Information Processing Systems (NeurIPS), Mathematics of Modern Machine Learning Workshop, 2023. doi: 10.48550/arXiv.2311.08309. [5]Stefan Lee, Senthil Purushwalkam, Michael Cogswell, David Crandall, and Dhruv Batra. Why M Heads are Better than One: Training a Diverse Ensemble of Deep Networks, 2015. [6]Lisa Wimmer, Yusuf Sale, Paul Hofman, Bern Bischl, and Eyke Hüllermeier. Quantifying Aleatoric and Epistemic Uncertainty in Machine Learning: Are Conditional Entropy and Mutual Information Appropriate Measures? InConference on Uncertainty in Artificial Intelligence, 2023. doi: 10.48550/arXiv.2209.03302. [7]Andreas Kirsch. (Implicit) Ensembles of Ensembles: Epistemic Uncertainty Collapse in Large Models.Transactions on Machine Learning Research, 2025. doi: 10.48550/arXiv.2409.02628. [8]Vardan Papyan, X. Y . Han, and David L. Donoho. Prevalence of neural collapse during the terminal phase of deep learning training.Proceedings of the National Academy of Sciences, 117 (40):24652–24663, 2020. doi: 10.1073/pnas.2015509117. [9]Sophie Steger, Christian Knoll, Bernhard Klein, Holger Fröning, and Franz Pernkopf. Function Space Diversity for Uncertainty Prediction via Repulsive Last-Layer Ensembles, 2024. [10] Ajay J. Joshi, Fatih Porikli, and Nikolaos Papanikolopoulos. Multi-class active learning for image classification. InCVPR, pages 2372–2379, 2009. doi: 10.1109/CVPR.2009.5206627. [11] Neil Houlsby, Ferenc Huszár, Zoubin Ghahramani, and Máté Lengyel. Bayesian Active Learning for Classification and Preference Learning, 2011. [12] Gilles Louppe and",
    "Prediction via Repulsive Last-Layer Ensembles, 2024. [10] Ajay J. Joshi, Fatih Porikli, and Nikolaos Papanikolopoulos. Multi-class active learning for image classification. InCVPR, pages 2372–2379, 2009. doi: 10.1109/CVPR.2009.5206627. [11] Neil Houlsby, Ferenc Huszár, Zoubin Ghahramani, and Máté Lengyel. Bayesian Active Learning for Classification and Preference Learning, 2011. [12] Gilles Louppe and Manoj Kumar. Bayesian optimization withskopt, 2016. [13] Sergey Zagoruyko and Nikos Komodakis. Wide Residual Networks, 2017. 5 Supporting Information Uncertainty Estimation using Variance-Gated Distributions H. Martin Gillis, Isaac Xu, and Thomas Trappenberg Faculty of Computer Science, Dalhousie University, 6050 University Avenue, Halifax, NS B3H 4R2, Canada E-mail: tt@cs.dal.ca (Thomas Trappenberg) Table of Contents S1 Variance-Gated Predictive Uncertainty Decomposition. . . . . . . . . . . . . . . . .S2 S1.1 Framework Definition and Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . S2 S1.2 Uncertainty Decomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . S2 S1.3 Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . S2 S1.4 Summary of Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . S3 S2 Variance-Gated Margin Uncertainty: Multiclass. . . . . . . . . . . . . . . . . . . .S3 S2.1 Framework Definition and Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . S3 S2.2 Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . S3 S2.3 Summary of Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . S4 S3 Variance-Gated Margin Uncertainty: Multilabel. . . . . . . . . . . . . . . . . . . .S4 S3.1 Framework Definition and Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . S4 S3.2 Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . S5 S3.3 Summary of Equations . . .",
    "S4 S3.2 Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . S5 S3.3 Summary of Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . S5 S4 Experimental Details. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .S5 S4.1 Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . S5 S4.2 Network Architectures, Training, and Calibration . . . . . . . . . . . . . . . . . . S5 S5 Additional Experimental Results. . . . . . . . . . . . . . . . . . . . . . . . . . . . .S6 S5.1 MNIST . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . S7 S5.2 SVHN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . S8 S5.3 CIFAR10 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . S12 S5.4 CIFAR100 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . S16 S1 S1 Variance-Gated Predictive Uncertainty Decomposition S1.1 Framework Definition and Setup We consider a multiclass classification setting where predictions are estimated from an ensemble model. For each inputx, we define (per-class): µ(y) =1 MMX m=1p(y|x,w m)(S1) σ(y) =vuut1 MMX m=1(p(y|x,w m)−µ(y))2(S2) where µ(y) is the ensemble mean prediction for class yandσ(y) is the corresponding predictive standard deviation. We then define avariance-gating functionas: Γk(y) =\u0014 1−exp\u0012 −µ(y) kσ(y) +ϵ\u0013\u0015 (S3) where k >0 is a tunable sensitivity hyperparameter and ϵ >0 (e.g., 10−8) ensures numerical stability. Thevariance-gated normalized predictive distributionis then defined as: ˜pm,k=pm(y)·Γ k(y)P jpm(j)·Γ k(j)(S4) S1.2 Uncertainty Decomposition Using Shannon entropy, we define the following: Total uncertainty (TU): T U˜pk=H[˜p k] =−X y˜pk(y) log ˜p k(y)(S5) Entropy of the variance-gated distribution quantifies total",
    "tunable sensitivity hyperparameter and ϵ >0 (e.g., 10−8) ensures numerical stability. Thevariance-gated normalized predictive distributionis then defined as: ˜pm,k=pm(y)·Γ k(y)P jpm(j)·Γ k(j)(S4) S1.2 Uncertainty Decomposition Using Shannon entropy, we define the following: Total uncertainty (TU): T U˜pk=H[˜p k] =−X y˜pk(y) log ˜p k(y)(S5) Entropy of the variance-gated distribution quantifies total predictive uncertainty after variance-gated corrections. Aleatoric uncertainty (AU): AU˜pk=Ew∼˜p k(w|D) [H(˜p k(y|x,w))] =1 MMX m=1H[˜p k(y|x,w m)](S6) This is the aleatoric uncertainty that captures irreducible data noise and ambiguity. It is the stan- dard formulation used by Houlsby et al. [11], Gal and Ghahramani [1], Lakshminarayanan et al. [2] and Schweighofer et al. [4]. Epistemic uncertainty (EU): EU˜pk=T U ˜pk− AU ˜pk (S7) This is the residual model uncertainty after removing ensemble variability with the variance-gated distribution. It captures the reduction of confidence due to ensemble disagreement (i.e., epistemic). S1.3 Properties NormalizationThe gating function ensures that ˜pm,kis a valid probability distribution given, pm(y)·Γ k(y)≥0andP y˜pm,k(y) = 1. S2 Limit Behavior lim µ(y)→1 kσ(y)→0+pm(y)×\u0014 1−exp\u0012 −1 ϵ\u0013\u0015 ≈pm(y)(confident, certain) lim µ(y)→1 kσ(y)→∞pm(y)×\u0014 1−exp\u0012 −1 ∞\u0013\u0015 = 0(confident, uncertain) lim µ(y)→0+ kσ(y)→0+pm(y)×\u0014 1−exp\u0012 −0+ ϵ\u0013\u0015 = 0(ambiguous, certain) lim µ(y)→0+ kσ(y)→∞pm(y)×\u0014 1−exp\u0012 −0+ ∞\u0013\u0015 = 0(ambiguous, uncertain) BoundednessFor all k >0 ,µ(y)∈[0,1] , andσ(y)≥0 , the gating function is bounded such that, 0≤p m(y)·Γ k(y)< p m(y). This bound ensures that uncertainty suppresses confidence without exceeding ensemble agreement. Distributional AssumptionThe variance-gated formulation assumes an approximate normal distribution of class probabilities across model predictions. However, other distributional models can be employed and implemented (e.g., Gaussian mixture models). S1.4 Summary of Equations Γk(y) =\u0014 1−exp\u0012 −µ(y) kσ(y) +ϵ\u0013\u0015 T U˜pk=−X y˜pk(y) log ˜p k(y) ˜pm,k=pm(y)·Γ k(y)P jpm(j)·Γ k(j)AU˜pk=1 MMX m=1H[˜p k(y|x,w m)] EU˜pk=T U ˜pk− AU ˜pk S2 Variance-Gated Margin Uncertainty: Multiclass S2.1 Framework Definition and Setup As discussed in the main text. S2.2 Properties Limit BehaviorWe analyze the behavior under four distinct limit regimes, defined by whether the mean difference tends to 1 or 0 and whether the uncertainty tends to∞or 0. lim µ(i)−µ(j)→1 σ(i)+σ(j)→0+µ(i)×\u0014 1−exp\u0012 −1 ϵ\u0013\u0015 ≈µ(i)(confident, certain) lim µ(i)−µ(j)→1 σ(i)+σ(j)→∞µ(i)×\u0014 1−exp\u0012 −1 ∞\u0013\u0015 = 0(confident, uncertain) lim µ(i)−µ(j)→0+ σ(i)+σ(j)→0+µ(i)×\u0014 1−exp\u0012 −0+ ϵ\u0013\u0015 = 0(ambiguous, certain) lim µ(i)−µ(j)→0+ σ(i)+σ(j)→∞µ(i)×\u0014 1−exp\u0012 −0+ ∞\u0013\u0015 = 0(ambiguous, uncertain) S3 BoundednessFor all µ(i)−µ(j)∈[0,1] andσ(i) +σ(j)≥0 , the gating function Γ(i, j) satisfies 0≤Γ(i, j)<1 =⇒0≤µ(i)·Γ(i, j)< µ(i) This guarantees that the confidence gate only reduces the predicted score relative to the ensemble model raw agreementsµ(i). Distributional AssumptionAs noted for the variance-gated normalized predictive distribution, the GMU formulation assumes an approximate normal distribution. Here again, alternative distributional models can be validated and implemented with the framework. S2.3 Summary of Equations ˆy=\u001aiifµ(i)−kσ(i)> µ(j) +kσ(j) uncertain otherwise SNR=µ(i)−µ(j) σ(i) +σ(j) +ϵ> k Γ(i, j) =\u0014 1−exp\u0012 −µ(i)−µ(j) σ(i) +σ(j) +ϵ\u0013\u0015 GMU= 1−µ(i)·Γ(i, j) S3 Variance-Gated Margin Uncertainty: Multilabel S3.1 Framework Definition and Setup We consider the a multilabel classification setting, where predictions are estimated from an ensemble model (Eq. S1 and Eq. S2) and derive a variance-gating function based on the signal-to-noise ratio (SNR) of the top-2 predictions. In this case, the top-1 prediction is defined as µ(i) = max(u(i),1−",
    "Definition and Setup We consider the a multilabel classification setting, where predictions are estimated from an ensemble model (Eq. S1 and Eq. S2) and derive a variance-gating function based on the signal-to-noise ratio (SNR) of the top-2 predictions. In this case, the top-1 prediction is defined as µ(i) = max(u(i),1− u(i)) and the corresponding top-2 prediction is the complementary value µ(j) = 1−µ(i) , such the µ(i) +µ(j) = 1andσ(i) =σ(j). We define a prediction rule and derive a SNR as: SNR=2µ(i)−1 2σ(i) +ϵ> k,ˆy=\u001aiifµ(i)−kσ(i)>(1−µ(i)) +kσ(i) uncertain otherwise(S8) where 2µ(i)−1 is the probability margin between the top-1 prediction with its complement and 2σ(i) +ϵ is the combined predictive spread, with ϵ >0 (e.g., 10−8) to ensures numerical stability. This is exactly analogous to the multiclass setting; however, the top-2 predictions are derived from a single multilabel prediction. For a multilabel setting, the SNR can be interpreted as a binary decision boundary for label ibeing present or not, restricted by k. We then define amultilabel variance-gated margin uncertainty (GMU), using a variance-gating function that rescales model predictions by incorporating both confidence and variance information (as done previously). GMU=\u001a1−µ(i)·Γ(i)ifµ(i)>0.5 µ(i)·Γ(i)otherwise,Γ(i) =\u0014 1−exp\u0012 −2µ(i)−1 2σ(i) +ϵ\u0013\u0015 (S9) S4 S3.2 Properties Limit Behavior lim µ(i)→1 σ(i)→0+µ(i)×\u0014 1−exp\u0012 −1 ϵ\u0013\u0015 ≈µ(i)(confident, certain) lim µ(i)→1 σ(i)→∞µ(i)×\u0014 1−exp\u0012 −1 ∞\u0013\u0015 = 0(confident, uncertain) lim µ(i)→0+ σ(i)→0+µ(i)×\u0014 1−exp\u0012 −0+ ϵ\u0013\u0015 = 0(ambiguous, certain) lim µ(i)→0+ σ(i)→∞µ(i)×\u0014 1−exp\u0012 −0+ ∞\u0013\u0015 = 0(ambiguous, uncertain) BoundednessFor all2µ(i)−1∈[0,1]and2σ(i)≥0, the gating functionΓ(i)satisfies: 0≤Γ(i)<1 =⇒0≤µ(i)·Γ(i)or(1−µ(i))·Γ(i)< µ(i) This guarantees that the confidence gate only reduces the predicted score relative to the ensemble model raw agreementsµ(i). Distributional AssumptionSame as multiclass setting. S3.3 Summary of Equations ˆy=\u001aiifµ(i)−kσ(i)>(1−µ(i)) +kσ(i) uncertain otherwise SNR=2µ(i)−1 2σ(i) +ϵ> k Γ(i) =\u0014 1−exp\u0012 −2µ(i)−1 2σ(i) +ϵ\u0013\u0015 GMU=\u001a1−µ(i)·Γ(i)ifµ(i)>0.5 µ(i)·Γ(i)otherwise S4 Experimental Details S4.1 Datasets The MNIST, SVHN, CIFAR10/100 datasets were obtained from the PyTorch dataset repository. Dataset means and standard deviations were computed and used during training and inference. For OOD experiments, OOD samples were normalized using the in-domain (ID) means and standard deviations. S4.2 Network Architectures, Training, and Calibration MNISTFor the MNIST dataset, a CNN was used for MCD, LLE, and MCD-LLE networks. Here, the CNN consisted of a feature extraction module with two blocks of convolutions, ReLU activations, maxpooling, and a dropout layer ( p= 0.05 ). Both blocks used convolution with 128 output channels, a kernel size of 5, and a maxpooling layer with a kernel and stride size of 2. The second block was subsequently flattened before being passed to a linear layer (2048 input and 2048 output channels), ReLU activation, and a dropout layer ( p= 0.05 ). A classifier was added which consisted of a linear layer (2048 input and 10 output channels). In the case of the LLE, the classifier was converted S5 to a list of 100 classifiers. Training was done with a batch size of 128 over 100 epochs using the Adam optimizer with a learning rate of 1.0×10−5and the cross-entropy loss objective function (for each classifier). Mean classifier loss was then calculated before being backpropagated during optimization. Calibration was performed by finding the optimal temperature (i.e.,",
    "Training was done with a batch size of 128 over 100 epochs using the Adam optimizer with a learning rate of 1.0×10−5and the cross-entropy loss objective function (for each classifier). Mean classifier loss was then calculated before being backpropagated during optimization. Calibration was performed by finding the optimal temperature (i.e., scaling of logits) that minimizes the cross-entropy loss on the validation dataset. This was achieved using a pre-trained model and Bayesian optimization with Gaussian Process (GP) regression [ 12]. The optimization explored a temperature search space of 0.01–10.0, using a log-uniform prior which was configured to perform 50 evaluations of the objective function. For LLE, the temperature for each committee member was optimized separately. After finding the optimal temperature, the pre-trained model was used to perform inference on the testing dataset, scaling the logits with the optimized calibrated temperature(s). SVHN, CIFAR10, and CIFAR100The WideResNet-28-10 network architecture was used as described by Zagoruyko and Komodakis [13] with a dropout rate of p= 0.05 for SVHN and p= 0.3 for CIFAR10/100 datasets. For LLE, a list of classifiers was added as described for MNIST dataset. The SVHN dataset was trained over 100 epochs with a batch size of 128 using the Adam optimizer with a learning rate of 1.0×10−6and the cross-entropy loss objective function. A cosine annealing scheduler was applied during training using an initial and final learning rate factor of 0.1, where the rate was increased over the first 25% of the total number of epochs. CIFAR10 and CIFAR100 were trained over 250 or 350 epochs with a batch size of 128 using the Adam optimizer with a learning rate of 1.0×10−4and a weight decay of 1.0×10−5. The same learning rate scheduling strategy was applied and for the SVHN dataset. Network calibrations for SVHN, CIFAR10, and CIFAR100 were done as described for the MNIST dataset. S5 Additional Experimental Results This section presents supplementary experimental results across multiple benchmark datasets, pro- viding a broader validation of our methods. We provide an overview of performance in Table S1, followed by analyses for the MNIST, SVHN, CIFAR10, and CIFAR100 datasets. For each dataset, we include results using different ensemble and calibration settings, such as MCD, MCD-LLE, LLE, and LLE (calibrated). The figures illustrate how the proposed framework behaves across increasingly complex tasks, identifying collapse, and the impact of calibration on predictive uncertainty estimates. These results demonstrate that the variance-gated approach generalizes across diverse data and sup- ports the main claims of the paper. Table S1: Performance and calibration metrics for ensemble models. Dataset Ensemble Accuracy F1-score ECE1 MNIST MCD 0.992 0.992 0.002 MCD-LLE 0.994 0.994 0.003 LLE 0.994 0.994 0.002 LLE (calibrated) 0.994 0.994 0.002 SVHN MCD 0.920 0.913 0.030 MCD-LLE 0.924 0.916 0.051 LLE 0.992 0.911 0.019 LLE (calibrated) 0.921 0.911 0.007 CIFAR10 MCD 0.956 0.956 0.017 MCD-LLE 0.957 0.957 0.014 LLE 0.957 0.957 0.025 LLE (calibrated) 0.957 0.957 0.006 CIFAR100 MCD 0.771 0.771 0.075 MCD-LLE 0.775 0.775 0.049 LLE 0.772 0.770 0.105 LLE (calibrated) 0.772 0.770 0.025 1Expected calibration error. S6 S5.1 MNIST 0 2000 4000 6000 8000 10000 Sorted Samples (by score)0.00.51.01.52.02.5Total UncertaintyTotal",
    "0.956 0.017 MCD-LLE 0.957 0.957 0.014 LLE 0.957 0.957 0.025 LLE (calibrated) 0.957 0.957 0.006 CIFAR100 MCD 0.771 0.771 0.075 MCD-LLE 0.775 0.775 0.049 LLE 0.772 0.770 0.105 LLE (calibrated) 0.772 0.770 0.025 1Expected calibration error. S6 S5.1 MNIST 0 2000 4000 6000 8000 10000 Sorted Samples (by score)0.00.51.01.52.02.5Total UncertaintyTotal Uncertainty GMU TU GTU(k=1) GTU(k=2) GTU(k=3) EPCE GEPCE(k=1) GEPCE(k=2) GEPCE(k=3) 0 2000 4000 6000 8000 10000 Sorted Samples (by score)0.00.20.40.60.81.0Aleatoric UncertaintyAleatoric Uncertainty GMU AU GAU(k=1) GAU(k=2) GAU(k=3) 0 2000 4000 6000 8000 10000 Sorted Samples (by score)0.00.20.40.60.81.0Epistemic UncertaintyEpistemic Uncertainty GMU EU GEU(k=1) GEU(k=2) GEU(k=3) EPKL GEPKL(k=1) GEPKL(k=2) GEPKL(k=3) EPJS GEPJS(k=1) GEPJS(k=2) GEPJS(k=3)Evaluation of Uncertainty Estimators for the MNIST Dataset (MCD, M=100, D=1.9×104) 0 2000 4000 6000 8000 10000 Sorted Samples (by score)0.00.51.01.52.02.5Total UncertaintyTotal Uncertainty GMU TU GTU(k=1) GTU(k=2) GTU(k=3) EPCE GEPCE(k=1) GEPCE(k=2) GEPCE(k=3) 0 2000 4000 6000 8000 10000 Sorted Samples (by score)0.00.20.40.60.81.0Aleatoric UncertaintyAleatoric Uncertainty GMU AU GAU(k=1) GAU(k=2) GAU(k=3) 0 2000 4000 6000 8000 10000 Sorted Samples (by score)0.00.20.40.60.81.0Epistemic UncertaintyEpistemic Uncertainty GMU EU GEU(k=1) GEU(k=2) GEU(k=3) EPKL GEPKL(k=1) GEPKL(k=2) GEPKL(k=3) EPJS GEPJS(k=1) GEPJS(k=2) GEPJS(k=3)Evaluation of Uncertainty Estimators for the MNIST Dataset (MCD-LLE, M=100, D=4.5×106) 0 2000 4000 6000 8000 10000 Sorted Samples (by score)0.00.51.01.52.02.5Total UncertaintyTotal Uncertainty GMU TU GTU(k=1) GTU(k=2) GTU(k=3) EPCE GEPCE(k=1) GEPCE(k=2) GEPCE(k=3) 0 2000 4000 6000 8000 10000 Sorted Samples (by score)0.00.20.40.60.8Aleatoric UncertaintyAleatoric Uncertainty GMU AU GAU(k=1) GAU(k=2) GAU(k=3) 0 2000 4000 6000 8000 10000 Sorted Samples (by score)0.00.20.40.60.8Epistemic UncertaintyEpistemic Uncertainty GMU EU GEU(k=1) GEU(k=2) GEU(k=3) EPKL GEPKL(k=1) GEPKL(k=2) GEPKL(k=3) EPJS GEPJS(k=1) GEPJS(k=2) GEPJS(k=3)Evaluation of Uncertainty Estimators for the MNIST Dataset (LLE, M=100, D=4.5×106) 0 2000 4000 6000 8000 10000 Sorted Samples (by score)0.00.51.01.52.02.5Total UncertaintyTotal Uncertainty GMU TU GTU(k=1) GTU(k=2) GTU(k=3) EPCE GEPCE(k=1) GEPCE(k=2) GEPCE(k=3) 0 2000 4000 6000 8000 10000 Sorted Samples (by score)0.00.20.40.60.8Aleatoric UncertaintyAleatoric Uncertainty GMU AU GAU(k=1) GAU(k=2) GAU(k=3) 0 2000 4000 6000 8000 10000 Sorted Samples (by score)0.00.20.40.60.8Epistemic UncertaintyEpistemic Uncertainty GMU EU GEU(k=1) GEU(k=2) GEU(k=3) EPKL GEPKL(k=1) GEPKL(k=2) GEPKL(k=3) EPJS GEPJS(k=1) GEPJS(k=2) GEPJS(k=3)Evaluation of Uncertainty Estimators for the MNIST Dataset (LLE, calibrated, M=100, D=4.1×106) Figure S1: Uncertainty decompositions for theMNISTdataset usingMCD, MCD-LLE, LLE, and LLE (calibrated)networks and variance-gated distributions, compared against baseline measures. Diversity (D) was quantified as the expectation over samples iand classes c, of the variance across models ( Ei,c[Var M]). Uncertainty measures were normalized by the largest metric to allow direct comparison across methods. S7 S5.2 SVHN 0 5000 10000 15000 20000 25000 Sorted Samples (by score)0123Total UncertaintyTotal Uncertainty GMU TU GTU(k=1) GTU(k=2) GTU(k=3) EPCE GEPCE(k=1) GEPCE(k=2) GEPCE(k=3) 0 5000 10000 15000 20000 25000 Sorted Samples (by score)0.00.20.40.60.81.0Aleatoric UncertaintyAleatoric Uncertainty GMU AU GAU(k=1) GAU(k=2) GAU(k=3) 0 5000 10000 15000 20000 25000 Sorted Samples (by score)0.00.51.01.5Epistemic UncertaintyEpistemic Uncertainty GMU EU GEU(k=1) GEU(k=2) GEU(k=3) EPKL GEPKL(k=1) GEPKL(k=2) GEPKL(k=3) EPJS GEPJS(k=1) GEPJS(k=2) GEPJS(k=3)Evaluation of Uncertainty Estimators for the SVHN Dataset (ID samples; MCD, M=100, D=1.8×103) 0 2000 4000 6000 8000 10000 Sorted Samples (by score)0.00.51.01.52.02.53.03.5Total UncertaintyGMU TU GTU(k=1) GTU(k=2) GTU(k=3) EPCE GEPCE(k=1) GEPCE(k=2) GEPCE(k=3) 0 2000 4000 6000 8000 10000 Sorted Samples (by score)0.00.20.40.60.81.0Aleatoric UncertaintyGMU AU GAU(k=1) GAU(k=2) GAU(k=3) 0 2000 4000 6000 8000 10000 Sorted Samples (by score)0.000.250.500.751.001.251.501.75Epistemic UncertaintyGMU EU GEU(k=1) GEU(k=2) GEU(k=3)",
    "D=1.8×103) 0 2000 4000 6000 8000 10000 Sorted Samples (by score)0.00.51.01.52.02.53.03.5Total UncertaintyGMU TU GTU(k=1) GTU(k=2) GTU(k=3) EPCE GEPCE(k=1) GEPCE(k=2) GEPCE(k=3) 0 2000 4000 6000 8000 10000 Sorted Samples (by score)0.00.20.40.60.81.0Aleatoric UncertaintyGMU AU GAU(k=1) GAU(k=2) GAU(k=3) 0 2000 4000 6000 8000 10000 Sorted Samples (by score)0.000.250.500.751.001.251.501.75Epistemic UncertaintyGMU EU GEU(k=1) GEU(k=2) GEU(k=3) EPKL GEPKL(k=1) GEPKL(k=2) GEPKL(k=3) EPJS GEPJS(k=1) GEPJS(k=2) GEPJS(k=3)Evaluation of Uncertainty Estimators for the CIFAR10 Dataset (OOD samples; MCD, M=100, D=8.0×103) 0.0 0.2 0.4 0.6 0.8 1.0 False Positive Rate (FPR)0.00.20.40.60.81.0True Positive Rate (TPR)Total Uncertainty GMU AUC = 0.915; FPR@95TPR = 0.239 TU AUC = 0.934; FPR@95TPR = 0.226 EPCE AUC = 0.938; FPR@95TPR = 0.221 EPCE(k=1) AUC = 0.933; FPR@95TPR = 0.234 EPCE(k=2) AUC = 0.931; FPR@95TPR = 0.236 EPCE(k=3) AUC = 0.930; FPR@95TPR = 0.238 0.0 0.2 0.4 0.6 0.8 1.0 False Positive Rate (FPR)0.00.20.40.60.81.0True Positive Rate (TPR)Aleatoric Uncertainty GMU AUC = 0.915; FPR@95TPR = 0.239 AU AUC = 0.930; FPR@95TPR = 0.232 GAU(k=1) AUC = 0.926; FPR@95TPR = 0.243 GAU(k=2) AUC = 0.925; FPR@95TPR = 0.244 GAU(k=3) AUC = 0.925; FPR@95TPR = 0.242 0.0 0.2 0.4 0.6 0.8 1.0 False Positive Rate (FPR)0.00.20.40.60.81.0True Positive Rate (TPR)Epistemic Uncertainty GMU AUC = 0.915; FPR@95TPR = 0.239 EU AUC = 0.941; FPR@95TPR = 0.209 EPKL AUC = 0.939; FPR@95TPR = 0.209 EPKL(k=1) AUC = 0.936; FPR@95TPR = 0.219 EPKL(k=2) AUC = 0.934; FPR@95TPR = 0.224 EPKL(k=3) AUC = 0.933; FPR@95TPR = 0.225 EPJS AUC = 0.938; FPR@95TPR = 0.213ROC Curves for OOD Detection Methods: ID, SVHN; OOD, CIFAR10 (MCD, M=100) Figure S2:Uncertainty decompositionsandOODfor theSVHNdataset usingMCDnetwork and variance-gated distributions, compared against baseline measures. Diversity (D) was quantified as the expectation over samples iand classes c, of the variance across models ( Ei,c[Var M]). Uncertainty measures were normalized by the largest metric to allow direct comparison across methods. S8 0 5000 10000 15000 20000 25000 Sorted Samples (by score)0123Total UncertaintyTotal Uncertainty GMU TU GTU(k=1) GTU(k=2) GTU(k=3) EPCE GEPCE(k=1) GEPCE(k=2) GEPCE(k=3) 0 5000 10000 15000 20000 25000 Sorted Samples (by score)0.00.20.40.60.81.0Aleatoric UncertaintyAleatoric Uncertainty GMU AU GAU(k=1) GAU(k=2) GAU(k=3) 0 5000 10000 15000 20000 25000 Sorted Samples (by score)0.000.250.500.751.001.25Epistemic UncertaintyEpistemic Uncertainty GMU EU GEU(k=1) GEU(k=2) GEU(k=3) EPKL GEPKL(k=1) GEPKL(k=2) GEPKL(k=3) EPJS GEPJS(k=1) GEPJS(k=2) GEPJS(k=3)Evaluation of Uncertainty Estimators for the SVHN Dataset (ID samples; MCD-LLE, M=100, D=1.5×103) 0 2000 4000 6000 8000 10000 Sorted Samples (by score)0.00.51.01.52.02.53.03.5Total UncertaintyGMU TU GTU(k=1) GTU(k=2) GTU(k=3) EPCE GEPCE(k=1) GEPCE(k=2) GEPCE(k=3) 0 2000 4000 6000 8000 10000 Sorted Samples (by score)0.00.20.40.60.81.0Aleatoric UncertaintyGMU AU GAU(k=1) GAU(k=2) GAU(k=3) 0 2000 4000 6000 8000 10000 Sorted Samples (by score)0.00.20.40.60.81.01.21.4Epistemic UncertaintyGMU EU GEU(k=1) GEU(k=2) GEU(k=3) EPKL GEPKL(k=1) GEPKL(k=2) GEPKL(k=3) EPJS GEPJS(k=1) GEPJS(k=2) GEPJS(k=3)Evaluation of Uncertainty Estimators for the CIFAR10 Dataset (OOD samples; MCD-LLE, M=100, D=6.9×103) 0.0 0.2 0.4 0.6 0.8 1.0 False Positive Rate (FPR)0.00.20.40.60.81.0True Positive Rate (TPR)Total Uncertainty GMU AUC = 0.908; FPR@95TPR = 0.278 TU AUC = 0.922; FPR@95TPR = 0.280 GTU(k=1) AUC = 0.915; FPR@95TPR = 0.306 GTU(k=2) AUC = 0.913; FPR@95TPR = 0.314 GTU(k=3) AUC = 0.913; FPR@95TPR = 0.312 EPCE AUC = 0.927; FPR@95TPR = 0.270 0.0 0.2 0.4 0.6 0.8 1.0 False Positive Rate (FPR)0.00.20.40.60.81.0True Positive Rate (TPR)Aleatoric Uncertainty GMU AUC",
    "AUC = 0.922; FPR@95TPR = 0.280 GTU(k=1) AUC = 0.915; FPR@95TPR = 0.306 GTU(k=2) AUC = 0.913; FPR@95TPR = 0.314 GTU(k=3) AUC = 0.913; FPR@95TPR = 0.312 EPCE AUC = 0.927; FPR@95TPR = 0.270 0.0 0.2 0.4 0.6 0.8 1.0 False Positive Rate (FPR)0.00.20.40.60.81.0True Positive Rate (TPR)Aleatoric Uncertainty GMU AUC = 0.908; FPR@95TPR = 0.278 AU AUC = 0.915; FPR@95TPR = 0.293 GAU(k=1) AUC = 0.909; FPR@95TPR = 0.318 GAU(k=2) AUC = 0.908; FPR@95TPR = 0.325 GAU(k=3) AUC = 0.908; FPR@95TPR = 0.323 0.0 0.2 0.4 0.6 0.8 1.0 False Positive Rate (FPR)0.00.20.40.60.81.0True Positive Rate (TPR)Epistemic Uncertainty GMU AUC = 0.908; FPR@95TPR = 0.278 EU AUC = 0.942; FPR@95TPR = 0.216 GEU(k=1) AUC = 0.938; FPR@95TPR = 0.233 GEU(k=2) AUC = 0.936; FPR@95TPR = 0.241 GEU(k=3) AUC = 0.935; FPR@95TPR = 0.245 EPKL AUC = 0.940; FPR@95TPR = 0.214 EPJS AUC = 0.939; FPR@95TPR = 0.226ROC Curves for OOD Detection Methods: ID, SVHN; OOD, CIFAR10 (MCD-LLE, M=100)Figure S3:Uncertainty decompositionsandOODfor theSVHNdataset usingMCD-LLEnetwork and variance-gated distributions, compared against baseline measures. Diversity (D) was quantified as the expectation over samples iand classes c, of the variance across models ( Ei,c[Var M]). Uncertainty measures were normalized by the largest metric to allow direct comparison across methods. S9 0 5000 10000 15000 20000 25000 Sorted Samples (by score)0.00.51.01.52.02.53.0Total UncertaintyTotal Uncertainty GMU TU GTU(k=1) GTU(k=2) GTU(k=3) EPCE GEPCE(k=1) GEPCE(k=2) GEPCE(k=3) 0 5000 10000 15000 20000 25000 Sorted Samples (by score)0.00.20.40.60.81.0Aleatoric UncertaintyAleatoric Uncertainty GMU AU GAU(k=1) GAU(k=2) GAU(k=3) 0 5000 10000 15000 20000 25000 Sorted Samples (by score)0.00.20.40.60.81.0Epistemic UncertaintyEpistemic Uncertainty GMU EU GEU(k=1) GEU(k=2) GEU(k=3) EPKL GEPKL(k=1) GEPKL(k=2) GEPKL(k=3) EPJS GEPJS(k=1) GEPJS(k=2) GEPJS(k=3)Evaluation of Uncertainty Estimators for the SVHN Dataset (ID samples; LLE, M=100, D=4.2×105) 0 2000 4000 6000 8000 10000 Sorted Samples (by score)0.00.51.01.52.02.53.0Total UncertaintyGMU TU GTU(k=1) GTU(k=2) GTU(k=3) EPCE GEPCE(k=1) GEPCE(k=2) GEPCE(k=3) 0 2000 4000 6000 8000 10000 Sorted Samples (by score)0.00.20.40.60.81.0Aleatoric UncertaintyGMU AU GAU(k=1) GAU(k=2) GAU(k=3) 0 2000 4000 6000 8000 10000 Sorted Samples (by score)0.00.20.40.60.81.0Epistemic UncertaintyGMU EU GEU(k=1) GEU(k=2) GEU(k=3) EPKL GEPKL(k=1) GEPKL(k=2) GEPKL(k=3) EPJS GEPJS(k=1) GEPJS(k=2) GEPJS(k=3)Evaluation of Uncertainty Estimators for the CIFAR10 Dataset (OOD samples; LLE, M=100, D=2.5×104) 0.0 0.2 0.4 0.6 0.8 1.0 False Positive Rate (FPR)0.00.20.40.60.81.0True Positive Rate (TPR)Total Uncertainty GMU AUC = 0.891; FPR@95TPR = 0.383 TU AUC = 0.899; FPR@95TPR = 0.385 GTU(k=1) AUC = 0.899; FPR@95TPR = 0.385 GTU(k=2) AUC = 0.898; FPR@95TPR = 0.390 GTU(k=3) AUC = 0.897; FPR@95TPR = 0.391 EPCE AUC = 0.899; FPR@95TPR = 0.385 0.0 0.2 0.4 0.6 0.8 1.0 False Positive Rate (FPR)0.00.20.40.60.81.0True Positive Rate (TPR)Aleatoric Uncertainty GMU AUC = 0.891; FPR@95TPR = 0.383 AU AUC = 0.899; FPR@95TPR = 0.385 GAU(k=1) AUC = 0.899; FPR@95TPR = 0.385 GAU(k=2) AUC = 0.898; FPR@95TPR = 0.390 GAU(k=3) AUC = 0.897; FPR@95TPR = 0.392 0.0 0.2 0.4 0.6 0.8 1.0 False Positive Rate (FPR)0.00.20.40.60.81.0True Positive Rate (TPR)Epistemic Uncertainty GMU AUC = 0.891; FPR@95TPR = 0.383 EU AUC = 0.912; FPR@95TPR = 0.371 GEU(k=1) AUC = 0.912; FPR@95TPR = 0.368 GEU(k=2) AUC = 0.912; FPR@95TPR = 0.368 GEU(k=3) AUC = 0.912; FPR@95TPR = 0.369 EPKL AUC = 0.912; FPR@95TPR = 0.371 EPJS AUC = 0.912;",
    "Positive Rate (TPR)Epistemic Uncertainty GMU AUC = 0.891; FPR@95TPR = 0.383 EU AUC = 0.912; FPR@95TPR = 0.371 GEU(k=1) AUC = 0.912; FPR@95TPR = 0.368 GEU(k=2) AUC = 0.912; FPR@95TPR = 0.368 GEU(k=3) AUC = 0.912; FPR@95TPR = 0.369 EPKL AUC = 0.912; FPR@95TPR = 0.371 EPJS AUC = 0.912; FPR@95TPR = 0.370ROC Curves for OOD Detection Methods: ID, SVHN; OOD, CIFAR10 (LLE, M=100)Figure S4:Uncertainty decompositionsandOODfor theSVHNdataset usingLLEnetwork and variance-gated distributions, compared against baseline measures. Diversity (D) was quantified as the expectation over samples iand classes c, of the variance across models ( Ei,c[Var M]). Uncertainty measures were normalized by the largest metric to allow direct comparison across methods. S10 0 5000 10000 15000 20000 25000 Sorted Samples (by score)0.00.51.01.52.02.53.0Total UncertaintyTotal Uncertainty GMU TU GTU(k=1) GTU(k=2) GTU(k=3) EPCE GEPCE(k=1) GEPCE(k=2) GEPCE(k=3) 0 5000 10000 15000 20000 25000 Sorted Samples (by score)0.00.20.40.60.81.0Aleatoric UncertaintyAleatoric Uncertainty GMU AU GAU(k=1) GAU(k=2) GAU(k=3) 0 5000 10000 15000 20000 25000 Sorted Samples (by score)0.00.20.40.60.81.0Epistemic UncertaintyEpistemic Uncertainty GMU EU GEU(k=1) GEU(k=2) GEU(k=3) EPKL GEPKL(k=1) GEPKL(k=2) GEPKL(k=3) EPJS GEPJS(k=1) GEPJS(k=2) GEPJS(k=3)Evaluation of Uncertainty Estimators for the SVHN Dataset (ID samples; LLE, calibrated, M=100, D=4.5×105) 0 2000 4000 6000 8000 10000 Sorted Samples (by score)0.00.51.01.52.02.53.0Total UncertaintyGMU TU GTU(k=1) GTU(k=2) GTU(k=3) EPCE GEPCE(k=1) GEPCE(k=2) GEPCE(k=3) 0 2000 4000 6000 8000 10000 Sorted Samples (by score)0.00.20.40.60.81.0Aleatoric UncertaintyGMU AU GAU(k=1) GAU(k=2) GAU(k=3) 0 2000 4000 6000 8000 10000 Sorted Samples (by score)0.00.20.40.60.81.0Epistemic UncertaintyGMU EU GEU(k=1) GEU(k=2) GEU(k=3) EPKL GEPKL(k=1) GEPKL(k=2) GEPKL(k=3) EPJS GEPJS(k=1) GEPJS(k=2) GEPJS(k=3)Evaluation of Uncertainty Estimators for the CIFAR10 Dataset (OOD samples; LLE, calibrated, M=100, D=2.8×104) 0.0 0.2 0.4 0.6 0.8 1.0 False Positive Rate (FPR)0.00.20.40.60.81.0True Positive Rate (TPR)Total Uncertainty GMU AUC = 0.890; FPR@95TPR = 0.383 TU AUC = 0.899; FPR@95TPR = 0.384 GTU(k=1) AUC = 0.898; FPR@95TPR = 0.384 GTU(k=2) AUC = 0.897; FPR@95TPR = 0.386 GTU(k=3) AUC = 0.896; FPR@95TPR = 0.388 EPCE AUC = 0.899; FPR@95TPR = 0.384 0.0 0.2 0.4 0.6 0.8 1.0 False Positive Rate (FPR)0.00.20.40.60.81.0True Positive Rate (TPR)Aleatoric Uncertainty GMU AUC = 0.890; FPR@95TPR = 0.383 AU AUC = 0.898; FPR@95TPR = 0.384 GAU(k=1) AUC = 0.898; FPR@95TPR = 0.385 GAU(k=2) AUC = 0.897; FPR@95TPR = 0.387 GAU(k=3) AUC = 0.896; FPR@95TPR = 0.389 0.0 0.2 0.4 0.6 0.8 1.0 False Positive Rate (FPR)0.00.20.40.60.81.0True Positive Rate (TPR)Epistemic Uncertainty GMU AUC = 0.890; FPR@95TPR = 0.383 EU AUC = 0.912; FPR@95TPR = 0.366 GEU(k=1) AUC = 0.912; FPR@95TPR = 0.366 GEU(k=2) AUC = 0.912; FPR@95TPR = 0.367 GEU(k=3) AUC = 0.911; FPR@95TPR = 0.367 EPKL AUC = 0.912; FPR@95TPR = 0.366 EPJS AUC = 0.912; FPR@95TPR = 0.365ROC Curves for OOD Detection Methods: ID, SVHN; OOD, CIFAR10 (LLE, calibrated, M=100)Figure S5:Uncertainty decompositionsandOODfor theSVHNdataset usingLLE, calibrated network and variance-gated distributions, compared against baseline measures. Diversity (D) was quantified as the expectation over samples iand classes c, of the variance across models ( Ei,c[Var M]). Uncertainty measures were normalized by the largest metric to allow direct comparison across methods S11 S5.3 CIFAR10 0 2000 4000 6000 8000 10000 Sorted Samples (by score)0123Total UncertaintyTotal Uncertainty GMU TU GTU(k=1) GTU(k=2) GTU(k=3) EPCE GEPCE(k=1) GEPCE(k=2) GEPCE(k=3) 0 2000 4000 6000 8000",
    "the variance across models ( Ei,c[Var M]). Uncertainty measures were normalized by the largest metric to allow direct comparison across methods S11 S5.3 CIFAR10 0 2000 4000 6000 8000 10000 Sorted Samples (by score)0123Total UncertaintyTotal Uncertainty GMU TU GTU(k=1) GTU(k=2) GTU(k=3) EPCE GEPCE(k=1) GEPCE(k=2) GEPCE(k=3) 0 2000 4000 6000 8000 10000 Sorted Samples (by score)0.00.20.40.60.81.0Aleatoric UncertaintyAleatoric Uncertainty GMU AU GAU(k=1) GAU(k=2) GAU(k=3) 0 2000 4000 6000 8000 10000 Sorted Samples (by score)0.00.51.01.52.0Epistemic UncertaintyEpistemic Uncertainty GMU EU GEU(k=1) GEU(k=2) GEU(k=3) EPKL GEPKL(k=1) GEPKL(k=2) GEPKL(k=3) EPJS GEPJS(k=1) GEPJS(k=2) GEPJS(k=3)Evaluation of Uncertainty Estimators for the CIFAR10 Dataset (ID samples; MCD, M=100, D=1.3×103) 0 5000 10000 15000 20000 25000 Sorted Samples (by score)0.00.51.01.52.02.53.03.5Total UncertaintyGMU TU GTU(k=1) GTU(k=2) GTU(k=3) EPCE GEPCE(k=1) GEPCE(k=2) GEPCE(k=3) 0 5000 10000 15000 20000 25000 Sorted Samples (by score)0.00.20.40.60.81.0Aleatoric UncertaintyGMU AU GAU(k=1) GAU(k=2) GAU(k=3) 0 5000 10000 15000 20000 25000 Sorted Samples (by score)0.00.51.01.52.0Epistemic UncertaintyGMU EU GEU(k=1) GEU(k=2) GEU(k=3) EPKL GEPKL(k=1) GEPKL(k=2) GEPKL(k=3) EPJS GEPJS(k=1) GEPJS(k=2) GEPJS(k=3)Evaluation of Uncertainty Estimators for the SVHN Dataset (OOD samples; MCD, M=100, D=8.6×103) 0.0 0.2 0.4 0.6 0.8 1.0 False Positive Rate (FPR)0.00.20.40.60.81.0True Positive Rate (TPR)Total Uncertainty GMU AUC = 0.931; FPR@95TPR = 0.178 TU AUC = 0.943; FPR@95TPR = 0.176 EPCE AUC = 0.935; FPR@95TPR = 0.179 EPCE(k=1) AUC = 0.935; FPR@95TPR = 0.174 EPCE(k=2) AUC = 0.934; FPR@95TPR = 0.174 EPCE(k=3) AUC = 0.934; FPR@95TPR = 0.173 0.0 0.2 0.4 0.6 0.8 1.0 False Positive Rate (FPR)0.00.20.40.60.81.0True Positive Rate (TPR)Aleatoric Uncertainty GMU AUC = 0.931; FPR@95TPR = 0.178 AU AUC = 0.949; FPR@95TPR = 0.171 GAU(k=1) AUC = 0.947; FPR@95TPR = 0.169 GAU(k=2) AUC = 0.945; FPR@95TPR = 0.169 GAU(k=3) AUC = 0.944; FPR@95TPR = 0.169 0.0 0.2 0.4 0.6 0.8 1.0 False Positive Rate (FPR)0.00.20.40.60.81.0True Positive Rate (TPR)Epistemic Uncertainty GMU AUC = 0.931; FPR@95TPR = 0.178 EU AUC = 0.922; FPR@95TPR = 0.190 EPKL AUC = 0.918; FPR@95TPR = 0.192 EPKL(k=1) AUC = 0.922; FPR@95TPR = 0.184 EPKL(k=2) AUC = 0.922; FPR@95TPR = 0.183 EPKL(k=3) AUC = 0.923; FPR@95TPR = 0.183 EPJS AUC = 0.924; FPR@95TPR = 0.184ROC Curves for OOD Detection Methods: ID, CIFAR10; OOD, SVHN (MCD, M=100) Figure S6:Uncertainty decompositionsandOODfor theCIFAR10dataset usingMCDnetwork and variance-gated distributions, compared against baseline measures. Diversity (D) was quantified as the expectation over samples iand classes c, of the variance across models ( Ei,c[Var M]). Uncertainty measures were normalized by the largest metric to allow direct comparison across methods S12 0 2000 4000 6000 8000 10000 Sorted Samples (by score)0.00.51.01.52.02.53.0Total UncertaintyTotal Uncertainty GMU TU GTU(k=1) GTU(k=2) GTU(k=3) EPCE GEPCE(k=1) GEPCE(k=2) GEPCE(k=3) 0 2000 4000 6000 8000 10000 Sorted Samples (by score)0.00.20.40.60.81.0Aleatoric UncertaintyAleatoric Uncertainty GMU AU GAU(k=1) GAU(k=2) GAU(k=3) 0 2000 4000 6000 8000 10000 Sorted Samples (by score)0.000.250.500.751.001.251.50Epistemic UncertaintyEpistemic Uncertainty GMU EU GEU(k=1) GEU(k=2) GEU(k=3) EPKL GEPKL(k=1) GEPKL(k=2) GEPKL(k=3) EPJS GEPJS(k=1) GEPJS(k=2) GEPJS(k=3)Evaluation of Uncertainty Estimators for the CIFAR10 Dataset (ID samples; MCD-LLE, M=100, D=1.2×103) 0 5000 10000 15000 20000 25000 Sorted Samples (by score)0.00.51.01.52.02.53.03.5Total UncertaintyGMU TU GTU(k=1) GTU(k=2) GTU(k=3) EPCE GEPCE(k=1) GEPCE(k=2) GEPCE(k=3) 0 5000 10000 15000 20000 25000 Sorted Samples (by score)0.00.20.40.60.81.0Aleatoric UncertaintyGMU AU GAU(k=1) GAU(k=2) GAU(k=3) 0 5000 10000 15000 20000 25000 Sorted Samples (by",
    "CIFAR10 Dataset (ID samples; MCD-LLE, M=100, D=1.2×103) 0 5000 10000 15000 20000 25000 Sorted Samples (by score)0.00.51.01.52.02.53.03.5Total UncertaintyGMU TU GTU(k=1) GTU(k=2) GTU(k=3) EPCE GEPCE(k=1) GEPCE(k=2) GEPCE(k=3) 0 5000 10000 15000 20000 25000 Sorted Samples (by score)0.00.20.40.60.81.0Aleatoric UncertaintyGMU AU GAU(k=1) GAU(k=2) GAU(k=3) 0 5000 10000 15000 20000 25000 Sorted Samples (by score)0.000.250.500.751.001.251.501.75Epistemic UncertaintyGMU EU GEU(k=1) GEU(k=2) GEU(k=3) EPKL GEPKL(k=1) GEPKL(k=2) GEPKL(k=3) EPJS GEPJS(k=1) GEPJS(k=2) GEPJS(k=3)Evaluation of Uncertainty Estimators for the CIFAR10 Dataset (OOD samples; MCD-LLE, M=100, D=6.1×103) 0.0 0.2 0.4 0.6 0.8 1.0 False Positive Rate (FPR)0.00.20.40.60.81.0True Positive Rate (TPR)Total Uncertainty GMU AUC = 0.889; FPR@95TPR = 0.337 TU AUC = 0.897; FPR@95TPR = 0.339 GTU(k=1) AUC = 0.896; FPR@95TPR = 0.341 GTU(k=2) AUC = 0.895; FPR@95TPR = 0.343 GTU(k-3) AUC = 0.894; FPR@95TPR = 0.344 EPCE AUC = 0.891; FPR@95TPR = 0.338 0.0 0.2 0.4 0.6 0.8 1.0 False Positive Rate (FPR)0.00.20.40.60.81.0True Positive Rate (TPR)Aleatoric Uncertainty GMU AUC = 0.889; FPR@95TPR = 0.337 AU AUC = 0.902; FPR@95TPR = 0.338 GAU(k=1) AUC = 0.900; FPR@95TPR = 0.342 GAU(k=2) AUC = 0.899; FPR@95TPR = 0.344 GAU(k=3) AUC = 0.898; FPR@95TPR = 0.344 0.0 0.2 0.4 0.6 0.8 1.0 False Positive Rate (FPR)0.00.20.40.60.81.0True Positive Rate (TPR)Epistemic Uncertainty GMU AUC = 0.889; FPR@95TPR = 0.337 EU AUC = 0.879; FPR@95TPR = 0.332 GEU(k=1) AUC = 0.883; FPR@95TPR = 0.331 GEU(k=2) AUC = 0.883; FPR@95TPR = 0.331 GEU(k=3) AUC = 0.883; FPR@95TPR = 0.331 EPKL AUC = 0.877; FPR@95TPR = 0.331 EPJS AUC = 0.881; FPR@95TPR = 0.332ROC Curves for OOD Detection Methods: ID, CIFAR10; OOD, SVHN (MCD-LLE, M=100)Figure S7:Uncertainty decompositionsandOODfor theCIFAR10dataset usingMCD-LLE network and variance-gated distributions, compared against baseline measures. Diversity (D) was quantified as the expectation over samples iand classes c, of the variance across models ( Ei,c[Var M]). Uncertainty measures were normalized by the largest metric to allow direct comparison across methods S13 0 2000 4000 6000 8000 10000 Sorted Samples (by score)0.00.51.01.52.0Total UncertaintyTotal Uncertainty GMU TU GTU(k=1) GTU(k=2) GTU(k=3) EPCE GEPCE(k=1) GEPCE(k=2) GEPCE(k=3) 0 2000 4000 6000 8000 10000 Sorted Samples (by score)0.00.20.40.60.8Aleatoric UncertaintyAleatoric Uncertainty GMU AU GAU(k=1) GAU(k=2) GAU(k=3) 0 2000 4000 6000 8000 10000 Sorted Samples (by score)0.00.20.40.60.8Epistemic UncertaintyEpistemic Uncertainty GMU EU GEU(k=1) GEU(k=2) GEU(k=3) EPKL GEPKL(k=1) GEPKL(k=2) GEPKL(k=3) EPJS GEPJS(k=1) GEPJS(k=2) GEPJS(k=3)Evaluation of Uncertainty Estimators for the CIFAR10 Dataset (LLE, M=100, D=8.6×108) 0 5000 10000 15000 20000 25000 Sorted Samples (by score)0.00.51.01.52.02.5Total UncertaintyGMU TU GTU(k=1) GTU(k=2) GTU(k=3) EPCE GEPCE(k=1) GEPCE(k=2) GEPCE(k=3) 0 5000 10000 15000 20000 25000 Sorted Samples (by score)0.00.20.40.60.81.0Aleatoric UncertaintyGMU AU GAU(k=1) GAU(k=2) GAU(k=3) 0 5000 10000 15000 20000 25000 Sorted Samples (by score)0.00.20.40.60.81.0Epistemic UncertaintyGMU EU GEU(k=1) GEU(k=2) GEU(k=3) EPKL GEPKL(k=1) GEPKL(k=2) GEPKL(k=3) EPJS GEPJS(k=1) GEPJS(k=2) GEPJS(k=3)Evaluation of Uncertainty Estimators for the CIFAR10 Dataset (OOD samples; LLE, M=100, D=1.6×106) 0.0 0.2 0.4 0.6 0.8 1.0 False Positive Rate (FPR)0.00.20.40.60.81.0True Positive Rate (TPR)Total Uncertainty GMU AUC = 0.888; FPR@95TPR = 0.390 TU AUC = 0.890; FPR@95TPR = 0.393 EPCE AUC = 0.890; FPR@95TPR = 0.393 GEPCE(k=1) AUC = 0.890; FPR@95TPR = 0.393 GEPCE(k=2) AUC = 0.890; FPR@95TPR = 0.393 GEPCE(k=3) AUC = 0.890; FPR@95TPR = 0.393 0.0 0.2 0.4 0.6 0.8 1.0 False Positive Rate (FPR)0.00.20.40.60.81.0True Positive Rate",
    "FPR@95TPR = 0.390 TU AUC = 0.890; FPR@95TPR = 0.393 EPCE AUC = 0.890; FPR@95TPR = 0.393 GEPCE(k=1) AUC = 0.890; FPR@95TPR = 0.393 GEPCE(k=2) AUC = 0.890; FPR@95TPR = 0.393 GEPCE(k=3) AUC = 0.890; FPR@95TPR = 0.393 0.0 0.2 0.4 0.6 0.8 1.0 False Positive Rate (FPR)0.00.20.40.60.81.0True Positive Rate (TPR)Aleatoric Uncertainty GMU AUC = 0.888; FPR@95TPR = 0.390 AU AUC = 0.890; FPR@95TPR = 0.393 GAU(k=1) AUC = 0.890; FPR@95TPR = 0.393 GAU(k=2) AUC = 0.890; FPR@95TPR = 0.393 GAU(k=3) AUC = 0.890; FPR@95TPR = 0.393 0.0 0.2 0.4 0.6 0.8 1.0 False Positive Rate (FPR)0.00.20.40.60.81.0True Positive Rate (TPR)Epistemic Uncertainty GMU AUC = 0.888; FPR@95TPR = 0.390 EU AUC = 0.916; FPR@95TPR = 0.301 EPKL AUC = 0.922; FPR@95TPR = 0.288 GEPKL(k=1) AUC = 0.922; FPR@95TPR = 0.288 GEPKL(k=2) AUC = 0.922; FPR@95TPR = 0.288 GEPKL(k=3) AUC = 0.922; FPR@95TPR = 0.288 EPJS AUC = 0.919; FPR@95TPR = 0.294ROC Curves for OOD Detection Methods: ID, CIFAR10; OOD, SVHN (LLE, M=100)Figure S8:Uncertainty decompositionsandOODfor theCIFAR10dataset usingLLEnetwork and variance-gated distributions, compared against baseline measures. Diversity (D) was quantified as the expectation over samples iand classes c, of the variance across models ( Ei,c[Var M]). Uncertainty measures were normalized by the largest metric to allow direct comparison across methods S14 0 2000 4000 6000 8000 10000 Sorted Samples (by score)0.00.51.01.52.02.5Total UncertaintyTotal Uncertainty GMU TU GTU(k=1) GTU(k=2) GTU(k=3) EPCE GEPCE(k=1) GEPCE(k=2) GEPCE(k=3) 0 2000 4000 6000 8000 10000 Sorted Samples (by score)0.00.20.40.60.8Aleatoric UncertaintyAleatoric Uncertainty GMU AU GAU(k=1) GAU(k=2) GAU(k=3) 0 2000 4000 6000 8000 10000 Sorted Samples (by score)0.00.20.40.60.8Epistemic UncertaintyEpistemic Uncertainty GMU EU GEU(k=1) GEU(k=2) GEU(k=3) EPKL GEPKL(k=1) GEPKL(k=2) GEPKL(k=3) EPJS GEPJS(k=1) GEPJS(k=2) GEPJS(k=3)Evaluation of Uncertainty Estimators for the CIFAR10 Dataset (ID samples; LLE, calibrated, M=100, D=5.9×108) 0 5000 10000 15000 20000 25000 Sorted Samples (by score)0.00.51.01.52.02.53.0Total UncertaintyGMU TU GTU(k=1) GTU(k=2) GTU(k=3) EPCE GEPCE(k=1) GEPCE(k=2) GEPCE(k=3) 0 5000 10000 15000 20000 25000 Sorted Samples (by score)0.00.20.40.60.81.0Aleatoric UncertaintyGMU AU GAU(k=1) GAU(k=2) GAU(k=3) 0 5000 10000 15000 20000 25000 Sorted Samples (by score)0.00.20.40.60.81.0Epistemic UncertaintyGMU EU GEU(k=1) GEU(k=2) GEU(k=3) EPKL GEPKL(k=1) GEPKL(k=2) GEPKL(k=3) EPJS GEPJS(k=1) GEPJS(k=2) GEPJS(k=3)Evaluation of Uncertainty Estimators for the CIFAR10 Dataset (OOD samples; LLE, calibrated, M=100, D=8.9×107) 0.0 0.2 0.4 0.6 0.8 1.0 False Positive Rate (FPR)0.00.20.40.60.81.0True Positive Rate (TPR)Total Uncertainty GMU AUC = 0.887; FPR@95TPR = 0.414 TU AUC = 0.889; FPR@95TPR = 0.427 GTU(k=1) AUC = 0.889; FPR@95TPR = 0.427 GTU(k=2) AUC = 0.889; FPR@95TPR = 0.427 GTU(k-3) AUC = 0.889; FPR@95TPR = 0.427 EPCE AUC = 0.889; FPR@95TPR = 0.427 0.0 0.2 0.4 0.6 0.8 1.0 False Positive Rate (FPR)0.00.20.40.60.81.0True Positive Rate (TPR)Aleatoric Uncertainty GMU AUC = 0.887; FPR@95TPR = 0.414 AU AUC = 0.889; FPR@95TPR = 0.427 GAU(k=1) AUC = 0.889; FPR@95TPR = 0.427 GAU(k=2) AUC = 0.889; FPR@95TPR = 0.427 GAU(k=3) AUC = 0.889; FPR@95TPR = 0.427 0.0 0.2 0.4 0.6 0.8 1.0 False Positive Rate (FPR)0.00.20.40.60.81.0True Positive Rate (TPR)Epistemic Uncertainty GMU AUC = 0.887; FPR@95TPR = 0.414 EU AUC = 0.928; FPR@95TPR = 0.344 GEU(k=1) AUC = 0.928; FPR@95TPR = 0.344 GEU(k=2) AUC = 0.928; FPR@95TPR = 0.344 GEU(k=3) AUC = 0.928; FPR@95TPR = 0.344 EPKL AUC = 0.928; FPR@95TPR",
    "0.8 1.0 False Positive Rate (FPR)0.00.20.40.60.81.0True Positive Rate (TPR)Epistemic Uncertainty GMU AUC = 0.887; FPR@95TPR = 0.414 EU AUC = 0.928; FPR@95TPR = 0.344 GEU(k=1) AUC = 0.928; FPR@95TPR = 0.344 GEU(k=2) AUC = 0.928; FPR@95TPR = 0.344 GEU(k=3) AUC = 0.928; FPR@95TPR = 0.344 EPKL AUC = 0.928; FPR@95TPR = 0.343 EPJS AUC = 0.928; FPR@95TPR = 0.343ROC Curves for OOD Detection Methods: ID, CIFAR10; OOD, SVHN (LLE, calibrated, M=100)Figure S9:Uncertainty decompositionsandOODfor theCIFAR10dataset usingLLE, calibrated network and variance-gated distributions, compared against baseline measures. Diversity (D) was quantified as the expectation over samples iand classes c, of the variance across models ( Ei,c[Var M]). Uncertainty measures were normalized by the largest metric to allow direct comparison across methods S15 S5.4 CIFAR100 0 2000 4000 6000 8000 10000 Sorted Samples (by score)0123456Total UncertaintyTotal Uncertainty GMU TU GTU(k=1) GTU(k=2) GTU(k=3) EPCE GEPCE(k=1) GEPCE(k=2) GEPCE(k=3) 0 2000 4000 6000 8000 10000 Sorted Samples (by score)0.00.20.40.60.81.0Aleatoric UncertaintyAleatoric Uncertainty GMU AU GAU(k=1) GAU(k=2) GAU(k=3) 0 2000 4000 6000 8000 10000 Sorted Samples (by score)01234Epistemic UncertaintyEpistemic Uncertainty GMU EU GEU(k=1) GEU(k=2) GEU(k=3) EPKL GEPKL(k=1) GEPKL(k=2) GEPKL(k=3) EPJS GEPJS(k=1) GEPJS(k=2) GEPJS(k=3)Evaluation of Uncertainty Estimators for the CIFAR100 Dataset (MCD, M=100, D=6.5×104) 0 2000 4000 6000 8000 10000 Sorted Samples (by score)0123456Total UncertaintyTotal Uncertainty GMU TU GTU(k=1) GTU(k=2) GTU(k=3) EPCE GEPCE(k=1) GEPCE(k=2) GEPCE(k=3) 0 2000 4000 6000 8000 10000 Sorted Samples (by score)0.00.20.40.60.81.0Aleatoric UncertaintyAleatoric Uncertainty GMU AU GAU(k=1) GAU(k=2) GAU(k=3) 0 2000 4000 6000 8000 10000 Sorted Samples (by score)0.00.51.01.52.02.5Epistemic UncertaintyEpistemic Uncertainty GMU EU GEU(k=1) GEU(k=2) GEU(k=3) EPKL GEPKL(k=1) GEPKL(k=2) GEPKL(k=3) EPJS GEPJS(k=1) GEPJS(k=2) GEPJS(k=3)Evaluation of Uncertainty Estimators for the CIFAR100 Dataset (MCD-LLE, M=100, D=4.9×104) 0 2000 4000 6000 8000 10000 Sorted Samples (by score)012345Total UncertaintyTotal Uncertainty GMU TU GTU(k=1) GTU(k=2) GTU(k=3) EPCE GEPCE(k=1) GEPCE(k=2) GEPCE(k=3) 0 2000 4000 6000 8000 10000 Sorted Samples (by score)0.00.20.40.60.81.0Aleatoric UncertaintyAleatoric Uncertainty GMU AU GAU(k=1) GAU(k=2) GAU(k=3) 0 2000 4000 6000 8000 10000 Sorted Samples (by score)0.00.20.40.60.81.0Epistemic UncertaintyEpistemic Uncertainty GMU EU GEU(k=1) GEU(k=2) GEU(k=3) EPKL GEPKL(k=1) GEPKL(k=2) GEPKL(k=3) EPJS GEPJS(k=1) GEPJS(k=2) GEPJS(k=3)Evaluation of Uncertainty Estimators for the CIFAR100 Dataset (LLE, M=100, D=1.6×107) 0 2000 4000 6000 8000 10000 Sorted Samples (by score)012345Total UncertaintyTotal Uncertainty GMU TU GTU(k=1) GTU(k=2) GTU(k=3) EPCE GEPCE(k=1) GEPCE(k=2) GEPCE(k=3) 0 2000 4000 6000 8000 10000 Sorted Samples (by score)0.00.20.40.60.81.0Aleatoric UncertaintyAleatoric Uncertainty GMU AU GAU(k=1) GAU(k=2) GAU(k=3) 0 2000 4000 6000 8000 10000 Sorted Samples (by score)0.00.20.40.60.81.0Epistemic UncertaintyEpistemic Uncertainty GMU EU GEU(k=1) GEU(k=2) GEU(k=3) EPKL GEPKL(k=1) GEPKL(k=2) GEPKL(k=3) EPJS GEPJS(k=1) GEPJS(k=2) GEPJS(k=3)Evaluation of Uncertainty Estimators for the CIFAR100 Dataset (LLE, calibrated, M=100, D=4.1×106) Figure S10: Uncertainty decompositions for theCIFAR100dataset usingMCD, MCD-LLE, LLE, and LLE (calibrated)networks and variance-gated distributions, compared against baseline measures. Diversity (D) was quantified as the expectation over samples iand classes c, of the variance across models ( Ei,c[Var M]). Uncertainty measures were normalized by the largest metric to allow direct comparison across methods. S16",
    "allow direct comparison across methods. S16"
  ]
}