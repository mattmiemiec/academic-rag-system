{
  "filename": "2509.00056v2.pdf",
  "total_chunks": 18,
  "text_length": 58638,
  "chunks": [
    "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 14, NO. 8, DECEMBER 2024 1 MESTI-MEGANet: Micro-expression Spatio-Temporal Image and Micro-expression Gradient Attention Networks Luu Tu Nguyen∗, Vu Tram Anh Khuong∗, Thanh Ha Le∗, Thi Duyen Ngo∗ ∗Faculty of Information Technology, VNU University of Engineering and Technology, Ha Noi, Viet Nam Abstract—Micro-expression recognition (MER) is a challenging task due to the subtle and fleeting nature of micro-expressions. Traditional input modalities, such as Apex Frame, Optical Flow, and Dynamic Image, often fail to adequately capture these brief facial movements, resulting in suboptimal performance. In this study, we introduce the Micro-expression Spatio-Temporal Image (MESTI), a novel dynamic input modality that trans- forms a video sequence into a single image while preserving the essential characteristics of micro-movements. Additionally, we present the Micro-expression Gradient Attention Network (MEGANet), which incorporates a novel Gradient Attention block to enhance the extraction of fine-grained motion features from micro-expressions. By combining MESTI and MEGANet, we aim to establish a more effective approach to MER. Extensive experiments were conducted to evaluate the effectiveness of MESTI, comparing it with existing input modalities across reg- ular architectures. Moreover, we demonstrate that replacing the input of previously published MER networks with MESTI leads to consistent performance improvements. The performance of MEGANet is also evaluated, showing that our proposed network achieves state-of-the-art results on the SMIC-HS, SAMM and competitive performance on CASMEII datasets. The combina- tion of MEGANet and MESTI achieves the highest accuracy reported to date, setting a new benchmark for micro-expression recognition. These findings underscore the potential of MESTI as a superior input modality and MEGANet as an advanced recognition network, aiming to more effective MER systems in a variety of applications. Index Terms—Micro-expression, micro-expression recognition, gradient attention, micro-expression input modality, micro- expression recognition network. I. INTRODUCTION Facial expression, a vital channel of non-verbal commu- nication, encompasses two primary types: macro expressions and micro expressions. Macro expressions are typically de- liberate, easily observable, and last for an extended period, conveying a person’s emotions openly [1]. In contrast, micro- expressions (MEs) are brief, involuntary facial movements that last less than 0.5 seconds [2], [3], making them significantly challenging to control or fabricate. Unlike macro expressions, MEs reveal a person’s genuine emotions, often surfacing when one attempts to conceal their true feelings [4]. These fleeting expressions are especially revealing in high-risk situations [5], [6], where concealing emotions is common. Since these unique characteristics of MEs, they have garnered significant attention as a channel for uncovering individuals’ genuine thoughts Corresponding author: Thi Duyen Ngo (duyennt@vnu.edu.vn)and emotions. Their involuntary nature provides valuable insights, making them highly applicable in a range of critical fields. For instance, MEs play a crucial role in enhancing the accuracy of deception detection systems, providing valuable insights that more prolonged expressions may not capture [7]. In criminal investigations, law enforcement officers can assess a suspect’s truthfulness by analyzing MEs that may contradict verbal statements [8]. Beyond security applications, MEs are increasingly relevant in healthcare, particularly in clinical settings, where they can provide essential clues about a patient’s emotional state and aid medical professionals in assessing recovery progress",
    "investigations, law enforcement officers can assess a suspect’s truthfulness by analyzing MEs that may contradict verbal statements [8]. Beyond security applications, MEs are increasingly relevant in healthcare, particularly in clinical settings, where they can provide essential clues about a patient’s emotional state and aid medical professionals in assessing recovery progress [9]. Despite its potential, ME recognition (MER) presents sig- nificant challenges due to the brevity and subtle intensity of MEs. Studies have shown that even experts achieve only 47% accuracy in recognizing MEs, highlighting the inherent com- plexity of this task [12]. However, leveraging advancements in computational capabilities, as well as modern machine learn- ing and deep learning algorithms, computer-based systems for ME analysis have demonstrated significant superiority over human performance, with accuracy rates often exceeding 50%. These advancements offer a promising pathway for achieving more accurate and reliable recognition of MEs across a wide range of applications [13]. Early MER approaches primarily relied on handcrafted features and machine learning techniques. These handcrafted features can generally be classified into two main categories: those that capture variations in facial texture and those that focus on variations in facial illumination. A foundational texture-based approach utilized Local Binary Patterns on Three Orthogonal Planes (LBP-TOP) [?], extending the LBP operator [20] to capture spatiotemporal features from facial videos. Building upon LBP-TOP, various enhancements were introduced to better capture subtle dynamic texture changes, including the use of second-order Gaussian jets [21], LBP Six Intersection Points (LBP-SIP) [22], Space–Time LBP (STLBP) [23], and Space–Time Completed Local Quantized Patterns (STCLQP) [24]. These advancements aimed to refine the representation of facial texture variations over time. In contrast, for illumination-based analysis, optical flow [31] de- rived methods have been extensively studied. Directional Mean Optical Flow (MDMO) [32], Bi-weighted Oriented Optical Flow (Bi-WOOF) [33], and Fuzzy Optical Flow Directional Histograms (FHOFO) [34] have been proposed to capture subtle changes in facial illumination and motion.arXiv:2509.00056v2 [cs.CV] 7 Sep 2025 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 14, NO. 8, DECEMBER 2024 2 While these handcrafted approaches (both texture and illumination-based) provided a crucial baseline, they remained limited due to their restricted accuracy and the complex feature extraction process required. Previous surveys [13], [28] have shown that the accuracy of handcrafted features combined with machine learning ranges from approximately 40-60%, depending on the dataset and evaluation protocol used. In contrast, deep learning-based methods have demonstrated su- perior performance in MER. Currently, deep learning-based MER methods are considered state-of-the-art [13], therefore, this study focuses on leveraging deep learning approaches. Within the context of deep learning for MER, two key factors play a pivotal role in determining system performance: input modalities and network architecture [13]. Input modal- ities for MER can be broadly classified into three categories: (1) Static Images, (2) Optical Flow, and (3) Dynamic Imaging Techniques. Each modality presents distinct advantages and challenges, and their selection significantly impacts the overall effectiveness of MER systems. Static Images:Static image-based approaches often utilize the apex frame, which represents the peak intensity of a ME. This method reduces the complexity of video data by condensing an entire sequence into",
    "Techniques. Each modality presents distinct advantages and challenges, and their selection significantly impacts the overall effectiveness of MER systems. Static Images:Static image-based approaches often utilize the apex frame, which represents the peak intensity of a ME. This method reduces the complexity of video data by condensing an entire sequence into a single frame, thereby decreasing computational overhead. However, static images lack temporal information, which is crucial for identifying MEs characterized by their rapid onset and offset. The absence of these temporal cues may lead to misclassification, as the system cannot discern the subtle temporal changes critical for distinguishing MEs. Consequently, static image-based MER systems often exhibit suboptimal accuracy [14], [15]. Optical Flow:Optical flow, a widely used technique for motion representation, provides both the magnitude and di- rection of pixel movement through a two-dimensional vector field comprising horizontal and vertical flows. This method introduces temporal information into the analysis, enhancing the system’s ability to capture ME dynamics. However, the effective integration of optical flow in MER systems often requires the design of multi-stream network architectures to process both horizontal and vertical flow components. Addi- tionally, optical flow techniques are susceptible to noise caused by lighting variations, head movements, and other extraneous factors, potentially impairing recognition accuracy. [19], [34] Dynamic Imaging Methods:Dynamic imaging methods summarize a video sequence into a single image that encodes temporal information. A notable example of this technique is the Dynamic Image [36], which represents a video as a single image capturing the overall temporal essence of the sequence. This method has been shown to be effective in action recognition tasks, as demonstrated by the original authors. Subsequently, it has been adapted for ME recognition in several studies [17], [68]. However, the direct inheritance of this technique from action recognition tasks has limited its effectiveness in MER, as it struggles to accurately capture the subtle and minute facial movements characteristic of MEs. Recognizing the inadequacy of existing input representations for MER, some researchers have attempted to refine dynamic imaging techniques specifically for this domain. For instance, Affective Image [41] and Active Image [18] are examples ofefforts to tailor dynamic imaging approaches to better suit MER tasks. While these studies made progress in adapting the dynamic image concept, their input representations remain limited in comprehensiveness. Both approaches designed spe- cialized networks for their respective inputs, yet the recog- nition accuracy of these networks has remained constrained, achieving only 50–60% on four-class classification tasks, rather than the five-class standard used in earlier research. In summary, existing input modalities fail to comprehen- sively address the challenges of ME recognition. Static images, while computationally efficient, lack temporal information critical for capturing transient micro-movements. Optical flow, though effective in encoding motion, struggles to represent the subtle intensity and brevity of MEs, often resulting in noisy estimations due to their low-amplitude characteristics. These limitations underscore the pressing need for a modality that seamlessly integrates nuanced spatio-temporal motion cues into a compact and discriminative representation, tailored specifically for ME dynamics. On the other hand, although Convolutional Neural Networks (CNN) have demonstrated effectiveness in extracting spatial features from",
    "estimations due to their low-amplitude characteristics. These limitations underscore the pressing need for a modality that seamlessly integrates nuanced spatio-temporal motion cues into a compact and discriminative representation, tailored specifically for ME dynamics. On the other hand, although Convolutional Neural Networks (CNN) have demonstrated effectiveness in extracting spatial features from facial expressions, they face notable limita- tions when applied to MER. Traditional CNN architectures tend to focus on features with prominent magnitudes, often overlooking the extremely subtle and transient motion signals inherent in MEs. This underrepresentation of fine-grained mo- tion details reduces the model’s ability to distinguish between different MEs, particularly when the movements are brief and of low intensity. These challenges highlight the need for more specialized architectures and input representations that can effectively capture and amplify subtle motion cues. Previous studies have proposed various network architec- tures to address these limitations, yet significant gaps remain. For instance, MER-GCN [66] employs Graph Convolutional Networks (GCNs) to model the spatio-temporal relationships between Action Units (AUs). While this approach shows promise, it suffers from high computational complexity and limited generalization across diverse datasets such as SAMM. Additionally, the use of AUs as input modality is suboptimal, as accurately identifying AUs in real-world conditions is still highly challenging. LEARNet [63] utilizes Dynamic Image as input with a standard CNN architecture but lacks a dedicated attention mechanism tailored for micro-movements, leading to the omission of critical features. GEME [68] introduces a multi-task framework that incorporates gender information, but its feature extraction process still fails to fully leverage subtle gradient variations. AMAN [64] integrates an attention mechanism into CNNs to focus on facial regions; however, this mechanism primarily relies on raw pixel intensity rather than gradient-based motion, making it less effective in detecting low-intensity changes. Similarly, CapsNet [62] and optical flow-based methods using OFF-ApexNet [54] have shown improved accuracy but remain susceptible to noise caused by lighting variations and head movements, particularly for MEs with low amplitude. These limitations underscore the need for a more robust and efficient approach that can amplify subtle motion informations and direct attention to regions exhibiting significant gradient IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 14, NO. 8, DECEMBER 2024 3 changes. To address these challenges, in this paper, a novel MER method is proposed that introduces a dynamic input modality called the ME Spatio-Temporal Image (MESTI). This modality represents a video sequence as a single image by capturing the motion features of MEs. Furthermore, this paper introduces introduce ME Gradient Attention Networks (MEGANet), a novel network architecture that incorporates a Gradient Attention mechanism. This mechanism leverages gradient information from intermediate layers to highlight regions with subtle motion, thereby enhancing the extraction of fine-grained features that are critical for accurate ME recognition. The contributions of the paper are as follows: •A novel input modality, MESTI, which represents video sequences as single images that specifically synthesize and highlight micro-movements within ME videos. •MEGANet: By integrating a novel Gradient Attention Block and Residual Attention Block, we develop a ME network capable of focusing on motion regions, thereby improving the performance of ME recognition. •A",
    "input modality, MESTI, which represents video sequences as single images that specifically synthesize and highlight micro-movements within ME videos. •MEGANet: By integrating a novel Gradient Attention Block and Residual Attention Block, we develop a ME network capable of focusing on motion regions, thereby improving the performance of ME recognition. •A comprehensive set of experimental scenarios is de- signed to validate the effectiveness of the proposed com- ponents, achieving performance that outperforms previ- ous state-of-the-art studies. Through extensive experiments, the effectiveness of each pro- posed component (MESTI, MEGANet) is demonstrated by evaluating their individual contributions and their combination with previously published methods. The results show that each component of our proposed method enhances the performance of the ME recognition process, and when combined, MESTI and MEGANet yield a effective overall MER approach. II. PROPOSED METHOD A. Micro-expression Spatio-Temporal Image The initial idea for creating an effective input representation for ME stemmed from observing and studying the motion characteristics of MEs. The intensity of motion gradually increases from the onset frame (the starting frame) to the apex (the frame with the highest ME intensity), then decreases towards the offset frame (the final frame representing the ME). Based on this characteristic, the proposed method simulates this motion in the process of constructing a distinctive repre- sentation for MEs, namely MESTI. Our objective is to create a spatio-temporal image that effectively represents a ME video. To achieve this, a temporal encoding approach introduced that transforms the entire video sequence into a single rep- resentative image. Additionally, our method incorporates the process of aggregating spatial information from the video into a compact static representation. Inspired by the approximate rank pooling method, which has been used in modeling video evolution [37], a similar strategy is proposed to encode the temporal evolution of MEs into a single image. This approach captures the dynamic variations in facial expressions over time while preserving the spatial structure necessary for effective ME recognition. 1)Spatial Encoding:A video is represented as a sequence of consecutive frames, denoted asI 1, . . . , I t, . . . , I T, whereT is the total number of frames, andI trepresents the frame at Fig. 1: Motion intensity in ME. time stept. Letψ(I t)∈Rddenote the feature vector extracted from each individual frameI t. In this study,ψ(I t)is a vector that directly encodes the RGB components of each pixel in the frameI t. Letd∈Rdbe defined as a parameter vector responsible for assigning a score to each frame(S(t|d))at timetusing a ranking function in Equation 1. S(t|d) =⟨d, ψ(I t)⟩(1) The parameterdis learned based on the entire frame sequence, ensuring that the scores assigned to each frame reflect their relative ranking. The learning process ofdis formulated as a convex optimization problem using RankSVM [58],d∗refers to the optimal parameter vectordthat is learned based on the entire frame sequence, as described in Equation 2. d∗=ρ(I 1, . . . , I T;ψ) = arg min dE(d)(2) This process integrates spatial information from individual frames into a ME image that preserves structural and appear- ance details. By leveraging the extracted RGB feature",
    "is learned based on the entire frame sequence, as described in Equation 2. d∗=ρ(I 1, . . . , I T;ψ) = arg min dE(d)(2) This process integrates spatial information from individual frames into a ME image that preserves structural and appear- ance details. By leveraging the extracted RGB feature vectors, the method ensures that spatial characteristics of each frame are considered in the ranking process, allowing the network to learn an optimal frame-ordering that reflects their relative importance in the sequence. 2)Temporal Encoding:Temporal encoding is performed based on the characteristic motion patterns of MEs, which serve as a basis for assigning scores to each frame during the rank pooling process of spatial encoding. Figure 1 illustrates the intensity of motion in ME. The motion characteristics of MEs can be easily observed: the intensity gradually increases from the first frame (onset), peaks at the apex frame, and then gradually decreases toward the final frame (offset) of the ME. Therefore, in this study, we aim to model the motion characteristics of MEs within the temporal encoding process to construct a ME image from the video. Temporal encoding is implemented by generating a ranking score that simulates the motion intensity of the ME in a straightforward manner during the rank pooling process. Let Iadefined as the apex frame, where the motion intensity of the ME reaches its maximum. Given any two framesI q, It, the frame closer to the apex frame is assigned a higher ranking score in our ranking function. IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 14, NO. 8, DECEMBER 2024 4 Fig. 2: MESTI and Dynamic Image and their frame coefficients in Ranking Function. Thus, for any pair of frames{I q, It}such that|a−q| ≤ |a−t|, establishing the ranking score:S(q|d)> S(t|d). Accordingly, Equation 2 is further expanded as Equation 3: E(d) =λ 2∥d∥2+ 2 T(T−1)×X |a−q|≤|a−t|max{0,1−S(q|d) +S(t|d)}. (3) The first term in Equation 3 is the standard quadratic regularizer used in SVMs. The second term is a hinge-loss function that soft-counts how many pairs of frames are incor- rectly ranked by the scoring function. To solve the equations involving Equation 1 and Equation 2, the ARP method [36] is used. Starting withd= ⃗0, the first approximated solution obtained by gradient descent is: d∗=⃗0−η∇E(d)|d=˜0∝ −∇E(d)|d=˜0for anyη >0 where: ∇E(⃗0)∝X |a−q|>|a−t|∇max{0,1−S(q|d) +S(t|d)}|d=˜0 =X |a−q|>|a−t|∇⟨d, ψ(I t)−ψ(I q)⟩=X |a−q|>|a−t|(ψ(I t)−ψ(I q)) d∗can be expanded as follows: d∗∝X |a−q|>|a−t|(ψ(I q)−ψ(I t)) =(P q>t(ψ(I q)−ψ(I t))if1≤t≤aP q>t(ψ(I t)−ψ(I q))ifa < t≤T =(Pa t=1αtψ(It)if1≤t≤aPT t=a+1αtψ(It)ifa < t≤Twhereα tis scalar coefficients. By expanding the sum: When the action in the range of onset frame and apex frame (1≤t≤a):P q>tψ(Iq)−ψ(I t) = (ψ(I 2)−ψ(I 1)) +(ψ(I 3)−ψ(I 1)) + (ψ(I 3)−ψ(I 2)) +···+ (ψ(I a)−ψ(I 1)) + (ψ(I a)−ψ(I 2)) +...+ (ψ(I a)−ψ(I a−1)) When the action in the range of apex frame and offset frame (a < t≤T):P q>tψ(It)−ψ(I q) = (ψ(I a+1)−ψ(I a+2)) +(ψ(I a+1)−ψ(I a+3)) + (ψ(I a+2)−ψ(I a+3)) +···+ (ψ(I a+1)−ψ(I T)) + (ψ(I a+2)−ψ(I T)) +...+ (ψ(I T−1)− ψ(IT)) Finally, the coefficientα tcan be efficiently computed in two scenarios by aggregating the coefficients ofψ(I t)along with",
    "apex frame and offset frame (a < t≤T):P q>tψ(It)−ψ(I q) = (ψ(I a+1)−ψ(I a+2)) +(ψ(I a+1)−ψ(I a+3)) + (ψ(I a+2)−ψ(I a+3)) +···+ (ψ(I a+1)−ψ(I T)) + (ψ(I a+2)−ψ(I T)) +...+ (ψ(I T−1)− ψ(IT)) Finally, the coefficientα tcan be efficiently computed in two scenarios by aggregating the coefficients ofψ(I t)along with their respective positive and negative signs: αt=( (t−1)−(a−t)if1≤t≤a (T−t)−(t−a−1)ifa < t≤T ⇒α t=( 2t−a−1if1≤t≤a T−2t+a+ 1ifa < t≤T(4) Henced∗can be present as the rank pooling operator after using ARP calculation: d∗≈ˆρ(I 1, . . . , I T;ψ) =(Pa t=1αtψ(It)if1≤t≤aPT t=a+1αtψ(It)ifa < t≤T (5) Finally, the MESTI construction is approximated by mul- tiplying the feature vector representing the RGB component IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 14, NO. 8, DECEMBER 2024 5 Fig. 3: The architecture of the proposed ME Gradient Attention Network (MEGANet) of each frame at timetwith theαcoefficient provided in Equation 4. The MESTI construction result is shown in Figure 2b using frame sequence (Figure 2a) as input. From the input frame sequence, we represent and observe the motion intensity of the ME representation and have the graph below. The MESTI construction results show that, firstly, our method generates a ranking function that better simulates the nature of the ME motion. Second, through the visual representation results, MESTI has shown more clearly the action units in the ME on the final image constructed compared to the traditional dynamic image method as showed in Figure 2c. B. Micro-expression Gradient Attention Networks The challenge in ME recognition lies in capturing the subtle, transient spatiotemporal patterns that characterize MEs, which often involve subtle intensity changes that conventional CNNs struggle to detect. These expressions are fleeting, making it difficult for traditional methods to effectively focus on the most critical regions of motion. To address this, MEGANet is proposed, a MER network that aims to enhance the detection of MEs by directing attention to areas with significant gradient changes. The core idea behind MEGANet is to combine gradient-guided attention with spatial self-attention, enabling the network to focus on both fine-grained motion transitions and the broader spatial context. The proposed architecture consists of two key compo- nents as showed in Figure 3: the Gradient Attention Block and the Residual Attention Block. The Gradient Attention Block focuses on amplifying micro-intensity transitions by computing both horizontal and vertical gradients to identify regions with sharp intensity changes. This block generates an attention map through convolution and sigmoid activation, which is then multiplied with the input, enabling the net- work to prioritize areas with significant micro-movement. The Residual Attention Block, on the other hand, further refines the features by considering the spatial context, ensuring that important structural information is preserved during the feature extraction process. The overall network follows a structured pipeline comprising multiple processing layers: •Input layer: The input to the network is an RGB image of size224×224×3. •Gradient Attention Block: Computes spatial gradients to enhance subtle ME features. A convolutional layer fol- lowed by a sigmoid activation generates an attention map, which is multiplied with the original input to highlight key regions. Fig. 4: Gradient Attention Block •Convolutional",
    "to the network is an RGB image of size224×224×3. •Gradient Attention Block: Computes spatial gradients to enhance subtle ME features. A convolutional layer fol- lowed by a sigmoid activation generates an attention map, which is multiplied with the original input to highlight key regions. Fig. 4: Gradient Attention Block •Convolutional Feature Extraction: A7×7convolutional layer with 64 filters, followed by batch normalization, ReLU activation, and max pooling, extracts low-level spatial features from the input image. •Residual-Attention Blocks: Three residual attention blocks process the feature maps hierarchically. Each block consists of two3×3convolutional layers, batch normalization, ReLU activation, and a residual connection. A self-attention module is integrated to capture long-range spatial dependencies. •Global Feature Aggregation: A global average pooling layer compresses the spatial feature maps into a com- pact feature vector, significantly reducing the number of parameters while retaining crucial information. •Fully Connected Layer and Classification: The final fea- ture vector is passed through a fully connected (FC) layer and a softmax activation function. This architecture effectively captures ME dynamics by leveraging gradient-based attention and residual learning, im- proving the network’s ability to recognize subtle facial move- ments. 1) Gradient Attention Block:This block, illustrated in Figure 4, explicitly models horizontal and vertical intensity gradients to localize ME regions. Given an input image X∈RB×C×H×W, horizontal and vertical gradients at the spatial location(i, j)are computed as: Gx(i, j) =∥X(i, j+ 1)−X(i, j)∥ pad Gy(i, j) =∥X(i+ 1;j)−X(i, j)∥ pad(6) whereG x, Gy∈RB×C×H×Wand∥ · ∥ paddenotes zero- padded absolute differences. Combined gradient maps are generated through element-wise summation: Gcombined =Gx⊕Gy (7) The gradient map is then processed through a learnable 3x3 convolutional filterW g(Wg∈R1×C×3×3), followed by sigmoid activation: Fattn=σ(W g∗Gcombined )(8) IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 14, NO. 8, DECEMBER 2024 6 Fig. 5: The corresponding Gradient Attention map is generated with the input MESTI. Fig. 6: Residual-Attention Block The final output is obtained via element-wise multiplication: Y=X⊙F attn (9) This attention map emphasizes regions with significant inten- sity transitions critical for ME analysis. Figure 5 illustrate the gradient attention map constructed from our proposed MESTI as input image and gradient attention block. 2) Residual - Attention Block:Our Residual - Attention Block is illustrated in Figure 6, building upon residual con- Fig. 7: Self-attention based on SAGANnection and SAGAN’s self-attention [52], this block aims to integrates self-attention into a residual framework to enhance spatial context modeling. LetF(X)denote the transformation by two convolutional layers: F(X) =BN 2(Conv 2(RELU(BN 1(Conv 1(X))))) Conv 1:RB×C in×H×W− →RB×C out×H′×W′ Conv 2:RB×C out×H′×W′− →RB×C out×H′×W′(10) A shortcut connection handles dimension mismatches: Xshortcut =\u001aConv 1×1(X) X(11) The residual output becomes: Xres=X shortcut +F(X)(12) Followed by Self-Attention Module proposed by SAGAN illustrated in Figure 7, specifically: Q=Conv 1×1(Xres), Q∈RB×C 8×HW K=Conv 1×1(Xres), K∈RB×C 8×HW E=softmax(QTK) V=Conv 1×1(Xres), V∈RB×C×HW Yattn=γ(VET), γis learnable(13) Finally: Y=Dropout(ReLU(Y attn))(14) III. EXPERIMENTS AND RESULTS A. Experiment scenarios and objectives To evaluate the effectiveness of the proposed method for MER, which includes the MESTI as input representation, the MEGANet as MER network, and the combined approach of MESTI and MEGANet, three experimental scenarios were conducted to assess the performance of each proposed",
    "EXPERIMENTS AND RESULTS A. Experiment scenarios and objectives To evaluate the effectiveness of the proposed method for MER, which includes the MESTI as input representation, the MEGANet as MER network, and the combined approach of MESTI and MEGANet, three experimental scenarios were conducted to assess the performance of each proposed com- ponent: Experiment 01:This experiment aims to evaluate the ef- fectiveness of the MESTI input representation. Specifically, it compares MESTI with other input modalities previously used in MER studies, such as Apex Frame, Optical Flow, Dynamic Image, Active Image, and Affective Image. Furthermore, the experiment continues by replacing the input in previously published MER networks with MESTI to investigate whether MESTI improves MER performance in these prior works. Experiment 02:This experiment evaluates the performance of MEGANet in MER. And analysis the effective of key block proposed in MEGANet. Experiment 03:This experiment assesses the overall ef- fectiveness of the proposed MER method, combining MESTI and MEGANet. The results of this experiment are compared with recent SOTA methods to demonstrate the superiority of the proposed approach. IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 14, NO. 8, DECEMBER 2024 7 Fig. 8: Visualization of MESTI and corresponding Gradient Attention Map characterize each emotion of ME (Best viewed in color) B. Dataset & Data preprocessing 1) Datasets:The experiments are conducted on three pub- licly available ME recognition datasets, namely SMIC-HS [27], CASME II [45] and SAMM [50], which are widely used as standard benchmarks for MER and for comparison with previous studies. 2) Data preprocessing:To ensure a fair comparison, the data preprocessing steps are minimalized, limiting them to face cropping, histogram equalization and resizing the images to dimensions appropriate for each network’s input requirements. This minimalistic approach eliminates potential biases from complex preprocessing techniques, allowing us to isolate and highlight the contributions of each input modality to overall network performance. To ensure fair comparison with prior studies, this work conducts both 3-class and 5-class evaluations. In the 3-class evaluation, three common categories across the datasets areconsidered: positive, negative, and surprise. For the 5-class evaluation, the original emotion annotations provided in the CASME II and SAMM datasets are retained. Specifically, the CASME II dataset comprises the ME labels disgust (60 samples), happiness (33), other (102), repression (27), and surprise (25). The SAMM dataset includes the labels anger (57 samples), happiness (26), contempt (12), surprise (15), and other (49). C. Experimental settings 1) Experiment 01:To ensure a fair comparison of the effec- tiveness of all input modalities, a common procedure is applied to all modalities. A train-test split protocol is used, with 90% for training and 10% for testing. The input is sequentially fed into three widely recognized deep learning networks: VGG19, ResNet50, and EfficientNetB0. This standardized approach minimizes external factors that could influence performance IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 14, NO. 8, DECEMBER 2024 8 TABLE I: Comparison of input modalities in MER with Train–Test split protocol. Input modality for MER VGG19 ResNet50 EfficientNet-B0 CASME II SAMM CASME II SAMM CASME II SAMM Static Apex frame 50.00% 43.75% 46.15% 50.00% 34.62% 43.75% Dynamic Optical flow (Onset–Apex) 50.00%",
    "COMPUTING, VOL. 14, NO. 8, DECEMBER 2024 8 TABLE I: Comparison of input modalities in MER with Train–Test split protocol. Input modality for MER VGG19 ResNet50 EfficientNet-B0 CASME II SAMM CASME II SAMM CASME II SAMM Static Apex frame 50.00% 43.75% 46.15% 50.00% 34.62% 43.75% Dynamic Optical flow (Onset–Apex) 50.00% 50.00% 46.15% 37.50% 26.92% 43.75% Optical flow (Apex–Offset) 53.85% 50.00% 34.62% 43.75% 46.15% 43.75% Dynamic image 57.69% 50.00% 53.85% 37.50% 53.85%50.00% Affective motion image 50.00% 43.75% 53.85% 50.00% 46.15% 43.75% Active image 48.00% 57.14% 44.00% 50.00% 52.00% 48.00% MESTI (ours) 73.08% 62.50% 65.38% 56.25% 61.54% 50.00% TABLE II: MESTI Input representation in published MER networks with LOSO protocol compared with the original research. Input NetworkCASME II SAMM ACC ACC Apex Image*Micro-attention [55] 65.9% 48.5% MESTI Micro-attention [55]71.02% 63.24% Dynamic Image*VGG19 [63] 51.02% 43.23% MESTI VGG19 [63]69.39% 60.29% *Results from the original research (baseline input); other rows use our MESTI representation. outcomes, allowing the observed differences to be directly attributed to the input modality itself. MESTI is further used as an alternative input for the MER networks employed in two prior studies. To ensure fairness and the significance of the comparison results, we implement the experimental method in the same manner as described in their studies. Both studies used the Leave-One-Subject-Out (LOSO) protocol for evaluation. 2) Experiment 02:MEGANet is evaluated through an ab- lation study. Two ablation scenarios are conducted. In the first, individual components of MEGANet: the Gradient Attention Block and the Residual Attention Block are isolated and evaluated independently. In the second, the performance of MEGANet is assessed with respect to the varying number of Residual Attention Blocks to determine the most suitable configuration. Both ablations used the LOSO protocol for evaluation. 3) Experiment 03:This experiment is designed to evaluate the proposed method in this paper, using MESTI as the input and MEGANet as the MER network. The experiment is conducted using the LOSO protocol for evaluation to ensure a meaningful comparison with previously published methods. 4) Specific configuration and training methodology:The following configuration and training methodology were used in this study: •Data Augmentation: The dataset is augmented using horizontal flipping and rotations at 5° and 10° (both clockwise and counterclockwise). •Loss Function: Focal Loss was used to address class imbalance and improve the network’s focus on hard-to- classify samples.•Optimizer: The Adam optimizer was employed with a learning rate of1e−3and weight decay of1e−4to optimize the network. •Training Duration: The network was trained for 50 epochs to ensure convergence and adequate learning. •Metric: The primary evaluation metric is accuracy, un- weighted F1-score (UF1), unweighted average recall (UAR). D. Results 1)Visual representation:The visual results of MESTI and its corresponding Gradient Attention Map are shown in Figure 8 to observe how MESTI captures the characteristic features of each ME emotion type and how the Gradient Attention Map highlights the regions of interest within MESTI. A key observation is that MESTI effectively captures and highlights the defining motion patterns of MEs, making them perceptible to the human eye in a single image representation. More specifically, both MESTI and the Gradient Attention Map successfully depict the",
    "Gradient Attention Map highlights the regions of interest within MESTI. A key observation is that MESTI effectively captures and highlights the defining motion patterns of MEs, making them perceptible to the human eye in a single image representation. More specifically, both MESTI and the Gradient Attention Map successfully depict the characteristic Action Units cor- responding to different ME emotions. For Disgust, the key motion regions primarily appear around the eyebrows, one side of the nose, and the corners of the mouth. Repression manifests as subtle downward movements on both sides of the mouth and the chin. Happiness is expressed by an upward motion at the corners of the mouth, whereas Surprise is predominantly reflected in eyebrow elevation and lower lip movement. These findings highlight the capability of MESTI to encode motion dynamics effectively in a compact and visually interpretable format. 2)MESTI Representation compared with other input modalities:Table I summarizes the comparative performance of various input modalities in the ME recognition task, evalu- ated using deep learning network on the CASMEII and SAMM datasets. The results consistently demonstrate that MESTI outperforms all other input modalities across the three widely used CNN architectures: VGG19, ResNet50, and Efficient- NetB0. Specifically, MESTI achieves the highest accuracy of 73.08% on CASMEII and 62.5% on SAMM with VGG19, surpassing the second-best input modality (Dynamic Image) by 15.39% and 12.5%, respectively. This superior performance underscores MESTI’s effectiveness in capturing subtle motion features, which are crucial for ME recognition. IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 14, NO. 8, DECEMBER 2024 9 TABLE III: Comparison with recent SOTA methods in 3-class evaluation with LOSO protocol Method YearCASME II SMIC-HS SAMM UF1 UAR ACC UF1 UAR ACC UF1 UAR ACC FeatRef [51] 2022 0.892 0.887 – 0.701 0.708 – 0.737 0.716 – Dual-ATME [65] 2023 0.765 0.751 0.817 0.646 0.658 0.646 0.562 0.538 0.714 DS - 3DCNN [30] 2023 – – – 0.789 0.806 0.788 0.755 0.783 0.792 FRL-DGT [29] 2023 0.919 0.903 – 0.743 0.749 – 0.772 0.758 – SelfME [26] 2023 0.908 0.929 – 0.697 0.701 – – – – Micron-BERT [25] 2023 0.903 0.891 – 0.855 0.838 – – – – MERASTC [61] 20230.933 0.950– 0.790 0.862 – – – – GLEFFN [11] 20230.883 0.911– 0.771 0.786 – – – – MCCA - VNet [42] 2024 0.915 0.923 – 0.816 0.811 – 0.883 0.871 – ROI + WArcFace [44] 2025 0.924 0.9100.940 0.811 0.819 0.818 0.787 0.785 0.862 SODA4MER [43] 2025 0.887 0.881 – 0.886 0.888 – – – – OFVIG-Net [59] 2025 0.713 0.720 – 0.644 0.640 – 0.607 0.579 – MESTI-MEGANet2025 0.913 0.929 0.932 0.917 0.924 92.68 0.890 0.914 0.918 Boldindicates the best result in each column; underline indicates the second-best result. “–” denotes that the metric was not reported in the cited work. TABLE IV: Comparison with recent SOTA methods in 5-class evaluation with LOSO protocol. Method YearCASME II SAMM UF1 UAR ACC UF1 UAR ACC GEME [68] 2021 0.735 - 75.20 0.454 - 55.88 MER-Supcon [60] 2022 0.729 - 73.58 0.625 - 67.65 CMNet [56] 2023 0.740 - 78.05 0.772 - 78.68 C3DBed [49] 2023",
    "with recent SOTA methods in 5-class evaluation with LOSO protocol. Method YearCASME II SAMM UF1 UAR ACC UF1 UAR ACC GEME [68] 2021 0.735 - 75.20 0.454 - 55.88 MER-Supcon [60] 2022 0.729 - 73.58 0.625 - 67.65 CMNet [56] 2023 0.740 - 78.05 0.772 - 78.68 C3DBed [49] 2023 0.752 - 77.64 0.722 - 75.73 KPCANet [35] 2023 0.659 - 70.46 0.522 - 63.83 JGULF [10] 2024 0.807 - 82.04 0.720 - 80.71 AU GCN [39] 2024 0.776 - 81.85 0.757 - 79.82 SODA4MER [43] 20250.814-84.18 0.789 - 80.30 LRT3O [38] 2025 0.791 - 81.78 0.757 - 80.15 MELLM [57] 2025 0.485 0.534 64.34 - - - MESTI-MEGANet2025 0.7790.78682.04 0.791 0.803 80.88 Boldindicates the best result in each column; underline indicates the second-best result. “–” denotes that the metric was not reported in the cited work. For the SAMM dataset, the overall recognition performance is lower compared to CASMEII across all input modalities, a trend consistent with previous studies due to SAMM’s greater diversity and complexity. Despite this challenge, MESTI continues to demonstrate superior recognition capabilities, achieving 62.5% with VGG19 and 56.25% with ResNet50, reinforcing its robustness across different datasets and deep learning architectures. To further validate MESTI’s effectiveness, we investigated whether its superior performance was specific to our proposed pipeline or if it could enhance other established MER archi- tectures. The original input modalities are replaced by two previously published works with MESTI: VGG19 (originally using Dynamic Image) and Micro-Attention (originally using Apex Frame). The results, presented in Table II, show that for VGG19, replacing the input with MESTI improved recognition accuracy from 51.02% to 69.39% on CASMEII and from 43.23% to 60.29% on SAMM. Similarly, for Micro-Attention, using MESTI as input improved accuracy from 65.90% to 71.02% on CASMEII and from 48.5% to 63.24% on SAMM. These results confirm that MESTI not only enhances our pro- posed network but also significantly improves the performance of other MER architectures, demonstrating its capability to effectively represent ME dynamics in a single image.3)Compared with State-of-the-art methods in MER:The comparative results with recent state-of-the-art methods are reported in Table III (for the 3-class evaluation) and Table IV (for the 5-class evaluation). Overall, the proposed method outperforms existing SOTA approaches on the SAMM and SMIC-HS datasets and achieves competitive performance on the CASME II dataset, as reflected across all three evaluation metrics: accuracy, UF1, and UAR. 3-class evaluation On SMIC-HS, MESTI-MEGANet attains the best per- formance on all three metrics (UF1=0.917, UAR=0.924, ACC=92.68%). The accuracy margin over the next best method (ROI+WArcFace, ACC=0.818) is≈+10.9 percentage points, indicating a substantial gain. On SAMM, our method again ranks first across metrics (UF1=0.890, UAR=0.914, ACC=0.918), with an accuracy improvement of +5.6% over the strongest competitor (ROI+WArcFace, ACC=0.862). On CASMEII, our scores are competitive but not leading: UF1=0.913 and UAR=0.929, ACC=0.932. The best UF1/UAR are achieved by MERASTC (UF1=0.933, UAR=0.950), while the highest accuracy belongs to ROI+WArcFace (ACC=0.940). The gaps to the leaders are modest: 0.020 in UF1, 0.021 in UAR, and 0.008 in ACC. 5-class evaluation On CASMEII, SODA4MER yields the best UF1 and IEEE TRANSACTIONS ON",
    "and UAR=0.929, ACC=0.932. The best UF1/UAR are achieved by MERASTC (UF1=0.933, UAR=0.950), while the highest accuracy belongs to ROI+WArcFace (ACC=0.940). The gaps to the leaders are modest: 0.020 in UF1, 0.021 in UAR, and 0.008 in ACC. 5-class evaluation On CASMEII, SODA4MER yields the best UF1 and IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 14, NO. 8, DECEMBER 2024 10 TABLE V: Ablation study of key blocks of MEGANet. Gradient Attention Block Residual Block Self-attention UF1 UAR ACC – ✓ – 0.788 0.821 80.49 – – ✓ 0.746 0.775 75.61 – ✓ ✓ 0.8304 0.8609 83.54 ✓ – – 0.8084 0.8551 81.10 ✓ ✓ ✓ 0.917 0.924 92.68 TABLE VI: Ablation study of number of Residual Attention Blocks Residual Attention Block UF1 UAR ACC ×2 0.802 0.842 82.32 ×3 0.917 0.924 92.68 ×4 0.844 0.878 85.98 ACC (UF1=0.814, ACC=84.18%). MESTI-MEGANet reaches UF1=0.779, UAR=0.786, ACC=82.04 and matches JGULF on accuracy (82.04), trailing SODA4MER by 2.14. On SAMM, MESTI-MEGANet attains the top results across all reported metrics (UF1=0.791, UAR=0.803, ACC=80.88). The gains are small but consistent: UF1 is slightly higher than SODA4MER (0.789), and accuracy exceeds JGULF (80.71) by 0.17 and SODA4MER (80.30) by 0.58. Note that UAR on SAMM is not commonly reported by most baselines, so direct UAR comparisons are limited. Across protocols and datasets, MESTI-MEGANet delivers SOTA on SMIC-HS (3-class) and strong SOTA on SAMM (both 3-class and 5-class), while remaining competitive on CASMEII (second-best accuracy in 3-class; tied for accuracy but below the best UF1 in 5-class). These outcomes indicate that the method generalizes well to different datasets and label granularities, with the largest margins observed on SMIC-HS (dataset without apex frame annotated). This success is attributed to two key factors: •MESTI’s motion-specific encoding, which preserves spa- tiotemporal dynamics (Figure 2), and •MEGANet’s Gradient Attention mechanism, which fo- cuses on intensity transitions (Figure 5) while Residual Attention blocks model long-range dependencies (Figure 6). 4)Approach with apex frame free dataset:In the SMIC dataset, apex frame annotations are not provided; hence, the apex frame information cannot be directly utilized to construct MESTI. As an alternative, in this study we adopt a simple strategy of selecting the middle frame. Interestingly, the results on SMIC demonstrate strong per- formance despite the absence of apex frame supervision. This finding suggests that the proposed MESTI approach can remain effective even without precise apex frame information, highlighting its robustness and applicability in more challeng- ing scenarios where apex annotations are unavailable. E. Ablation study To evaluate the contribution of each component in MEGANet, we conducted an ablation study focusing on boththe key building blocks and the number of Residual Attention Blocks. As shown in Table V , removing any of the major com- ponents leads to a clear performance drop. Using only the Residual Block without Gradient Attention or Self-attention yields the lowest performance (UF1 = 0.746, UAR = 0.775, ACC = 75.61). Incorporating Self-attention alone provides some improvement (ACC = 83.54), while Gradient Attention Block alone achieves ACC = 81.10. The best performance is obtained when all three modules are integrated, resulting in significant gains (UF1 =",
    "or Self-attention yields the lowest performance (UF1 = 0.746, UAR = 0.775, ACC = 75.61). Incorporating Self-attention alone provides some improvement (ACC = 83.54), while Gradient Attention Block alone achieves ACC = 81.10. The best performance is obtained when all three modules are integrated, resulting in significant gains (UF1 = 0.917, UAR = 0.924, ACC = 92.68). This demonstrates that the Gradient Attention Block, Residual Block, and Self-attention contribute complementary benefits, and their combination is essential for maximizing recognition accuracy. In addition, Table VI investigates the impact of varying the number of Residual Attention Blocks. With two blocks, the model achieves ACC = 82.32, which increases substantially to 92.68 when three blocks are employed. Interestingly, adding a fourth block slightly reduces performance (ACC = 85.98), indicating potential overfitting or redundancy. These findings suggest that three Residual Attention Blocks provide the op- timal balance between model complexity and representational power. IV. CONCLUSION In this work, we address the limitations of existing MER methodologies by introducing ME Spatio-Temporal Image as a novel input modality and ME Gradient Attention Network as a novel architecture. MESTI effectively encodes micro- movements into a single image, preserving both spatial and temporal features, while MEGANet utilizes a Gradient At- tention mechanism to enhance the detection of subtle motion cues. Our experimental results validate the effectiveness of MESTI by showing that it outperforms all other input modal- ities, including Apex Frame, Optical Flow, and Dynamic Image, across multiple deep learning networks. Furthermore, replacing the input of previously published MER architectures with MESTI results in significant improvements in recognition accuracy, highlighting its broad applicability. Additionally, MEGANet achieves state-of-the-art performance, particularly when combined with MESTI, confirming its effectiveness in ME analysis. These findings establish MESTI and MEGANet as highly effective solutions for MER, significantly improv- ing recognition accuracy. Future work could explore refining MESTI for real-time applications, integrating additional atten- tion mechanisms, or leveraging larger-scale datasets to further advance ME recognition systems. IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 14, NO. 8, DECEMBER 2024 11 REFERENCES [1] Nummenmaa, L., Saarim ¨akia, H., Glereana, E., Gotsopoulosa, A., J¨a¨askel ¨ainena, I., Harib, R., Samsa, M., Glerean, E., Hari, R., Hietanen, J. & Others Ekman, Paul (2007). Emotions Revealed. Recognizing faces and feelings to improve communication and emotional life. New York: Holt Paper-back, Montgomery, Arlene (2013) Neurobiology Essentials for Clinicians. What every therapist needs to know, New York, London, WW Nor. [2] Yan, W., Wu, Q., Liang, J., Chen, Y . & Fu, X. How Fast are the Leaked Facial Expressions: The Duration of Micro-Expressions.Journal Of Non- verbal Behavior.37, 217-230 (2013,12), https://doi.org/10.1007/s10919- 013-0159-8 [3] Matsumoto, D. & Hwang, H. Evidence for training the ability to read microexpressions of emotion.Motivation And Emotion.35, 181-191 (2011,6), https://doi.org/10.1007/s11031-011-9212-2 [4] Ekman, P. Darwin, deception, and facial expression.Ann N Y Acad Sci. 1000pp. 205-221 (2003,12) [5] Goh, K., Ng, C., Lim, L. & Sheikh, U. Micro-expression recognition: an updated review of current trends, challenges and solutions.The Visual Computer.36, 445-468 (2020,3), https://doi.org/10.1007/s00371- 018-1607-6 [6] Li, X., Hong, X., Moilanen, A., Huang, X., Pfister, T., Zhao, G. & Pietik ¨ainen, M. Towards Reading Hidden",
    "1000pp. 205-221 (2003,12) [5] Goh, K., Ng, C., Lim, L. & Sheikh, U. Micro-expression recognition: an updated review of current trends, challenges and solutions.The Visual Computer.36, 445-468 (2020,3), https://doi.org/10.1007/s00371- 018-1607-6 [6] Li, X., Hong, X., Moilanen, A., Huang, X., Pfister, T., Zhao, G. & Pietik ¨ainen, M. Towards Reading Hidden Emotions: A Comparative Study of Spontaneous Micro-Expression Spotting and Recognition Methods. IEEE Transactions On Affective Computing.9, 563-577 (2018) [7] Yildirim, S., Chimeumanu, M. & Rana, Z. The influence of micro- expressions on deception detection.Multimedia Tools And Applications. 82, 29115-29133 (2023,8), https://doi.org/10.1007/s11042-023-14551-6 [8] Frank, M. & Svetieva, E. Microexpressions and Deception.Understand- ing Facial Expressions In Communication: Cross-cultural And Multidis- ciplinary Perspectives. pp. 227-242 (2015), https://doi.org/10.1007/978- 81-322-1934-7-11 [9] Endres, J. & Laidlaw, A. Micro-expression recognition training in med- ical students: a pilot study.BMC Medical Education.9, 47 (2009,7), https://doi.org/10.1186/1472-6920-9-47 [10] Wang, F., Li, J., Qi, C., Wang, L. & Wang, P. JGULF: Joint global and unilateral local feature network for micro-expression recognition.Image And Vision Computing.147pp. 105091 (2024), https://www.sciencedirect.com/science/article/pii/S0262885624001951 [11] Guo, C. & Huang, H. GLEFFN: A Global-Local Event Feature Fusion Network for Micro-Expression Recognition.Proceedings Of The 3rd Workshop On Facial Micro-Expression: Advanced Tech- niques For Multi-Modal Facial Expression Analysis. pp. 17-24 (2023), https://doi.org/10.1145/3607829.3616446 [12] Frank, M., Herbasz, M., Sinuk, K., Keller, A. & Nolan, C. I see how you feel: Training laypeople and professionals to recognize fleeting emotions. The Annual Meeting Of The International Communication Association. Sheraton New York, New York City. pp. 1-35 (2009) [13] Li, Y ., Wei, J., Liu, Y ., Kauttonen, J. & Zhao, G. Deep Learning for Micro-Expression Recognition: A Survey.IEEE Transactions On Affective Computing.13, 2028-2046 (2022) [14] Quang, N., Chun, J. & Tokuyama, T. CapsuleNet for Micro- Expression Recognition.2019 14th IEEE International Conference On Automatic Face & Gesture Recognition (FG 2019). pp. 1-7 (2019), https://doi.org/10.1109/FG.2019.8756544 [15] Li, Y ., Huang, X. & Zhao, G. Can Micro-Expression be Recognized Based on Single Apex Frame?.2018 25th IEEE International Conference On Image Processing (ICIP). pp. 3094-3098 (2018) [16] Nie, X., Takalkar, M., Duan, M., Zhang, H. & Xu, M. GEME: Dual-stream multi-task GEnder-based micro- expression recognition.Neurocomputing.427pp. 13-28 (2021), https://www.sciencedirect.com/science/article/pii/S0925231220316957 [17] Quynh Le, T., Tran, T. & Rege, M. Dynamic image for micro-expression recognition on region-based framework.2020 IEEE 21st International Conference On Information Reuse And Integration For Data Science (IRI). pp. 75-81 (2020) [18] Verma, M., Vipparthi, S. & Singh, G. Non-Linearities Improve OrigiNet based on Active Imaging for Micro Expression Recognition. (2020,7) [19] Wu, J., Xu, J., Lin, D. & Tu, M. Optical Flow Filtering- Based Micro-Expression Recognition Method.Electronics.9(2020), https://www.mdpi.com/2079-9292/9/12/2056 [20] Hadid, A. The Local Binary Pattern Approach and its Applications to Face Analysis.2008 First Workshops On Image Processing Theory, Tools And Applications. pp. 1-9 (2008)[21] Ruiz-Hernandez, J. & Pietik ¨ainen, M. Encoding Local Binary Pat- terns using the re-parametrization of the second order Gaussian jet.2013 10th IEEE International Conference And Workshops On Automatic Face And Gesture Recognition (FG). pp. 1-6 (2013), https://api.semanticscholar.org/CorpusID:6766879 [22] Wang, Y ., See, J., Phan, R. & Oh, Y . LBP with Six Intersection Points: Reducing Redundant Information in LBP-TOP for Micro-expression Recognition.Computer Vision – ACCV 2014.",
    "second order Gaussian jet.2013 10th IEEE International Conference And Workshops On Automatic Face And Gesture Recognition (FG). pp. 1-6 (2013), https://api.semanticscholar.org/CorpusID:6766879 [22] Wang, Y ., See, J., Phan, R. & Oh, Y . LBP with Six Intersection Points: Reducing Redundant Information in LBP-TOP for Micro-expression Recognition.Computer Vision – ACCV 2014. pp. 525-537 (2015) [23] Huang, X., Wang, S., Zhao, G. & Pietik ¨ainen, M. Facial Micro- Expression Recognition Using Spatiotemporal Local Binary Pattern with Integral Projection. (2015,12) [24] Huang, X., Zhao, G., Hong, X., Zheng, W. & Pietik ¨ainen, M. Sponta- neous facial micro-expression analysis using Spatiotemporal Completed Local Quantized Patterns.Neurocomputing.175pp. 564-578 (2016), https://www.sciencedirect.com/science/article/pii/S0925231215015726 [25] Nguyen, X., Duong, C., Li, X., Gauch, S., Seo, H. & Luu, K. Micron-BERT: BERT-Based Facial Micro-Expression Recognition.2023 IEEE/CVF Conference On Computer Vision And Pattern Recognition (CVPR). pp. 1482-1492 (2023), https://api.semanticscholar.org/CorpusID:257985236 [26] Fan, X., Chen, X., Jiang, M., Shahid, A. & Yan, H. SelfME: Self- Supervised Motion Learning for Micro-Expression Recognition.2023 IEEE/CVF Conference On Computer Vision And Pattern Recognition (CVPR). pp. 13834-13843 (2023) [27] Li, X., Pfister, T., Huang, X., Zhao, G. & Pietik ¨ainen, M. A Spontaneous Micro-expression Database: Inducement, collection and baseline.2013 10th IEEE International Conference And Workshops On Automatic Face And Gesture Recognition (FG). pp. 1-6 (2013) [28] Zeng, X., Zhao, X., Zhong, X. & Liu, G. A Survey of Micro- expression Recognition Methods Based on LBP, Optical Flow and Deep Learning.Neural Processing Letters.55, 5995-6026 (2023,10), https://doi.org/10.1007/s11063-022-11123-x [29] Zhai, Z., Zhao, J., Long, C., Xu, W., He, S. & Zhao, H. Fea- ture Representation Learning with Adaptive Displacement Generation and Transformer Fusion for Micro-Expression Recognition. (2023), https://arxiv.org/abs/2304.04420 [30] Li, Z., Zhang, Y ., Xing, H. & Chan, K. Facial Micro-Expression Recog- nition Using Double-Stream 3D Convolutional Neural Network with Domain Adaptation.Sensors.23(2023), https://www.mdpi.com/1424- 8220/23/7/3577 [31] Barron, J., Fleet, D. & Beauchemin, S. Performance Of Optical Flow Techniques.International Journal Of Computer Vision.12pp. 43-77 (1994,2) [32] Liu, Y ., Zhang, J., Yan, W., Wang, S., Zhao, G. & Fu, X. A Main Di- rectional Mean Optical Flow Feature for Spontaneous Micro-Expression Recognition.IEEE Transactions On Affective Computing.7, 299-310 (2016) [33] Liong, S., See, J., Wong, K. & Phan, R. Less is more: Micro-expression recognition from video using apex frame. Signal Processing: Image Communication.62pp. 82-92 (2018), https://www.sciencedirect.com/science/article/pii/S0923596517302436 [34] Happy, S. & Routray, A. Fuzzy histogram of optical flow orientations for micro-expression recognition.IEEE Transactions On Affective Com- puting.10, 394-406 (2017) [35] Feng, W., Xu, M., Chen, Y ., Wang, X., Guo, J., Dai, L., Wang, N., Zuo, X. & Li, X. Nonlinear Deep Subspace Network for Micro- expression Recognition.Proceedings Of The 3rd Workshop On Facial Micro-Expression: Advanced Techniques For Multi-Modal Facial Expres- sion Analysis. pp. 1-8 (2023), https://doi.org/10.1145/3607829.3616444 [36] Bilen, H., Fernando, B., Gavves, E., Vedaldi, A. & Gould, S. Dynamic image networks for action recognition.Proceedings Of The IEEE Con- ference On Computer Vision And Pattern Recognition. pp. 3034-3042 (2016) [37] Fernando, B., Gavves, E., Oramas, J., Ghodrati, A. & Tuytelaars, T. Modeling Video Evolution for Action Recognition.Proceedings Of The IEEE Conference On Computer Vision And Pattern Recognition (CVPR). (2015,6) [38] Zhu, J., Zong, Y ., Shi, J., Lu, C.,",
    "ference On Computer Vision And Pattern Recognition. pp. 3034-3042 (2016) [37] Fernando, B., Gavves, E., Oramas, J., Ghodrati, A. & Tuytelaars, T. Modeling Video Evolution for Action Recognition.Proceedings Of The IEEE Conference On Computer Vision And Pattern Recognition (CVPR). (2015,6) [38] Zhu, J., Zong, Y ., Shi, J., Lu, C., Chang, H. & Zheng, W. Learning to Rank Onset-Occurring-Offset Representations for Micro-Expression Recognition.IEEE Transactions On Affective Computing. pp. 1-16 (2025) [39] Wang, L., Huang, P., Cai, W. & Liu, X. Micro-expression recognition by fusing action unit detection and Spatio-temporal features.ICASSP 2024 - 2024 IEEE International Conference On Acoustics, Speech And Signal Processing (ICASSP). pp. 5595-5599 (2024) [40] Fernando, B., Gavves, E., Oramas M., J., Ghodrati, A. & Tuytelaars, T. Rank Pooling for Action Recognition.IEEE Transactions On Pattern Analysis And Machine Intelligence.39, 773-787 (2017) IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 14, NO. 8, DECEMBER 2024 12 [41] Verma, M., Vipparthi, S. & Singh, G. AffectiveNet: Affective-Motion Feature Learning for Microexpression Recognition.IEEE MultiMedia.28, 17-27 (2021) [42] Zhang, D., Zhang, T., Sun, H., Tang, Y . & Liu, Q. MCCA-VNet: A Vit- Based Deep Learning Approach for Micro-Expression Recognition Based on Facial Coding.Sensors.24(2024), https://www.mdpi.com/1424- 8220/24/23/7549 [43] Bohao, Z., Wang, X., Wang, C. & He, G. Dynamic Stereotype The- ory Induced Micro-expression Recognition with Oriented Deformation. (2025,6) [44] Zhang, P., Wang, R., Luo, J. & Shi, L. Micro-Expression Recognition Algorithm Using Regions of Interest and the Weighted ArcFace Loss. Electronics.14(2025), https://www.mdpi.com/2079-9292/14/1/2 [45] Yan, W., Li, X., Wang, S., Zhao, G., Liu, Y ., Chen, Y . & Fu, X. CASME II: an improved spontaneous micro-expression database and the baseline evaluation.PLoS One.9, e86041 (2014,1) [46] Simonyan, K. & Zisserman, A. Very Deep Convolutional Networks for Large-Scale Image Recognition. (2015), https://arxiv.org/abs/1409.1556 [47] He, K., Zhang, X., Ren, S. & Sun, J. Deep Residual Learning for Image Recognition. (2015), https://arxiv.org/abs/1512.03385 [48] Tan, M. & Le, Q. EfficientNet: Rethinking Model Scaling for Convolu- tional Neural Networks. (2020), https://arxiv.org/abs/1905.11946 [49] Pan, H., Xie, L. & Wang, Z. C3DBed: Facial micro- expression recognition with three-dimensional convolutional neural network embedding in transformer model.Engineering Applications Of Artificial Intelligence.123pp. 106258 (2023), https://www.sciencedirect.com/science/article/pii/S0952197623004426 [50] Davison, A., Lansley, C., Costen, N., Tan, K. & Yap, M. SAMM: A Spontaneous Micro-Facial Movement Dataset.IEEE Transactions On Affective Computing.9, 116-129 (2018) [51] Zhou, L., Mao, Q., Huang, X., Zhang, F. & Zhang, Z. Feature refinement: An expression-specific feature learning and fusion method for micro- expression recognition.Pattern Recognition.122pp. 108275 (2022), https://www.sciencedirect.com/science/article/pii/S0031320321004556 [52] Zhang, H., Goodfellow, I., Metaxas, D. & Odena, A. Self-Attention Gen- erative Adversarial Networks. (2019), https://arxiv.org/abs/1805.08318 [53] Patel, D., Hong, X. & Zhao, G. Selective deep features for micro- expression recognition.2016 23rd International Conference On Pattern Recognition (ICPR). pp. 2258-2263 (2016) [54] Gan, Y ., Liong, S., Yau, W., Huang, Y . & Tan, L. OFF-ApexNet on micro-expression recognition system.Signal Processing: Image Commu- nication.74pp. 129-139 (2019) [55] Wang, C., Peng, M., Bi, T. & Chen, T. Micro-attention for micro- expression recognition.Neurocomputing.410pp. 354-362 (2020) [56] Wei, M., Jiang, X., Zheng, W., Zong, Y ., Lu, C. & Liu, J. Cmnet: contrastive magnification network for micro-expression recognition.Pro- ceedings Of The AAAI",
    "system.Signal Processing: Image Commu- nication.74pp. 129-139 (2019) [55] Wang, C., Peng, M., Bi, T. & Chen, T. Micro-attention for micro- expression recognition.Neurocomputing.410pp. 354-362 (2020) [56] Wei, M., Jiang, X., Zheng, W., Zong, Y ., Lu, C. & Liu, J. Cmnet: contrastive magnification network for micro-expression recognition.Pro- ceedings Of The AAAI Conference On Artificial Intelligence.37, 119-127 (2023) [57] Zhang, Z., Zhao, S., Liu, S., Yin, S., Mao, X., Xu, T. & Chen, E. MELLM: Exploring LLM-Powered Micro-Expression Understanding Enhanced by Subtle Motion Perception. (2025,5) [58] Smola, A. & Sch ¨olkopf, B. A tutorial on support vector regression. Statistics And Computing.14pp. 199-222 (2004) [59] Zhang, L., Zhang, Y ., Sun, X., Tang, W., Wang, X. & Li, Z. Micro-expression recognition based on direct learning of graph structure.Neurocomputing.619pp. 129135 (2025), https://www.sciencedirect.com/science/article/pii/S0925231224019064 [60] Zhi, R., Hu, J. & Wan, F. Micro-expression recognition with supervised contrastive learning.Pattern Recognition Letters.163pp. 25-31 (2022), https://www.sciencedirect.com/science/article/pii/S0167865522002690 [61] Gupta, P. MERASTC: Micro-Expression Recognition Using Effective Feature Encodings and 2D Convolutional Neural Network.IEEE Trans- actions On Affective Computing.14, 1431-1441 (2023) [62] Liu, N., Liu, X., Zhang, Z., Xu, X. & Chen, T. Offset or onset frame: A multi-stream convolutional neural network with capsulenet module for micro-expression recognition.2020 5th International Conference On Intelligent Informatics And Biomedical Sciences (ICIIBMS). pp. 236-240 (2020) [63] Verma, M., Vipparthi, S., Singh, G. & Murala, S. LEARNet: Dynamic Imaging Network for Micro Expression Recognition.IEEE Transactions On Image Processing.29pp. 1618-1627 (2020) [64] Wei, M., Zheng, W., Zong, Y ., Jiang, X., Lu, C. & Liu, J. A Novel Micro- Expression Recognition Approach Using Attention-Based Magnification- Adaptive Networks.ICASSP 2022 - 2022 IEEE International Conference On Acoustics, Speech And Signal Processing (ICASSP). pp. 2420-2424 (2022)[65] Zhou, H., Huang, S., Li, J. & Wang, S. Dual-ATME: Dual-Branch At- tention Network for Micro-Expression Recognition.Entropy.25(2023), https://www.mdpi.com/1099-4300/25/3/460 [66] Lo, L., Xie, H., Shuai, H. & Cheng, W. MER-GCN: Micro-expression recognition based on relation modeling with graph convolutional net- works.2020 IEEE Conference On Multimedia Information Processing And Retrieval (MIPR). pp. 79-84 (2020) [67] Li, Y ., Huang, X. & Zhao, G. Joint local and global information learning with single apex frame detection for micro-expression recognition.IEEE Transactions On Image Processing.30pp. 249-263 (2020) [68] Nie, X., Takalkar, M., Duan, M., Zhang, H. & Xu, M. GEME: Dual-stream multi-task GEnder-based micro- expression recognition.Neurocomputing.427pp. 13-28 (2021), https://www.sciencedirect.com/science/article/pii/S0925231220316957 [69] Verma, M., Vipparthi, S. & Singh, G. Non-Linearities Improve OrigiNet based on Active Imaging for Micro Expression Recognition.2020 Inter- national Joint Conference On Neural Networks (IJCNN). pp. 1-8 (2020)"
  ]
}