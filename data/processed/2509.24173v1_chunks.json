{
  "filename": "2509.24173v1.pdf",
  "total_chunks": 26,
  "text_length": 77223,
  "chunks": [
    "1 Fundamental Limit of Discrete Distribution Estimation under Utility-Optimized Local Differential Privacy Sun-Moon Yoon,Graduate Student Member, IEEE,Hyun-Young Park,Graduate Student Member, IEEE, Seung-Hyun Nam,Member, IEEE,and Si-Hyeon Lee,Senior Member, IEEE Abstract—We study the problem of discrete distribution esti- mation under utility-optimized local differential privacy (ULDP), which enforces local differential privacy (LDP) on sensitive data while allowing more accurate inference on non-sensitive data. In this setting, we completely characterize the fundamental privacy–utility trade-off. The converse proof builds on several key ideas, including a generalized uniform asymptotic Cram ´er–Rao lower bound, a reduction showing that it suffices to consider a newly defined class of extremal ULDP mechanisms, and a novel distribution decomposition technique tailored to ULDP constraints. For the achievability, we propose a class of utility- optimized block design (uBD) schemes, obtained as nontrivial modifications of the block design mechanism known to be optimal under standard LDP constraints, while incorporating the distribution decomposition idea used in the converse proof and a score-based linear estimator. These results provide a tight characterization of the estimation accuracy achievable under ULDP and reveal new insights into the structure of optimal mechanisms for privacy-preserving statistical inference. Index Terms—Local differential privacy, utility-optimized local differential privacy, distribution estimation, privacy-utility trade- off, block design I. INTRODUCTION Local differential privacy (LDP) [1]–[3] has already been deployed at scale in industry [4]–[8]. Under LDP, each in- dividual perturbs their data locally before transmission so that the reported value reveals provably little about the true record, thereby removing the need for a trusted curator [1], [2]. Formally, LDP bounds how much the distribution of a user’s reported value can differ as their true data vary, thereby embedding privacy directly at the source. This user-level privacy guarantee—quantified through information-theoretic bounds [9, Thm. 14]—has propelled LDP into a wide spectrum of statistical inference and machine-learning tasks involving sensitive data [7], [10]–[14]. However, LDP offers blanket protection: for any pair of input values, the distribution of reported data must remain nearly indistinguishable [9]. Although such protection is reas- suring, it can be inefficient in scenarios where only certain S.-M. Yoon, H.-Y . Park, and S.-H. Lee are with the School of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, South Korea (e-mail: ysm4078@kaist.ac.kr, phy811@kaist.ac.kr, sihyeon@kaist.ac.kr). S.-H. Nam is with the Information & Electronics Re- search Institute, KAIST, Daejeon, South Korea (e-mail: shnam@kaist.ac.kr). S.-M. Yoon and H.-Y . Park contributed equally to this work. (Corresponding author: Si-Hyeon Lee) The appendix of this paper is provided in the supplementary material.X Y ˆX Yes NoYes NoYes/No Yes/No (a) Warner’s randomized response [10]X Y ˆX Yes NoYes NoYes/No No (b) Mangat’s randomized response [16] Fig. 1: Comparison between two binary randomized response models, where ˆXdenotes a guess aboutXbased onY. inputs are truly sensitive. Consider the survey, “Have you violated a policy?”, withX∈ {“Yes”,“No”}as the truthful answer, adapted from the illustrative example in [15]. Under Warner’s randomized response (RR) [10] shown in Fig. 1 (a), respondents flip a biased coin and potentially report the opposite answer. Consequently, the reported valueYconveys only limited information aboutX, restricting the analyst’s ability to infer the true",
    "withX∈ {“Yes”,“No”}as the truthful answer, adapted from the illustrative example in [15]. Under Warner’s randomized response (RR) [10] shown in Fig. 1 (a), respondents flip a biased coin and potentially report the opposite answer. Consequently, the reported valueYconveys only limited information aboutX, restricting the analyst’s ability to infer the true outcome. Notably, however, only the affirmative response (“Yes”) is genuinely sensitive, while the negative response (“No”) is not. Mangat’s randomized response [16] in Fig. 1 (b), exploits this asymmetry: those who answered “Yes” always report truthfully, while those who answered “No” randomize their answer. This ensures privacy whenY=“Yes”, while enabling reliable inference whenY=“No”. Utility-optimized local differential privacy (ULDP) [15] formalizes this idea. A ULDP mechanism partitions outputs into protected symbols—subject to the full LDP guarantee—and invertible symbols that faith- fully reveal non-sensitive inputs. Thus, ULDP preserves LDP- level privacy exactly where needed, while permitting enhanced inference elsewhere. This approach has motivated extensions such as high–low LDP [17],(ϵ, δ)-ULDP [18], [19], input- discriminative LDP [20], [21], and utility-optimized key-value data LDP [22]. Furthermore, ULDP has already seen adoption in domains including clinical analytics [23], federated learning [24], and natural language processing [25]. For the representative statistical inference task of discrete distribution estimation, which is the focus of this paper, several ULDP schemes have been proposed [15], [17], [26], [27]. As will be elaborated in Section III, prior ULDP schemes have been analyzed only with respect to order-optimality in specific regimes [15], [17], [27], while the exact leading constant that characterizes the fundamental limit of the PUT under ULDP has not been identified. This unresolved gap motivates ourarXiv:2509.24173v1 [cs.CR] 29 Sep 2025 2 work, which establishes the exact attainable performance. A. Our Contribution Our main contribution is to establish the exact optimal PUT—including the leading constant—for discrete distribu- tion estimation under ULDP. We characterize the optimal PUT in terms of a finite-dimensional concave–convex saddle- point optimization problem that can be solved using convex optimization solvers and give a closed-form solution to this optimization problem for some regime. For converse, we develop two generalanalytical frame- workswhich extend previous works: (i) auniform asymp- totic Cram ´er-Rao lower bound, a versatile concept applicable beyond ULDP to general private estimation problems, and (ii) a reduction showing that optimal ULDP mechanisms can be restricted to a newly defined class ofextremal ULDP mechanisms, analogous to extremal LDP mechanisms [28], thereby confining the minimax analysis to this smaller class. After that, we complete the converse proof by exploiting a novel way todecompose the data distributioninto several parts based on the structure of the ULDP constraint. For achievability, we propose a class ofutility-optimized block design (uBD) schemes. The development of uBD schemes builds on several key ideas: (i) nontrivially modifying block design schemes[29] for discrete distribution estimation under LDP, (ii) exploiting the idea ofscore-based linear estimatorin [30], and (iii)decomposing the data distribution as in the converse proof. Notably, the class of uBD schemes subsumes one of the existing schemes, uRR [15], and our closed-form characterization implies that uRR is optimal in some regime. To the best of our knowledge, this is the",
    "idea ofscore-based linear estimatorin [30], and (iii)decomposing the data distribution as in the converse proof. Notably, the class of uBD schemes subsumes one of the existing schemes, uRR [15], and our closed-form characterization implies that uRR is optimal in some regime. To the best of our knowledge, this is the first work to prove the optimality of uRR in a certain regime. Furthermore, we show that there exists a regime in which pre- viously proposed ULDP schemes cannot achieve the optimal PUT, thereby establishing the strict superiority of our uBD scheme. B. Paper Outline The rest of this paper is organized as follows. Section II formalizes the discrete distribution estimation problem under the ULDP constraint. Section III provides the necessary back- ground and a review of the related works. Our main result—the characterization of the optimal PUT—is stated in Section IV. To establish the optimal PUT, Section V presents the converse proof, and Section VI establishes achievability through the proposed uBD scheme. Section VII provides the numerical evaluations of the proposed scheme and baselines on a real- world dataset, demonstrating that our scheme outperforms existing alternatives. Finally, Section VIII concludes the paper with some discussion on future works. C. Notations and Terminologies For natural numbersa≤b, we denote[a:b] := [a, b]∩N and[a] := [1 :a]. For a finite setX,P(X)denotes the set of all probability mass functions onX. Forx∈ Xand P∈ P(X), we write eitherP(x)orP xforP({x}). WhenX1 X2 Xn...PQ Q Q QY1 Y2 Yn...server ˆP (a) A discrete distribution estimation system under the ULDP constraint. ϵ-LDP XS XNYP YI (b) Illustration of a ULDP mechanism. Fig. 2: Overview of system model. X= [w]forw∈N ≥2, we identifyP(X)with the probability simplex, ∆w:=( P∈Rw:wX x=1Px= 1and∀x∈[w], P x≥0) .(1) The relative interior of∆ wis ∆◦ w:={P∈∆ w:Px>0,∀x∈[w]},(2) and its direction space is ⃗∆w:=( h∈Rw:wX x=1hx= 0) .(3) Forx∈[w], we writeδ(x;w)∈∆ wfor the point mass atx, i.e.,δ(x;w) x′=1(x=x′). For a conditional distributionQ:X → P(Y)between finite setsX,Y, we denotesupp(Q) :={y∈ Y:Q(y|x)> 0for somex∈ X}. For simplicity, we often regardQas the row stochastic matrix(Q x,y)(x,y)∈X×Y with entriesQ x,y= Q(y|x). Given an input distribution,P∈ P(X), we write QP∈ P(Y)for the marginal distribution of the output, that is,Q P(y) =P x∈XQ(y|x)P x. We write0 m,1m∈Rmas the all-zeros and all-ones vectors (and0 m×n,1m×n for their matrix analogues), andI mas the m×midentity matrix. Forγ∈Rm, we writediag(γ)as the m×mdiagonal matrix whosei-th diagonal element isγ i. For A∈Rn×m,S1⊂[n], andS 2⊂[m], we writeA S1,S2for the submatrix ofAwith rows indexed byS 1and columns indexed byS 2. For a positive definite matrixA, we definetr −1(A) := tr\u0000 A−1\u0001 , and for a non-invertible positive semidefinite matrix A, we definetr −1(A) :=∞. II. PROBLEMFORMULATION We consider the private discrete distribution estimation system withn∈Nclients and a single server, as depicted 3 in Fig. 2. For eachi∈[n], thei-th client has its own categorical dataX i∈ X= [w],w∈N ≥2, generated in an i.i.d. manner from an unknown distributionP∈ P(X). To mitigate privacy leakage, thei-th client perturbs its dataX i intoY ithrough a conditional distributionQ:X → P(Y)for some finite setY, called aprivacy mechanism, and transmits Yito the server. After collectingYn= (Y 1, . .",
    "categorical dataX i∈ X= [w],w∈N ≥2, generated in an i.i.d. manner from an unknown distributionP∈ P(X). To mitigate privacy leakage, thei-th client perturbs its dataX i intoY ithrough a conditional distributionQ:X → P(Y)for some finite setY, called aprivacy mechanism, and transmits Yito the server. After collectingYn= (Y 1, . . . , Y n), the server computes the estimate ˆPn:Yn→RwofP. Let ˆPdenote a sequence of estimators( ˆPn)∞ n=1and we call the pair(Q, ˆP)a private estimation scheme. Theestimation errorof a private estimation scheme(Q, ˆP)withnclients is measured by the worst-case mean squared error (MSE), Rn(Q,ˆP) := sup P∈∆ wRn(Q,ˆP;P),(4) where Rn(Q,ˆP;P) =E Yn∼Qn Ph ∥ˆPn(Yn)−P∥2 2i .(5) In this paper, we consider theutility-optimized local differ- ential privacy(ULDP) constraint assuming a partially sensitive domain: sensitive categories are privatized under anϵ-LDP whereas non-sensitive categories may be disclosed via invert- ible outputs to enhance utility. Specifically, we suppose that Xis partitioned into two sets: the set of sensitive elements XS⊂ Xand the set of non-sensitive elementsX N=X\\X S. This partition is common knowledge to all clients and the server. The ULDP is defined as follows. Definition 1(ULDP [15]).ForX S⊂ Xandϵ >0, a privacy mechanismQ:X → P(Y)is called an(X S, ϵ)-ULDP mechanismif there exists a partition{Y P,YI}ofY(called the sets ofprotected dataandinvertible data, respectively), such that 1)For allx, x′∈ Xandy∈ Y P, Q(y|x)≤eϵQ(y|x′).(6) 2)For ally∈ Y I, there existsx∈ X Nsuch that Q(y|x)>0, Q(y|x′) = 0for allx′∈ X, x′̸=x.(7) Without loss of generality, we assume that the set of sensitive elements is given asX S= [v],v≤w, and we simply call an(X S, ϵ)-ULDP mechanism as a(v, ϵ)-ULDP mecha- nism. We denote the class of all(v, ϵ)-ULDP mechanisms byQ w,v,ϵ . Also, a private estimation scheme(Q, ˆP)which satisfiesQ∈ Q w,v,ϵ is called a(v, ϵ)-ULDP scheme. Note that(w, ϵ)-ULDP is equivalent toϵ-LDP. Also, (7) indicates that whenever the event{Y∈ Y I}occurs, the server can perfectly infer the inputx∈ X Nof a ULDP mechanism. Thus, roughly speaking, an(v, ϵ)-ULDP mechanism provides partly more accurate data than anϵ-LDP mechanism. Theoptimal privacy-utility tradeoff(or, in short,optimal PUT) for a discrete distribution estimation system withn- clients under(v, ϵ)-ULDP constraint is defined as M∗ n(w, v, ϵ) := inf Q∈Q w,v,ϵinf ˆPRn(Q,ˆP).(8) In [15], it was shown thatM∗ n(w, v, ϵ) = Θ(1/n). Accord- ingly, theasymptotically optimal PUTis defined as M∗(w, v, ϵ) := lim inf n→∞n·M∗ n(w, v, ϵ),(9)and theasymptotic errorof a private estimation scheme (Q,ˆP)is defined as R(Q, ˆP) := lim sup n→∞n·R n(Q,ˆP).(10) We say that a(v, ϵ)-ULDP scheme(Q, ˆP)isasymptotically optimalif R(Q, ˆP) =M∗(w, v, ϵ).(11) In this paper, we focus on characterizing the asymptotically optimal PUTM∗(w, v, ϵ)and simply refer to the asymptoti- cally optimal PUT as the optimal PUT from now on. Note that M∗(w, w, ϵ)is the optimal PUT under theϵ-LDP constraint, which has already been established in [11]. Hence, from now on, we assumev < w. Remark 1.The following inequalities follow directly from the relation between LDP and ULDP: M∗ n(w, v, ϵ)≥M∗ n(v, v, ϵ), M∗(w, v, ϵ)≥M∗(v, v, ϵ).(12) In Remark 3, we show that the above bound is, in fact, tight under specific conditions. III. PRELIMINARIES ANDRELATEDWORKS",
    "now on, we assumev < w. Remark 1.The following inequalities follow directly from the relation between LDP and ULDP: M∗ n(w, v, ϵ)≥M∗ n(v, v, ϵ), M∗(w, v, ϵ)≥M∗(v, v, ϵ).(12) In Remark 3, we show that the above bound is, in fact, tight under specific conditions. III. PRELIMINARIES ANDRELATEDWORKS A. Discrete Distribution Estimation under LDP & ULDP Discrete distribution estimation under local differential pri- vacy (LDP) has been extensively studied, and a variety of LDP schemes have been proposed such as randomized response (RR) [10], randomized aggregatable privacy-preserving ordinal response (RAPPOR) [7], subset selection (SS) [11], Hadamard response (HR) [12], projective geometry response (PGR) [13], optimized unary encoding (OUE) protocol [14], and optimized local hasing (OLH) protocol [14]. Among these, the SS is known to attain the optimal PUTM∗(w, w, ϵ)[31]. Recently, Parket al.[29] introduced a class of block design schemes that subsumes RR, SS, HR, and PGR while also enabling the construction of new mechanisms that achieve optimal PUT with reduced communication costs. Turning to the utility-optimized LDP (ULDP) setting, pro- posed ULDP schemes are typically derived by modifying classical LDP schemes to satisfy the ULDP requirements in Definition 1. Concretely, uRR and uRAP [15] are derived from RR [10] and RAPPOR [7], respectively; uSS [27] is based on SS [11]; uOUE and uOLH [26], [27] are based on OUE and OLH [14]; and a variant of the Hadamard response [17]—referred to here as uHR—extends HR [12] to the ULDP constraint. To characterize the optimal PUT under ULDP, Murakamiet al.[15] utilized Remark 1 and known results in PUT over LDP in [2] to show that forϵ∈[0,1], M∗ n(w, v, ϵ) = Ω\u0000v nϵ2\u0001 . Then, they stated that uRAP attains an estimation error of4v nϵ2asϵ→0; hence, it isorder-optimalfor ϵ∈[0,1]. Subsequently, Acharyaet al.[12] refined the optimal PUT asM∗ n(w, v, ϵ) = Θ\u0010 v2 nϵ2+w nϵ\u0011 for the high privacy regimeϵ=O(1), and introduced the uHR scheme whose estimation error matches this rate up to constant factors.1 1They stated the sample complexity under the total variation loss (see [17, Theorem 3]). We adapt the argument to obtain the corresponding MSE. 4 Recently, [27] showed thatM∗ n(w, v, ϵ) = Θ\u0010 veϵ n(eϵ−1)2\u0011 for ϵ≤logv, and uSS, uOUE, and uOLH areorder-optimalin the regimeϵ≤logv, which was also proved from Remark 1 and results in PUT over LDP in [11]. Furthermore, it was experi- mentally shown that uSS [27] achieves the lowest estimation error among previous schemes. Despite these advances, theexactoptimal PUT under ULDP, in particular the determination of the leading constant, had remained an open problem until our work, where we establish a complete characterization. B. Asymptotic Cram ´er-Rao Lower Bound A well-known result in asymptotic statistics [32]–[34] is an asymptotic variant of the Cram ´er-Rao lower bound (CRLB), which we refer to as theasymptotic Cram ´er-Rao lower bound(asymptotic CRLB). In [31], a key step in establishing the converse of the optimal PUT under LDP is to derive a certain uniform version of the asymptotic CRLB, suitable for distribution estimation under LDP. This subsection revisits the classical asymptotic CRLB and sketches the key ideas",
    "theasymptotic Cram ´er-Rao lower bound(asymptotic CRLB). In [31], a key step in establishing the converse of the optimal PUT under LDP is to derive a certain uniform version of the asymptotic CRLB, suitable for distribution estimation under LDP. This subsection revisits the classical asymptotic CRLB and sketches the key ideas behind the uniform refinement of [31]. In Section V-A, we extend the uniform version of [31] substantially, laying the groundwork for showing the converse of the optimal PUT under ULDP. The standard asymptotic CRLB [32, Theorem 29.4] implies that for afixedconditional distributionQfrom[w], we have lim n→∞n·inf ˆPRn(Q,ˆP)≥sup P∈∆◦wtr−1(JP,Q),(13) whereJ P,Qis the Fisher information matrix ofQatP(defined in Definition 5). However, this cannot be directly applied to derive a lower bound on the optimal PUT under LDP, which involves the infimum over the privacy mechanismsQinside the limit overn. To circumvent this challenge, the authors of [31] first reduced the optimization overallϵ-LDP mechanisms to the optimization over a specific class ofϵ-LDP mechanisms, called theextremalϵ-LDP mechanismsintroduced in [28]. Then, by refining the classical proof of asymptotic CRLB in [33], [34], commonly referred to as thelocal asymptotic normality (LAN)argument, they derived a certain universal lower bound oninf ˆPRn(Q,ˆP), holding forall extremalϵ- LDP mechanismsQ. This gives M∗(w, w, ϵ)≥sup P∈∆◦winf Q∈QEw,ϵtr−1(JP,Q),(14) whereQE w,ϵis the set of all extremalϵ-LDP mechanisms [28]. We note that the proof exploits the special structures of (extremal) LDP mechanisms, and therefore it is not straight- forward to adopt the arguments of [31] in the ULDP setup. C. Block Design Schemes Our proposed ULDP mechanism builds on the block design (BD)–based LDP mechanism in [29], referred to as the BD mechanism. This subsection reviews the BD mechanism with emphasis on elements relevant to the development of the proposed ULDP scheme in Section VI. Definition 2(Block design [35]).Letv, b, r, k∈Nandλ∈ Z≥0. A hypergraph(V,E), that consists of a set of verticesVand a set of edges (or blocks)E ⊆2V, is called a(v, b, r, k, λ)- block design if|V|=v,|E|=b, and it satisfies the following symmetries: 1)r-regular: every vertex inVis contained inredges. 2)k-uniform:∀e∈ E,|e|=k. 3)λ-pairwise balanced: every pair of vertices inVis contained inλedges. Without loss of generality, we assumeV= [|V|]. The parameters(v, b, r, k, λ)of a block design should satisfy the following relations. Lemma 1.[35] For any(v, b, r, k, λ)-block design, we have b≥v≥k, b≥r≥λ,(15) bk=vr, r(k−1) =λ(v−1).(16) Also, ifv= 1, then(b, r, k, λ) = (1,1,1,0). Remark 2.For any givenv, k∈Nsuch thatk≤v, there exists a\u0010 v,\u0000v k\u0001 ,\u0000v−1 k−1\u0001 , k,\u0000v−2 k−2\u0001\u0011 -block design called the (v, k)-complete block design [35] (here, ifk= 1, we regard\u0000v−2 k−2\u0001 = 0). This is formed by lettingEbe the set ofallsize ksubsets ofV. Given(v, b, r, k, λ)-block design, a BD mechanism regards input data as a vertex and outputs an edge, enforcingϵ-LDP by biasing towards edges that contain the input vertex. Definition 3(Block design mechanism [29]).For givenϵ >0 and a(v, b, r, k, λ)-block design(X,Y), the corresponding (v, b, r, k, λ, ϵ)-block design mechanism is the conditional dis- tributionQ:X → P(Y)such that Q(y|x) =( eϵ reϵ+b−r(ifx∈y) 1 reϵ+b−r(ifx /∈y)(17) Parket al.also proposed the corresponding sequence of unbiased",
    "the input vertex. Definition 3(Block design mechanism [29]).For givenϵ >0 and a(v, b, r, k, λ)-block design(X,Y), the corresponding (v, b, r, k, λ, ϵ)-block design mechanism is the conditional dis- tributionQ:X → P(Y)such that Q(y|x) =( eϵ reϵ+b−r(ifx∈y) 1 reϵ+b−r(ifx /∈y)(17) Parket al.also proposed the corresponding sequence of unbiased estimators ˆPfor the BD mechanism [29, Thm. 3], and called such a pair(Q, ˆP)a(v, b, r, k, λ, ϵ)-BD scheme. For any(v, ϵ), it was shown that the estimation error of the BD scheme only depends on the uniformity parameterk. In [29, Prop. 1 and Thm. 5], an explicit characterization of optimal kthat minimizes the estimation error is provided and the BD scheme with optimalkis shown to achieve the optimal PUT. Theorem 2([29, Prop. 1 and Thm. 5]).Letv≥2and let (Q,ˆP)be a(v, b, r, k, λ, ϵ)-block design scheme. Its asymp- totic error is R(Q, ˆP) =RBD(v, k, ϵ) :=(v−1)2(keϵ+v−k)2 vk(v−k)(eϵ−1)2.(18) Moreover, we have M∗(v, v, ϵ) = min k∈[v−1]RBD(v, k, ϵ),(19) and K∗(v, ϵ) := arg min k∈[v−1]RBD(v, k, ϵ)(20) ={k:E(v, k)≤ϵ≤E(v, k−1)},(21) 5 where E(v, k) = lns (v−k)(v−k−1) k(k+ 1), E(v,0) :=∞.(22) By Theorem 2, it suffices to find a block design with k=k∗∈K∗(v, ϵ)to construct an optimalϵ-LDP scheme. Remark 2 guarantees the existence of such a(v, k∗, ϵ)-block design yielding a scheme equivalent to the optimal SS [11]. IV. MAINRESULTS Our main contribution is tocompletelycharacterize the optimal PUT for the discrete distribution estimation under the ULDP constraint, i.e., the characterization ofM∗(w, v, ϵ)for all possible(w, v, ϵ). Theorem 3.Forw > v≥1andϵ >0, M∗(w, v, ϵ) = sup α∈[0,1]inf t∈∆vM(α, t),(23) whereM(α, t) =P3 i=1Mi(α, t)such that2 M1(α, t) = (v−1)2 v(eϵ−1)2Pv k=1tkk(v−k) (αk(eϵ−1)+v)(keϵ+v−k),(24) M2(α, t) =(w−v−1)(1−α) (w−v)(eϵ−1)Pv k=1tkk keϵ+v−k,(25) M3(α, t) =w(1−α) v(w−v)(eϵ−1)Pv k=1tkk αk(eϵ−1)+v.(26) Here,M(α, t)is concave-convex and has a saddle point (α∗, t∗), which givessupαinftM(α, t) =M(α∗, t∗).3 For Theorem 3, the converse and the achievability parts are shown in Sections V and VI, respectively. The concave-convex property ofM(α, t)and the existence of a saddle point is proved in Appendix B. For converse, we first extend two previous results: (i) generalize the uniform asymptotic Cram ´er–Rao lower bound [31] introduced in Section III-B, and (ii) show that it suffices to consider only a newly defined class ofextremal ULDP mechanismsas optimal, in a manner analogous to the ex- tremal LDP mechanisms in [28]. Then, we introduce a novel argument ofdecomposingthe data distributionPinto three components: (i) the relative values ofP x’s over sensitivex, (ii) the relative values ofP x’s over non-sensitivex, and (iii) the total probability of sensitive dataP(X S). Using this, we derive a lower bound onM∗(w, v, ϵ)which can be interpreted as thesum of CRLBs for estimating each component. EachM i in Theorem 3 can be intuitively interpreted as the CRLB for estimating thei-th component of the data distributionP. For achievability, we propose a class ofutility-optimized block design schemes(uBD schemes). Each uBD scheme is 2Ifv= 1, we regardM 1(α, t) = 0. Ifv≥2andt=δ(v;v), we regard M1(α, t) =∞. 3(α∗, t∗)∈[0,1]×∆ vis a saddle point ofMifM(α∗, t∗) = supα∈[0,1] M(α, t∗) = inf t∈∆vM(α∗, t)[36].associated with parameters(α, t)appearing",
    "data distributionP. For achievability, we propose a class ofutility-optimized block design schemes(uBD schemes). Each uBD scheme is 2Ifv= 1, we regardM 1(α, t) = 0. Ifv≥2andt=δ(v;v), we regard M1(α, t) =∞. 3(α∗, t∗)∈[0,1]×∆ vis a saddle point ofMifM(α∗, t∗) = supα∈[0,1] M(α, t∗) = inf t∈∆vM(α∗, t)[36].associated with parameters(α, t)appearing in Theorem 3, and we show that if(α∗, t∗)is a saddle point ofM, then every uBD scheme with parameter(α, t) = (α∗, t∗)is asymptotically optimal. The privacy mechanismQof a uBD scheme builds upon the block design mechanisms [29] in Definition 3, with nontrivial modifications. The distinguishing idea is to exploit a mixtureof multiple BD mechanisms, rather than restricting to a single BD mechanism. The parametertrepresents the mixture proportions, wheret kdenotes the proportion of ak-uniform BD mechanism. The estimator sequence is constructed based on two ideas. First, we adopt the idea, also utilized in the con- verse proof, of decomposing the data distributionPinto three components, and estimate each component separately. Second, we propose a set of estimator sequences{ ˆP(α)}α∈[0,1] , each of which attains the Cram ´er-Rao lower bound at a specific data distributionP(α)∈∆ w. The supremum overαin Theorem 3 reflects the intuition that for the worst-case optimality, we use the estimator sequence that attains the worst-case CRLB. To attain the CRLB, the idea of ‘score-based linear estimator’ in [30] is utilized. Theorem 3 characterizes the optimal PUT in terms of a finite-dimensional concave–convex saddle-point optimization problem, which can be efficiently solved using existing op- timization solvers. Moreover, the following theorem gives a closed-form solution to this optimization problem for some regime, which is proved in Appendix E. Theorem 4. Case (a):Suppose that(w, v, ϵ)satisfies at least one of the following three conditions: (i)v= 1; (ii)v≥2andϵ≥ln w−v+r (w−1)(w−2) 2! ; (iii)v= 2andϵ≤ln 1 +r 2(w−2) w−1! ; Then, the following(α∗, t∗)is a saddle point ofM, i.e., M∗(w, v, ϵ) =M(α∗, t∗): α∗=\u0014v(eϵ−1−w+v) w(eϵ−1)\u0015 +, t∗=δ(1;v),(27) where[z] +:= max(0, z). Case (b):Suppose that(w, v, ϵ)satisfiesv≥4andϵ≤ lnq (v−1)(v−2) 2. Then, for eachk∗∈K∗(v, ϵ)∩[2 :v−1], the following(α∗, t∗)is a saddle point ofM, i.e.,M∗(w, v, ϵ) = M(α∗, t∗): α∗= 1, t∗=δ(k∗;v).(28) Here,K∗(v, ϵ)is defined in(20).4 The closed-form expression in Theorem 4 allows us to verify the optimality or suboptimality of existing schemes, as presented in the following corollary. Corollary 5.If(w, v, ϵ)corresponds to Case (a) of Theo- rem 4, uRR [15] is asymptotically optimal. 4Note thatlnq (v−1)(v−2) 2=E(v,1), whereEis in (22). Hence K∗(v, ϵ)∩[2 :v−1]̸=∅. 6 Corollary 6.If(w, v, ϵ)corresponds to Case (b) of Theorem 4 with strict inequality with respect toϵ, i.e.,v≥4andϵ < lnq (v−1)(v−2) 2, the uSS scheme is suboptimal. In other words, for the uSS scheme(Q uSS,k,ˆPuSS,k)with parameterk∈[v− 1], M∗(w, v, ϵ)<min k∈[1:v−1]R(Q uSS,k,ˆPuSS,k).(29) Corollary 5 is a special case of Corollary 14 and Corollary 6 is proved in Appendix F. Corollary 5 follows from the observa- tion that our uBD scheme with the optimal parameter coincides with uRR in the corresponding regime, resembling the fact that the optimal BD scheme in the low-privacy regime reduces to RR [10]. Since the proposal of uRR in [15], this is the first proof of its",
    "follows from the observa- tion that our uBD scheme with the optimal parameter coincides with uRR in the corresponding regime, resembling the fact that the optimal BD scheme in the low-privacy regime reduces to RR [10]. Since the proposal of uRR in [15], this is the first proof of its optimality in a certain regime. Corollary 6 shows that uSS scheme, which is the best known previous scheme, is suboptimal, thereby demonstrating a strict gap between uBD and the previously known ULDP schemes. We conclude this section with the following remark on the relation with the trivial converse bound in Remark 1. Remark 3.In the regime of Case (b) in Theorem 4, it can be easily seen that M(α∗, t∗) =RBD(v, k∗, ϵ) =M∗(v, v, ϵ),(30) whereRBDis in(18). Thus, the converse bound in Remark 1 is tight forv≥4andϵ≤lnq (v−1)(v−2) 2. V. CONVERSE The goal of this section is to prove the converse part of Theorem 3, that is, to show M∗(w, v, ϵ)≥sup α∈[0,1]inf t∈∆vM(α, t).(31) This section consists of three subsections. Section V-A pro- vides a generalized uniform version of asymptotic CRLB [31], and Section V-B provides the notion ofextremal ULDP mechanismsanalogous to extremal LDP mechanisms [28]. Finally, Section V-C finishes the converse proof by using adistribution decompositionargument, which also plays an important role in the construction of proposed schemes. A. Uniform Asymptotic Cram ´er-Rao Lower Bound As a first step for the converse proof, we show the general- ization of the uniform result in [31], which we call theuniform asymptotic Cram ´er-Rao lower bound (uniform asymptotic CRLB). Our version of uniform asymptotic CRLB covers the general private discrete distribution estimation, not only private estimation under LDP or ULDP constraint, and thus we expect that it has possible applications to other private estimation problems. Moreover, we provide a much shorter proof for the uniform asymptotic CRLB compared to [31]. First, we present the necessary definitions for CRLB. Definition 4.Letw∈N ≥2be given. LetQbe a conditional distribution from[w]to a finite setY, andP∈∆◦ w. For eachh∈ ⃗∆w, thescoreofQatPin directionhis a function ηP,Q;h : supp(Q)→R, defined by ηP,Q;h(y) =d dδ(logQ P+δh(y))| δ=0 (32) =⟨η P,Q(y), h⟩.(33) Here,η P,Q: supp(Q)→Rwis thescore vector, given by (ηP,Q(y))x=Q(y|x) QP(y),∀x∈[w].(34) Definition 5.TheFisher informationofQatPis a symmetric positive semidefinite bilinear mapJ P,Q:⃗∆2 w→R, defined as JP,Q(h, h′) = Cov Y∼Q P(ηP,Q;h(Y), η P,Q;h′(Y)).(35) If an orthonormal basisB= (h(1),···, h(w−1))of ⃗∆wis given, theFisher information matrixwith respect to such basis is the(w−1)×(w−1)matrixJ P,Q;B such that (JP,Q;B )i,j=JP,Q(h(i), h(j)). We omitBfrom the subscript if it is clear from the context. Remark 4.Fisher information matrices with respect to differ- ent orthonormal bases are conjugate via an orthogonal matrix. Therefore, many expressions involving trace of a function of Fisher information matrix (such astr −1(JP,Q)in(13)) are invariant under the choice of basis. If so, we may omit the subscriptBwithout ambiguity. Next, we present the notion of degradedness [37], which gives a systematic way of reducing the space of privacy mech- anisms to be optimized, as [31] reduced the optimization over all LDP mechanisms to that over extremal LDP mechanisms. Intuitively speaking, we say thatQdegrades ˜Qif ˜Qcan be simulated",
    "subscriptBwithout ambiguity. Next, we present the notion of degradedness [37], which gives a systematic way of reducing the space of privacy mech- anisms to be optimized, as [31] reduced the optimization over all LDP mechanisms to that over extremal LDP mechanisms. Intuitively speaking, we say thatQdegrades ˜Qif ˜Qcan be simulated by applying some post-processing to the output of Q. The formal definitions are as follows. Definition 6.For finite setsX,Y, ˜Yand two conditional distributionsQ:X → P(Y)and ˜Q:X → P( ˜Y), we sayQdegrades ˜Q, and denoteQ⊒ ˜Q, if there exists a conditional distributionT:Y → P( ˜Y)such that ˜Q(˜y|x) =P y∈YQ(y|x)T(˜y|y)for everyx∈ Xand˜y∈ ˜Y. Definition 7.For a finite setXand two classesQ, ˜Qof conditional distributions fromXto some finite set, we say Qdegrades ˜Q, and denoteQ ⊒ ˜Qif for every ˜Q∈ ˜Q, there existsQ∈ Qwhich degrades ˜Q. Now, we state the uniform asymptotic CRLB. Theorem 7(Uniform Asymptotic Cram ´er-Rao Lower Bound). Letw∈N ≥2be given, and letX= [w]. Let ˜Qbe a class of conditional distributions fromXto some finite set. Suppose that there exist afixedfinite setYand acompactsubsetQ of conditional distributions fromXtoY, which degrades ˜Q. Then, we have lim n→∞inf ˜Q∈˜Qinf ˆPn·R n(˜Q,ˆP)≥sup P∈∆◦winf Q∈Qtr−1(JP,Q).(36) We prove Theorem 7 by refining the proof of asymptotic CRLB in [32]. 7 Proof of Theorem 7.It suffices to show that for each fixed P∈∆◦ w, we have lim n→∞inf ˜Q∈˜Qinf ˆPn·R n(˜Q,ˆP)≥inf Q∈Qtr−1(JP,Q).(37) Hence, from now on, we fixP∈∆◦ w. a) Instantiation of Proof in [32]:The first half of the proof is the same as the proof of asymptotic CRLB in [32]. For the detailed reason for each step, refer to [32, Section 29.1]. SinceP∈∆◦ w, there existsD >0such that whenever h∈ ⃗∆w,∥h∥ ≤D, we haveP+h∈∆◦ w. Let us choose an orthonormal basisB= (h(1),···, h(w−1))of⃗∆w, and for eachθ∈Rw−1,∥θ∥ ≤R, let ˜P(θ):=P+Pw−1 i=1θih(i)∈∆◦ w. Then, whenevern≥(w−1)2/D4, we can bound the worst- case MSE by the average MSE, Rn(˜Q,ˆP)≥E Θ∼ϕ nh Rn(˜Q,ˆP;˜P(Θ))i ,(38) where underΘ∼ϕ n, each ofΘ 1,···,Θ w−1 is i.i.d., supported on[−n−1/4, n−1/4], and following the PDFf(z) = n1/4cos2\u0010 n1/4πz 2\u0011 .5Then, we apply another variant of CRLB, calledBayesian CRLB, resulting that wheneverQ⊒ ˜Qandn≥(w−1)2/D4, we have inf ˆPEΘ∼ϕ nh Rn(˜Q,ˆP;˜P(Θ))i ≥ tr−1(n·E Θ∼ϕ n[J˜P(Θ),Q] +π2n1/2Iw−1).(39) b) New Techniques for the Uniform Result:After that, to derive the uniform result, we establish a technical proposi- tion which shows that{P7→JP,Q}Qisequicontinuousin some sense. The formal statement is in Proposition 15 in Appendix C-A. Using this proposition and then−1/4order of the diameter of the support ofΘ, we derive in Appendix C-B that there exist constantsC >0andN∈N(which only depend on(w, P)) such that for everyn≥Nand every conditional distributionQfrom[w], we have EΘ∼ϕ n[J˜P(Θ),Q]⪯(1 +Cn−1/4)JP,Q.(40) Using a standard fact [39] thatA⪯BimpliesA−1⪰B−1 for positive definite matricesA, B, we obtain that inf ˆPRn(˜Q,ˆP)≥1 nRn(Q)(41) for everyQ⊒ ˜Q, where Rn(Q) = tr −1((1 +Cn−1/4)JP,Q+π2n−1/2Iw−1).(42) Thus, we have lim n→∞inf ˜Q∈˜Qinf ˆPn·R n(˜Q,ˆP)≥lim n→∞inf Q∈QRn(Q).(43) Note that Rn(Q)↑tr −1(JP,Q)asn→ ∞. Using com- pactness ofQand Dini’s theorem [40, Thm. 7.13], we show in Appendix C-C that lim n→∞inf Q∈QRn(P, Q) = inf Q∈Qtr−1(JP,Q).(44) This finishes the proof of Theorem 7. 5This PDF is shown [32], [38] to minimize the ‘prior Fisher information’ over all PDFs supported",
    "Note that Rn(Q)↑tr −1(JP,Q)asn→ ∞. Using com- pactness ofQand Dini’s theorem [40, Thm. 7.13], we show in Appendix C-C that lim n→∞inf Q∈QRn(P, Q) = inf Q∈Qtr−1(JP,Q).(44) This finishes the proof of Theorem 7. 5This PDF is shown [32], [38] to minimize the ‘prior Fisher information’ over all PDFs supported on[−n−1/4, n−1/4].B. Extremal ULDP Mechanisms As a next step, to utilize the uniform asymptotic CRLB in the ULDP setup, we find a certaincompactset of con- ditional distributions which degrades the class of all ULDP mechanisms. In short, we find such a set analogous to the set of extremal LDP mechanisms [28], which we call the set of extremal ULDP mechanisms. Definition 8.Let1≤v < w,ϵ >0be given, andX= [w], XS= [v]. A conditional distributionQis called anextremal (v, ϵ)-ULDP mechanismif the following holds: •Q:X → P(Y P⊔ YI), whereY P= 2[v]\\{∅}andY I= {{x}:x∈[v+ 1 :w]}. •There existsγ:Y P→R≥0such that ify∈ Y P, Q(y|x) =( γ(y)eϵ(ifx∈y) γ(y) (ifx /∈y).(45) •Ify∈ Y I, Q(y|x) = 1−X y′∈YPγ(y′)  1(x∈y).(46) We defineQE w,v,ϵ to be the set of all extremal(v, ϵ)-ULDP mechanisms. We can equivalently describe an extremal ULDP mechanism Qby its stochastic matrix as follows. The visualization of the stochastic matrix is in Figure 3. We identifyY P≃[1 : 2v−1] via a bijectiony↔Pv i=12i−1yi, and identifyY I≃[2v: 2v+w−v−1]via{x} ↔2v+x−v−1. First, we bring thestaircase pattern matrixin [28]: S(v):=1 v×2v−1+ (eϵ−1)BIN(v),(47) whereBIN(v)∈ {0,1}v×(2v−1)is the matrix whosey’th column corresponds to the binary representation ofy. Then, we setQ XS,YP=S(v)diag(γ), which is precisely an ex- tremalϵ-LDP mechanism [28]. Next, we setQ XN,YP= 1(w−v)×(2v−1)diag(γ). Intuitively, for givenQ XS,YP, we set each element ofQ XN,YPto be the minimum possible value while satisfying the ULDP constraint, so that the probability of returning invertible data given non-sensitive data is maxi- mized. Finally, we setQ XN,YIto be a constant multiple of an identity matrix, where the constantf(γ) = 1−P y′∈YPγ(y′) is determined byγso that for eachx∈ X N, thex’th row ofQ is summed up to one. Also, we setQ XS,YIto be the all-zero matrix. Clearly, every extremal(v, ϵ)-ULDP mechanism is a(v, ϵ)- ULDP mechanism. Also, note that among previously proposed mechanisms, uRR [15] and uHR [17] are equivalent to some extremal ULDP mechanisms.6It turns out that the set of all extremal ULDP mechanisms degrades the class of all ULDP mechanisms. Theorem 8.For every1≤v < wandϵ >0, we have QE w,v,ϵ⊒ Q w,v,ϵ . 6Here, the equivalence is up to relabelingYand the removal / addition of all zero columns ofQ. 8 S(3)= eϵ1eϵ1eϵ1eϵ 1eϵeϵ11eϵeϵ 111eϵeϵeϵeϵ  Q=\u0014YP YI XS S(v)diag(γ) 0v×(w−v) XN1(w−v)×(2v−1)diag(γ) f(γ)I w−v\u0015 Fig. 3: Visualization of staircase pattern matrix and extremal (v, ϵ)-ULDP mechanism. The proof is similar to the proof of degradation of LDP mechanisms by extremal LDP mechanisms in [41], [42], and we present the proof in Appendix C-D. To combine Theorems 7 and 8, it remains to show that QE w,v,ϵ is compact. It is straightforward to see that the given γ:Y P= 2[v]\\{∅} →R ≥0induces a valid extremal(v, ϵ)- ULDP mechanism (satisfyingP yQ(y|x) = 1for allx∈ X) if and only if X y∈Y Pγ(y)(1 + (eϵ−1)1(x∈y)) = 1,∀x∈[v].(48) Clearly, the set",
    "and 8, it remains to show that QE w,v,ϵ is compact. It is straightforward to see that the given γ:Y P= 2[v]\\{∅} →R ≥0induces a valid extremal(v, ϵ)- ULDP mechanism (satisfyingP yQ(y|x) = 1for allx∈ X) if and only if X y∈Y Pγ(y)(1 + (eϵ−1)1(x∈y)) = 1,∀x∈[v].(48) Clearly, the set of allγsatisfying (48) is compact, and hence QE w,v,ϵ is compact. Thus, by Theorems 7 and 8, we obtain M(w, v, ϵ)≥sup P∈∆◦winf Q∈QEw,v,ϵtr−1(JP,Q).(49) C. Distribution Decomposition for Tighter Bound Up to this point, we have established the converse bound by generalizing existing results. Now, we present our prominent idea ofdecomposingthe data distribution, which not only lies behind the converse proof, but also plays an important role in the construction of our proposed schemes in Section VI. Based on this idea, we present the complete converse proof at the end of this subsection. To formalize this idea, we introduce the following three mutually orthogonal subspaces ofRwwhich span the direction space ⃗∆win (3), H1:={h:h x= 0forx∈[v+ 1 :w],vX x=1hx= 0},(50) H2:={h:h x= 0forx∈[v],wX x=v+1hx= 0},(51) H3:= span\u0012\u0014(w−v)1 v −v1 w−v\u0015\u0013 .(52) Letd i:= dim(H i), that is,(d 1, d2, d3) = (v−1, w−v−1,1). Also, letΠ i:Rw→ H ibe the orthogonal projection onto Hi. Then, intuitively,Π 1(P)andΠ 2(P)represent the relative values ofP x’s over sensitivexand that over non-sensitive x, respectively. Also,Π 3(P)designates the probability of sensitive dataP(X S), as it is easy to see that Π3(P) =wP(X S)−v vw(w−v)\u0014(w−v)1 v −v1 w−v\u0015 .(53)Additionally, we define the class of distributionsP(α)∈∆ w, α∈[0,1], by the mixture of the uniform distribution onX S and the uniform distribution onX N, with a rateα: 1−α. More precisely, we have P(α) x=( α v(ifx∈ X S) 1−α w−v(ifx∈ X N).(54) The key idea for the converse proof is to give a lower bound on the optimal PUT, which can be intuitively interpreted as the sum of CRLBs for estimatingΠ i(P)overi= 1,2,3. Building on this idea, we now present the formal converse proof. Proposition 9.For everyw > v≥1andϵ >0, we have M∗(w, v, ϵ)≥sup α∈[0,1]inf t∈∆vM(α, t).(55) Proof.First, we further bound (49) by replacing the supremum over allP∈∆◦ wwith the supremum overP(α),α∈(0,1), obtaining M∗(w, v, ϵ)≥sup α∈(0,1)inf Q∈QEw,v,ϵtr−1\u0000 JP(α),Q\u0001 .(56) We choose an orthonormal basisB={h(1),···, h(w−1)}of H, such that the first(v−1)vectors form a basis ofH 1, the next(w−v−1)vectors form a basis ofH 2, and the last vectorh(w−1)is the unit vector inH 3. Now, letQ∈ QE w,v,ϵ andα∈(0,1)be given. Let B1, B2, B3be the principal submatrices ofJP(α),Q;B, where B1is formed by selecting the first(v−1)rows and columns, B2by selecting the nextw−v−1rows and columns, andB 3 by selecting the last row and column. Using the block matrix inversion formula [36], we derive in Appendix C-E that tr−1\u0000 JP(α),Q\u0001 ≥3X i=1tr−1(Bi).(57) By the arithmetic mean-harmonic mean inequality, we have tr−1(Bi)≥d2 i tr(Bi). Also, we derive in Appendix C-F that tr(B i) =d2 i Mi(α, t(Q)),∀i∈ {1,2,3},(58) wheret(Q)∈∆ vis the PMF of|Y′|whenY′∼QP(1)(recall that in an extremal ULDP mechanism,Yis a subset ofX). Thus, we obtain tr−1\u0000 JP(α),Q\u0001 ≥3X i=1Mi(α, t(Q)) =M(α, t(Q)).(59) Since this holds for everyQ∈ QE w,v,ϵ andα∈(0,1), we have M∗(w, v, ϵ)≥sup α∈(0,1)inf t∈∆vM(α,",
    "in Appendix C-F that tr(B i) =d2 i Mi(α, t(Q)),∀i∈ {1,2,3},(58) wheret(Q)∈∆ vis the PMF of|Y′|whenY′∼QP(1)(recall that in an extremal ULDP mechanism,Yis a subset ofX). Thus, we obtain tr−1\u0000 JP(α),Q\u0001 ≥3X i=1Mi(α, t(Q)) =M(α, t(Q)).(59) Since this holds for everyQ∈ QE w,v,ϵ andα∈(0,1), we have M∗(w, v, ϵ)≥sup α∈(0,1)inf t∈∆vM(α, t).(60) By continuity ofMand compactness of∆ v, we can replace sup α∈(0,1)bysup α∈[0,1]. This ends the converse proof. VI. ACHIEVABILITY: UTILITY-OPTIMIZEDBLOCKDESIGN SCHEME In this section, we propose a new class of ULDP schemes calledutility-optimized block design (uBD) schemes. After that, we show that an optimized uBD scheme is asymptotically optimal. 9 Fig. 4: Visualization of(5,3, ϵ, t)-uBD mechanism. A. Construction of uBD Schemes a) Privacy mechanism:We first propose auBD mecha- nismwhich perturbs the data. To construct a uBD mechanism, we first prepare a collection of block design mechanisms (Q(k): [v]→ P(2[v]))k∈[v] , where each ofQ(k)has the uniformity parameterk. This guarantees that the output spaces ofQ(k)’s are disjoint. Also, we prepare a mixture proportion t∈∆ v. Then, a uBD mechanismQis constructed as follows. The visualization ofQis in Figure 4. If a sensitive data x∈ X Sis given as the input of a uBD mechanismQ, then a uBD mechanism randomly chooses a block design mechanism Q(k), each with probabilityt k, respectively, and simulates the chosenQ(k)to generate its output. Hence, in terms of a stochastic matrix,Q XS,YPis the horizontal stack of the stochastic matrices ofQ(k)multiplied byt k. After that, we setQ XN,Yso that the resultantQbecomes anextremal(v, ϵ)- ULDP mechanism, which is defined in Definition 8. That is, for givenx∈ X N,Qeither outputs one of the edges from the support of someQ(k), or directly outputs its inputx in the form of an edge{x}. Here, the output distribution Q(·|x)is suitably determined to maximize the probability that a mechanism outputs{x}while satisfying the(v, ϵ)-ULDP constraint. The precise definition of a uBD mechanism is as follows. Definition 9.Letw > v≥1,X= [w],X S= [v],ϵ >0. Suppose that the following are given: •t∈∆ v, •A collection of sets(E k)k∈[v],Ek⊂2XS, such that for eachk,(X S,Ek)is a(v, b k, rk, k, λ k)-block design forsome(b k, rk, λk). Then, the corresponding(w, v, ϵ, t)-uBD mechanismis de- fined as an extremal(v, ϵ)-ULDP mechanism as in Definition 8, which is induced from the followingγ:Y P→R≥0: γ(y) =t k·1(y∈ E k) rk(eϵ−1) +b k,∀ysuch that|y|=k.(61) By the regularity of block design, it is straightforward to check that thisγsatisfies the necessary and sufficient condition to induce an extremal ULDP mechanism in (48). Note that for every(w, v, ϵ, t), there exists a(w, v, ϵ, t)-uBD mechanism, by setting each of(X S,Ek)to be the complete block design in Remark 2. Also, to specify a uBD mechanism, it suffices to designateE konly fork∈[v]such thatt k>0, not necessary for allk∈[v]. b) Estimator:Now, we propose a class of estimators for a uBD mechanism. The main ideas for our proposed estimators are as follows. (i) First, as we derive the lower bound based on a version of CRLB, we target to construct an estimator which saturates CRLB. For this purpose, we exploit the score vector in (34). (ii) Second, by exploiting the decomposition",
    "mechanism. The main ideas for our proposed estimators are as follows. (i) First, as we derive the lower bound based on a version of CRLB, we target to construct an estimator which saturates CRLB. For this purpose, we exploit the score vector in (34). (ii) Second, by exploiting the decomposition ⃗∆w=H 1⊕ H2⊕ H 3introduced in Section V-C, we construct an estimator based on the relationship between the projec- tions of the unknown distributionPand the score vector onto each ofH i. Based on these ideas, we observe that by the symmetries of block designs, for any givenα∈(0,1), the expectation of the projection of the score vector of a uBD mechanism at 10 P(α)(defined in (54)) is a linear function of the projection ofP−P(α), where the linear coefficient is related withM i apperaing in Theorem 3 andd i:= dim(H i). Proposition 10.LetQbe a(w, v, ϵ, t)-uBD mechanism, and α∈(0,1). Then, for eachi= 1,2,3, we have EY∼Q P[Πi(ηP(α),Q(Y))] = Π i(P−P(α))di Mi(α, t).(62) Recall thatΠ i:Rw→ H iis the orthogonal projection onto Hidefined in(50)-(52), and(d 1, d2, d3) = (v−1, w−v−1,1). The proof of Proposition 10 is in Appendix D-A. Using this, we can obtain an unbiased estimator for each ofαby inverting the linear relationship for eachi= 1,2,3to estimate eachΠ i(P−P(α)), and aggregating the results fori’s. An exception is the case ofv≥2,t=δ(v;v), where the linear relationship is non-invertible due toM 1(α, t) =∞. We note that the idea of score-based linear estimator was used in [30] for private estimation under LDP and a 1-bit communication constraint. Definition 10.LetQbe a(w, v, ϵ, t)-uBD mechanism. Sup- pose thatt̸=δ(v;v)whenv≥2. For eachα∈[0,1], we define a sequence of unbiased estimators ˆP(α)corresponding toQ, by ˆP(α) n(yn) =1 nPn j=1ˆP(α) 1(yj), where (i)Forα∈(0,1), ˆP(α) 1is given by7 ˆP(α) 1(y) =P(α)+3X i=1Mi(α, t) diΠi(ηP(α),Q(y)).(63) (ii)Forα∈ {0,1}, ˆP(α) 1 is given by the continuous extension of(63). We call such a pair(Q, ˆP(α))a(w, v, ϵ, α, t)-uBD scheme. To facilitate its implementation, we provide the table of each component ofΠ i(ηP(α),Q(y))for a uBD mechanism (or for a general extremal ULDP mechanism) in Table I in the beginning of the appendix. B. Estimation Error and Optimality of uBD Schemes The estimation error of a uBD scheme can be represented using the objective functionM(α, t)in Theorem 3 and its derivative with respect toα. Note that similar to block design schemes [29], given(w, v, ϵ), the estimation error of a uBD scheme only depends on(α, t), and does not depend on specific choices of block designs(X S,Ek). Proposition 11.Let(Q, ˆP)be a(w, v, ϵ, α, t)-uBD scheme. Suppose thatt̸=δ(v;v)whenv≥2. Then Rn(Q,ˆP) =1 nR1(Q,ˆP), R(Q, ˆP) =R 1(Q,ˆP),(64) and R(Q, ˆP) = sup β∈[0,1]\u0014 −w v(w−v)(β−α)2 + (β−α)F(α, t) +M(α, t)\u0015 ,(65) 7Here, ifd i= 0, we regardMi(α,t) diΠi(ηP(α),Q(y)) = 0.whereF(α, t) =∂ ∂αM(α, t). Proof.Since the estimator is unbiased andY 1,···, Y nare i.i.d. givenP, (64) is straightforward. Next, by the symmetry of a uBD scheme, the similar argument as [29] shows that wheneverP(X S) =β, we have R1(Q,ˆP;P)≤R 1(Q,ˆP;P(β)).(66) The detailed proof of (66) is in Appendix D-B. Hence, we have R(Q, ˆP) =R 1(Q,ˆP) = sup",
    "unbiased andY 1,···, Y nare i.i.d. givenP, (64) is straightforward. Next, by the symmetry of a uBD scheme, the similar argument as [29] shows that wheneverP(X S) =β, we have R1(Q,ˆP;P)≤R 1(Q,ˆP;P(β)).(66) The detailed proof of (66) is in Appendix D-B. Hence, we have R(Q, ˆP) =R 1(Q,ˆP) = sup β∈[0,1]R1(Q,ˆP;P(β)).(67) In Appendix D-C, we show that R1(Q,ˆP;P(β)) =−w v(w−v)(β−α)2+ (β−α)F(α, t) +M(α, t),(68) which proves (65). From this, we derive the asymptotic optimality of an opti- mized uBD scheme and finish the proof of Theorem 3. Corollary 12.Let(α∗, t∗)be a saddle point ofM(α, t), and let(Q, ˆP)be a(w, v, ϵ, α∗, t∗)-uBD scheme. Then,(Q, ˆP)is asymptotically optimal, andM∗(w, v, ϵ) =M(α∗, t∗). Proof.From the concavity ofM(α, t)onα, we have F(α∗, t∗)  = 0 (ifα∗∈(0,1)) ≤0 (ifα∗= 0) ≥0 (ifα∗= 1).(69) From this, it is clear that when(α, t) = (α∗, t∗), the supremum in (65) is attained atβ=α∗, and thusR(Q, ˆP) =M(α∗, t∗). This implies thatM(α∗, t∗)≥M∗(w, v, ϵ). Since we have proved in Section V thatM(α∗, t∗)≤M∗(w, v, ϵ), we conclude thatM(α∗, t∗) =M∗(w, v, ϵ), and thusR(Q, ˆP) = M∗(w, v, ϵ). Remark 5.Corollary 12 means that an optimized uBD scheme achieves the minimum possibleasymptoticerror amongall (v, ϵ)-ULDP schemes, which are not necessarily unbiased. Moreover, by the same argument as in [29], we can also show that for each offixedn, an optimized uBD scheme attains the minimum possible estimation error among allunbiased(v, ϵ)- ULDP schemes. C. Closed-form Solution: Simple uBD Schemes Note that in all regimes where we find the closed-form solution as in Theorem 4,t∗is a point mass. There is a special behavior of uBD schemes such thattis a point mass, which not only derives a simpler description of such schemes, but also plays an important role in the proof of Theorem 4. In short, the estimator ˆP(α)does not depend onα. Proposition 13.Letk∈[v]. LetQbe a(w, v, ϵ, δ(k;v))- uBD mechanism. Assume thatk̸=vwhenv≥2. Then, the estimators ˆP(α)in Definition 10 satisfy ˆP(α)=ˆP(α′)for everyα, α′∈[0,1]. More precisely, the following holds for allα∈[0,1]: 11 (i)Forx∈ X S= [v], (ˆP(α) 1(y))x=   1 +v−1 k(eϵ−1)(ifx∈y) −(k−1)(eϵ−1)+(v−1) (v−k)(eϵ−1)(ifx /∈y, y∈ Y P) −1 k(eϵ−1)(ify∈ Y I).(70) (ii)Forx∈ X N= [v+ 1 :w], (ˆP(α) 1(y))x=(k(eϵ−1) +v) k(eϵ−1)1(x∈y).(71) Proposition 13 can be shown by a direct calculation of (63) to verify that it is equal to the above expression, hence we omit the proof. Due to this proposition, we say a(w, v, ϵ, k)-simple uBD schemeto mean a(w, v, ϵ, α, δ(k;v))-scheme, without ambiguity on specifyingα. Proposition 13 can be used to implement the estimator for a simple uBD scheme in a simpler way than using (63). As a remark, the(w, v, ϵ,1)-simple uBD scheme is equivalent to uRR [15]. Granting Theorem 4 about the closed-form solution in some regimes, which is proved in Appendix E, we deduce a closed-form characterization of optimal uBD schemes in the corresponding regimes. Interestingly, for Case (a), we deduce the optimality of uRR [15], which has not been shown before. Corollary 14. (a)Suppose that at least one of the following three condi- tions holds: (i)v= 1; (ii)v≥2andϵ≥ln w−v+r",
    "Appendix E, we deduce a closed-form characterization of optimal uBD schemes in the corresponding regimes. Interestingly, for Case (a), we deduce the optimality of uRR [15], which has not been shown before. Corollary 14. (a)Suppose that at least one of the following three condi- tions holds: (i)v= 1; (ii)v≥2andϵ≥ln w−v+r (w−1)(w−2) 2! ; (iii)v= 2andϵ≤ln 1 +r 2(w−2) w−1! ; Then, uRR [15] (equivalently, the(w, v, ϵ,1)-simple uBD scheme) is asymptotically optimal. (b)Suppose thatv≥4andϵ≤lnq (v−1)(v−2) 2. Then, for eachk∗∈K∗(v, ϵ)∩[2 :v−1], any(w, v, ϵ, k∗)-simple uBD schemes are asymptotically optimal. Remark 6.If(X S,Ek)is the complete block design in Remark 2, the corresponding(w, v, ϵ, k)-simple uBD scheme can be seen as a modification of the SS scheme [11]. However, this uBD scheme is not equivalent to the uSS scheme [27], especially because the uBD scheme matches precisely one invertible element to each non-sensitive element, whereas the uSS scheme matches more than one invertible element, thereby introducing additional variance in the estimation. Remark 7.The main advantage of considering the class of uBD schemes is that we can find an optimal and “communication-efficient” ULDP scheme, similar to the class of block design schemes [29]. Here, the communication cost is defined aslog2|supp(Q)|bits per client. Specifically, a straightforward generalization of the argument in [29, Section IV-B] shows that in the regime of Case (b) in Theorem 4, if there exists a block design withb=vandk∈K∗(v, ϵ)∩[2 : v−1], then there exists a simple uBD scheme which is notonly optimal, but also has communication cost oflog2w. This amount of communication is the smallest possible communica- tion cost required for a consistent estimation [29], [43]. Note that in the regime of Case (a), uRR [15] achieves optimal PUT and the communication cost oflog2w. The region where a closed-form solution cannot be obtained through Theorem 4 isv≥2andϵ∈(ϵ L, ϵH), where ϵL=  ln\u0012 1 +q 2(w−2) w−1\u0013 (ifv= 2) lnq (v−1)(v−2) 2(ifv≥3),(72) ϵH= ln w−v+r (w−1)(w−2) 2! .(73) In the next section, while performing experiments on a real dataset, we find the optimal parameters(α, t)byMATLAB Optimization Toolboxin the region above and analyze the result. VII. EXPERIMENTS We empirically evaluate the PUTs of uBD and baseline schemes [15], [17], [26], [27]. We use the American Commu- nity Survey (ACS) 2023 1-year public use microdata sample (PUMS) [44], which contains records forn= 1,732,343 individuals. Six attributes are retained: age, educational at- tainment, marital status, disability record, employment sta- tus, and income-to-poverty ratio, whose category counts are (4,2,2,3,2,3), giving288in total; excluding empty nominal categories yieldsw= 277unique input alphabets. We applied two alternative criteria: (i)Stringent criterion(v= 35): A value is sensitive only if all three conditions hold simultaneously: •educationally disadvantaged, widowed/divorced, or disabled, •absence from the labor force, •income below the poverty threshold. (ii)Permissive criterion(v= 253): A value is sensitive if it satisfies any one of the above conditions. The stringent rule labels79,141records as sensitive, while the permissive rule identifies1,022,617. For each criterion, we randomly sample50,000users twenty times and report the average PUT across these subsam- ples. Guided by Theorem 4, we partition the privacy budget into three intervals:[0.1, ϵ L],[ϵL, ϵH], and[ϵ H,10],",
    "one of the above conditions. The stringent rule labels79,141records as sensitive, while the permissive rule identifies1,022,617. For each criterion, we randomly sample50,000users twenty times and report the average PUT across these subsam- ples. Guided by Theorem 4, we partition the privacy budget into three intervals:[0.1, ϵ L],[ϵL, ϵH], and[ϵ H,10], where (ϵL, ϵH)are in (72)-(73). Figs. 5 and 6 show that our scheme consistently outperforms all baselines, regardless of privacy regimes and sensitive-set sizes. In the intermediate regime ϵ∈(ϵ L, ϵH), where a closed-form expression of the optimal parameter(α∗, t∗)is unknown, we plot the empirical values oft∗in Fig. 7. Remarkably,t∗collapses to the two-point formt∗= (t 1, t2,0, . . . ,0).8This observation leads us to carefully conjecture that utilizing a single block design is not sufficient, but utilizing just two block designs—those with k= 1andk= 2—is sufficient to attain the optimal PUT 8Components below10−8are treated as numerical noise and forced to zero; after which the vector is renormalized. 12 (a)ϵ∈[0.1, ϵ L] (b)ϵ∈[ϵ L, ϵH] (c)ϵ∈[ϵ H,10] Fig. 5:ϵvs. PUT where(w, v) = (277,35). (a)ϵ∈[0.1, ϵ L] (b)ϵ∈[ϵ L, ϵH] (c)ϵ∈[ϵ H,10] Fig. 6:ϵvs. PUT where(w, v) = (277,253). (a)(w, v) = (277,35) (b)(w, v) = (277,253) Fig. 7:ϵvs.t∗ i, where the vertical lines representϵ Landϵ H. throughoutϵ∈(ϵ L, ϵH). The MATLAB codes for experiments are available at https://github.com/phy811/uBD. VIII. CONCLUSION In this paper, we characterized the optimal PUT for discrete distribution estimation under the ULDP constraint. We proved the converse by leveraging a uniform asymptotic Cram ´er-Raolower bound, a reduction to extremal ULDP mechanisms, and a distribution decomposition argument. Also, we pro- posed a class of PUT-optimal uBD schemes, obtained through nontrivial modifications of block design mechanisms and by employing a score-based estimator together with a distribution decomposition argument. An interesting direction for future research is to derive 13 closed-form solutions to our optimization problem in the intermediate privacy regime, thereby specifying the optimal parameters for the estimatorα∗and the mixture proportion t∗. Also, we could consider other formulations of PUT, such as instance-optimality setup [45] rather than worst-case setup, and considering generalℓ ploss orf-divergence loss (such as relative entropy) other than MSE loss (ℓ 2). It is also of interest to extend the analysis to a broader class of privacy metrics beyond LDP and ULDP. We believe that the analytical tools developed in this paper–particularly the uniform asymptotic Cram ´er-Rao lower bound–will provide a valuable foundation for pursuing these directions. REFERENCES [1] S. P. Kasiviswanathan, H. K. Lee, K. Nissim, S. Raskhodnikova, and A. Smith, “What can we learn privately?,”SIAM J. Comput., vol. 40, no. 3, pp. 793–826, 2011. [2] J. C. Duchi, M. I. Jordan, and M. J. Wainwright, “Local privacy and statistical minimax rates,” inProc. IEEE 54th Annu. Symp. Found. Comput. Sci., pp. 429–438, 2013. [3] C. Dwork, A. Roth,et al., “The algorithmic foundations of differential privacy,”Foundations and Trends® Theoretical Comput. Sci., vol. 9, no. 3–4, pp. 211–407, 2014. [4] B. Ding, J. Kulkarni, and S. Yekhanin, “Collecting telemetry data privately,” inProc. Adv. Neural Inf. Process. Syst., vol. 30, 2017. [5] Apple Differential Privacy Team, “Learning with privacy at",
    "Roth,et al., “The algorithmic foundations of differential privacy,”Foundations and Trends® Theoretical Comput. Sci., vol. 9, no. 3–4, pp. 211–407, 2014. [4] B. Ding, J. Kulkarni, and S. Yekhanin, “Collecting telemetry data privately,” inProc. Adv. Neural Inf. Process. Syst., vol. 30, 2017. [5] Apple Differential Privacy Team, “Learning with privacy at scale.” Apple Machine Learning Research, Dec. 2017. [6] Apple Machine Learning Research, “Understanding aggregate trends for apple intelligence using differential privacy.” Apple Machine Learning Research blog, Apr. 2025. [7] ´U. Erlingsson, V . Pihur, and A. Korolova, “RAPPOR: Randomized aggregatable privacy-preserving ordinal response,” inProc. 2014 ACM SIGSAC Conf. Comput. Commun. Secur., pp. 1054–1067, 2014. [8] A. Bittau, ´U. Erlingsson, P. Maniatis, I. Mironov, A. Raghunathan, D. Lie, M. Rudominer, U. Kode, J. Tinnes, and B. Seefeld, “Prochlo: Strong privacy for analytics in the crowd,” inProc. 26th Symp. Operating Syst. Princ., pp. 441–459, 2017. [9] I. Issa, A. B. Wagner, and S. Kamath, “An operational approach to information leakage,”IEEE Trans. Inf. Theory, vol. 66, no. 3, pp. 1625– 1657, 2020. [10] S. L. Warner, “Randomized response: A survey technique for eliminating evasive answer bias,”J. Am. Stat. Assoc., vol. 60, no. 309, pp. 63–69, 1965. [11] M. Ye and A. Barg, “Optimal schemes for discrete distribution estima- tion under locally differential privacy,”IEEE Trans. Inf. Theory, vol. 64, no. 8, pp. 5662–5676, 2018. [12] J. Acharya, Z. Sun, and H. Zhang, “Hadamard response: Estimating distributions privately, efficiently, and with little communication,” in Proc. 22nd Int. Conf. Artif. Intell. Stat., pp. 1120–1129, 2019. [13] V . Feldman, J. Nelson, H. Nguyen, and K. Talwar, “Private frequency estimation via projective geometry,” inProc. 39th Int. Conf. Mach. Learn., pp. 6418–6433, 2022. [14] T. Wang, J. Blocki, N. Li, and S. Jha, “Locally differentially private protocols for frequency estimation,” inProc. 26th USENIX Secur. Symp. (USENIX Secur. 17), pp. 729–745, 2017. [15] T. Murakami and Y . Kawamoto, “Utility-optimized local differential privacy mechanisms for distribution estimation,” inProc. 28th USENIX Secur. Symp. (USENIX Secur. 19), pp. 1877–1894, 2019. [16] N. Mangat and R. Singh, “An alternative randomized response proce- dure,”Biometrika, vol. 77, no. 2, pp. 439–442, 1990. [17] J. Acharya, K. Bonawitz, P. Kairouz, D. Ramage, and Z. Sun, “Context aware local differential privacy,” inProc. 37th Int. Conf. Mach. Learn., pp. 52–62, 2020. [18] Y . Zhang, Y . Zhu, Y . Zhou, and J. Yuan, “Frequency estimation mech- anisms under(ϵ, δ)-utility-optimized local differential privacy,”IEEE Trans. Emerg. Topics Comput., vol. 12, no. 1, pp. 316–327, 2024. [19] Y . Zhang, Y . Zhu, S. Wang, and X. Huang, “Mean estimation of numerical data under(ϵ, δ)-utility-optimized local differential privacy,” IEEE Trans. Inf. Forensics Security, 2024. [20] X. Gu, M. Li, L. Xiong, and Y . Cao, “Providing input-discriminative protection for local differential privacy,” inProc. IEEE 36th Int. Conf. Data Eng. (ICDE), pp. 505–516, 2020.[21] T. Murakami and Y . Sei, “Automatic tuning of privacy budgets in input-discriminative local differential privacy,”IEEE Internet Things J., vol. 10, no. 18, pp. 15990–16005, 2023. [22] B. Wang, C. Yang, and J. Ma, “UKVLDP: Utility-optimized local differential privacy mechanism for key-value IoT data collection,”IEEE Internet Things J.,",
    "505–516, 2020.[21] T. Murakami and Y . Sei, “Automatic tuning of privacy budgets in input-discriminative local differential privacy,”IEEE Internet Things J., vol. 10, no. 18, pp. 15990–16005, 2023. [22] B. Wang, C. Yang, and J. Ma, “UKVLDP: Utility-optimized local differential privacy mechanism for key-value IoT data collection,”IEEE Internet Things J., 2025. [23] L. Bonomi, S. Gousheh, and L. Fan, “Enabling health data sharing with fine-grained privacy,” inProc. 32nd ACM Int. Conf. Inf. Knowl. Manage., pp. 131–141, 2023. [24] S. Islam, S. Badsha, S. Sengupta, I. Khalil, and M. Atiquzzaman, “An intelligent privacy preservation scheme for EV charging infrastructure,” IEEE Trans. Ind. Informat., vol. 19, no. 2, pp. 1238–1247, 2023. [25] X. Yue, M. Du, T. Wang, Y . Li, H. Sun, and S. S. Chow, “Differential privacy for text analytics via natural text sanitization,” inFindings Assoc. Comput. Linguistics: ACL-IJCNLP 2021, pp. 3853–3866, 2021. [26] X. Liu, Q. Liu, J. Wang, and H. Sun, “Multidimensional epidemiological survey data aggregation scheme based on personalized local differential privacy,”Symmetry, vol. 16, no. 3, p. 294, 2024. [27] X. He, Y . Zhu, R. Liu, G. Pan, and C. Dong, “Addressing sensitivity distinction in local differential privacy: A general utility-optimized framework,” inProc. 34th USENIX Secur. Symp. (USENIX Secur. 25), pp. 2753–2769, 2025. [28] P. Kairouz, S. Oh, and P. Viswanath, “Extremal mechanisms for local differential privacy,”J. Mach. Learn. Res., vol. 17, no. 17, pp. 1–51, 2016. [29] H.-Y . Park, S.-H. Nam, and S.-H. Lee, “Exactly optimal and communication-efficient private estimation via block designs,”IEEE J. Select. Areas Inf. Theory, vol. 5, pp. 123–134, 2024. [30] S.-H. Nam, V . Y . F. Tan, and S.-H. Lee, “Optimal private discrete distribution estimation with 1-bit communication,”IEEE Trans. Inf. Forensics Security, vol. 19, pp. 6514–6528, 2024. [31] M. Ye and A. Barg, “Asymptotically optimal private estimation under mean square loss,” 2017.arXiv:1708.00059. [32] Y . Polyanskiy and Y . Wu,Information Theory: From Coding to Learn- ing. Cambridge University Press, 1st ed., Dec. 2024. [33] A. W. van der Vaart,Asymptotic Statistics. Cambridge Series in Statis- tical and Probabilistic Mathematics, Cambridge: Cambridge University Press, 1998. [34] L. Le Cam and G. Lo Yang,Asymptotics in Statistics. Springer Series in Statistics, New York, NY: Springer New York, 2000. [35] Y . J. Ionin and M. S. Shrikhande,Combinatorics of symmetric designs, vol. 5. Cambridge, U.K.: Cambridge University Press, 2006. [36] S. P. Boyd and L. Vandenberghe,Convex optimization. Cambridge, UK ; New York: Cambridge University Press, 2004. [37] D. Blackwell, “Equivalent comparisons of experiments,”Ann. Math. Stat., vol. 24, no. 2, pp. 265–272, 1953. [38] L. P. Barnes, W.-N. Chen, and A. ¨Ozg¨ur, “Fisher information under local differential privacy,”IEEE J. Select. Areas Inf. Theory, vol. 1, pp. 645– 659, Jan. 2020. [39] A. M. Bikchentaev, F. Kittaneh, M. S. Moslehian, and Y . Seo,Trace Inequalities: For Matrices and Hilbert Space Operators. Forum for Interdisciplinary Mathematics, Singapore: Springer Nature Singapore, 2024. [40] W. Rudin,Principles of Mathematical Analysis. International Series in Pure and Applied Mathematics, McGraw-Hill, 1976. [41] S.-H. Nam, H.-Y . Park, S.-H. Lee, and J. Bae, “Quantum advan- tage in locally differentially private hypothesis testing,” Aug. 2025. arXiv:2501.10152.",
    "Hilbert Space Operators. Forum for Interdisciplinary Mathematics, Singapore: Springer Nature Singapore, 2024. [40] W. Rudin,Principles of Mathematical Analysis. International Series in Pure and Applied Mathematics, McGraw-Hill, 1976. [41] S.-H. Nam, H.-Y . Park, S.-H. Lee, and J. Bae, “Quantum advan- tage in locally differentially private hypothesis testing,” Aug. 2025. arXiv:2501.10152. [42] C. Amorino and A. Gloter, “Factorization by extremal privacy mecha- nisms: New insights into efficiency,” July 2025.arXiv:2507.21769. [43] J. Acharya and Z. Sun, “Communication complexity in locally private distribution estimation and heavy hitters,” inProc. 36th Int. Conf. Mach. Learn., pp. 51–60, 2019. [44] U.S. Census Bureau, “American community survey (ACS) 2023 1-year public use microdata sample (PUMS).” [Online]. Available: https://www. census.gov/programs-surveys/acs/microdata.html, Oct. 2024. Accessed: Aug. 18, 2025. [45] L. Steinberger, “Efficiency in local differential privacy,”Ann. Stat., vol. 52, pp. 2139–2166, Oct. 2024. [46] M. Sion, “On general minimax theorems,”Pacific J. Math., vol. 8, pp. 171–176, Jan. 1958. 14 x∈ X S x∈ X N y∈ Y(k) P,x∈y y∈ Y(k) P,x /∈y y∈ Y I y∈ Y(k) Py∈ Y I, x∈y y∈ Y I, x /∈y (ηP(α),Q (y))xveϵ αk(eϵ−1) +vv αk(eϵ−1) +v0v αk(eϵ−1) +vw−v 1−α0 \u0000 Π1\u0000 ηP(α),Q (y)\u0001\u0001 x(v−k)(eϵ−1) αk(eϵ−1) +v−k(eϵ−1) αk(eϵ−1) +v0 0 0 0 \u0000 Π2\u0000 ηP(α),Q (y)\u0001\u0001 x0 0 0 0w−v−1 1−α−1 1−α \u0000 Π3\u0000 ηP(α),Q (y)\u0001\u0001 xk(w−v)(eϵ−1) w(αk(eϵ−1) +v)k(w−v)(eϵ−1) w(αk(eϵ−1) +v)−w−v w(1−α)−vk(eϵ−1) w(αk(eϵ−1) +v)v w(1−α)v w(1−α) TABLE I: Components of Score Vectors and Their Projections for Extremal ULDP Mechanisms.Y(k) Pis defined in Appendix A. APPENDIXA ADDITIONALNOTATIONS For anRn-valued random variableZ, we denote VAR(Z) :=Pn i=1Var(Z i). Suppose thatvis given. For eachk∈[v], we defineY(k) P:= {y⊂[v] :|y|=k}. APPENDIXB CONVEXITY ANDATTAINMENT OF THEOPTIMIZATION PROBLEM In this appendix, we briefly explain the reason that the objective functionM(α, t)has a saddle point and is concave- convex. First, we show thatMis concave-convex. For a fixedα, each ofM i(α, t)is convex int, being a composition of a linear function with a convex functionz7→1/z, z∈[0,∞). Also, for a fixedt, each ofM i(α, t)is proportional to the weighted harmonic mean of concave functions onαoverk with weightst k, where the corresponding concave functions fori= 1,2,3are(αk(eϵ−1)+v)(keϵ+v−k) k(v−k),(1−α)(keϵ+v−k) k, and(1−α)(αk(eϵ−1)+v) k, respectively. It is a standard fact [36] that the harmonic mean is concave; thus, each ofM i(α, t)is concave inα. Thus, we conclude that the objective function M(α, t)is concave-convex in(α, t). Next, we show thatMhas a saddle point. Forv= 1, we havedom(M) = [0,1]×∆ v, and forv≥2, we have dom(M) = [0,1]×(∆ v\\{δ(v;v)}). Also,Mis continuous everywhere in[0,1]×∆ v. Since[0,1]is compact, by Sion’s minimax theorem [46], the saddle point ofMexists. APPENDIXC DETAILEDPROOF FORSECTIONV A. Statement and Proof of Equicontinuity Result in Theorem 7 Proposition 15.LetP, P′∈∆◦ w. Then, for any conditional distributionQfrom[w]to a finite setY, we have JP′,Q⪯\u0012 max x∈[w]Px P′x\u0013 JP,Q.(74)Proof.Letr= max x∈[w]Px P′x. We need to show that: for any h∈⃗∆w, we have JP′,Q(h, h)≤rJ P,Q(h, h).(75) Observe that QP′(y) =X x∈[w]P′ xQ(y|x)(76) ≥X x∈[w](Px/r)Q(y|x) =Q P(y)/r.(77) Thus, we obtain JP′,Q(h, h) = Var Y∼QP′(ηP′,Q;h(Y))(78) =X y∈supp(Q)\u0010P x∈[w]hxQ(y|x)\u00112 QP′(y)(79) ≤rX y∈supp(Q)\u0010P x∈[w]hxQ(y|x)\u00112 QP(y)(80) =rJ P,Q(h, h).(81) B. Derivation of (40) in Theorem 7 First, observe that Px ˜P(θ) x=Px Px+Pw−1 i=1θih(i) x=1 1 +Pw−1 i=1θih(i)",
    "P,Q(h, h).(75) Observe that QP′(y) =X x∈[w]P′ xQ(y|x)(76) ≥X x∈[w](Px/r)Q(y|x) =Q P(y)/r.(77) Thus, we obtain JP′,Q(h, h) = Var Y∼QP′(ηP′,Q;h(Y))(78) =X y∈supp(Q)\u0010P x∈[w]hxQ(y|x)\u00112 QP′(y)(79) ≤rX y∈supp(Q)\u0010P x∈[w]hxQ(y|x)\u00112 QP(y)(80) =rJ P,Q(h, h).(81) B. Derivation of (40) in Theorem 7 First, observe that Px ˜P(θ) x=Px Px+Pw−1 i=1θih(i) x=1 1 +Pw−1 i=1θih(i) x Px.(82) Since∥h(i)∥= 1, we have Pw−1 i=1θih(i) x Px ≤(w−1) max i|θi| Px. Also, note that1 1−z≤1 +zforz∈(0,1). Thus, letting N= max (w−1)2 D4,\u0012(w−1) minxPx\u00134! + 1,(83) C=(w−1) minxPx,(84) we derive that whenevern≥NandΘ∼ϕ n, we have (w−1) max i|Θi| Px≤Cn−1/4<1almost surely, and thus max xPx ˜P(Θ) x≤1 1−Cn−1/4≤1 +Cn−1/4.(85) Combining this with Proposition 15, we derive (40). 15 C. Derivation of (44) in Theorem 7 We first remark thatJ P,Q is continuous inQbut it can be singular, and hencetr −1(JP,Q)is extended real-valued continuous function onQ. To address this issue, we note that there is an order-preserving homeomorphismσ:R∪ {−∞,∞} →[0,1], e.g., a logistic functionσ(x) =1 1+e−x. Then, by Dini’s theorem,σ( Rn(P, Q))↑σ(tr −1(JP,Q)) uniformlyasn→ ∞, and thus lim n→∞inf Q∈Qσ(Rn(P, Q)) = inf Q∈Qσ(tr−1(JP,Q)).(86) By takingσ−1on both sides, we obtain (44). D. Proof of Theorem 8 Let an(v, ϵ)-ULDP mechanism ˜Q:X → P( ˜Y)be given, whose sets of protected and invertible data are ˜YPand˜YI, respectively. Also, letY=Y P⊔ YI,YP= 2[v]\\{∅}, and YI={{x}:x∈[v+ 1 :w]}. We need to findQ∈ QE w,v,ϵ andT:Y → P( ˜Y)such that ˜Q(˜y|x) =X y∈YQ(y|x)T(˜y|y),∀x∈ X,˜y∈ ˜Y.(87) Suppose thatQ∈ QE w,v,ϵ is induced fromγ:Y P→R≥0as in Definition 8. Then, (87) is equivalent to that the following holds for every˜y∈ ˜Y: ˜Q˜y=X y∈Y PT(˜y|y)γ(y)s(y) +X x∈X NT(˜y|{x}) 1−X y′∈YPγ(y′) δ(x),(88) where ˜Q˜y:= ( ˜Q(˜y|1),···,(˜y|w)),δ(x):=δ(x;w), and for eachz⊂ X,s(z)∈ {1, eϵ}wis defined by s(z) x=( eϵ(ifx∈z) 1 (ifx /∈z).(89) Therefore, it suffices to findγandTwhich satisfy (88). By [11, Lemma 12] and the definition of protected data, for each˜y∈ ˜YP,˜Q˜yis in a conic hull of{s(z):z⊂ X}. Moreover, for eachz⊂ X,s(z)is in a conic hull ofS:= {s(y):y∈ Y P} ∪ {δ(x):x∈ X N}, as follows: •Ifz∩ X S̸=∅, then s(z)=s(z∩X S)+ (eϵ−1)X x∈X N∩zδ(x).(90) •Ifz∩ X S=∅, then s(z)=e−ϵs(XS)+ (eϵ−e−ϵ)X x∈X N∩zδ(x) + (1−e−ϵ)X x∈X N\\zδ(x).(91) Thus, for each˜y∈ ˜YP,˜Q˜yis in a conic hull ofS. Moreover, for each˜y∈ ˜YI,˜Q˜yis a constant multiple ofδ(x)for somex∈ X N, which is also in a conic hull ofS. Thus, for each ˜y∈˜Y, we can write ˜Q˜y=X y∈Y Pγ(y,˜y)s(y)+X x∈X Nu(x,˜y)δ(x)(92) for someγ(y,˜y), u(x,˜y)≥0. Using this, we can observe that (88) is achieved by the followingγandT: γ(y) =X ˜y∈˜Yγ(y,˜y),(93) T(˜y|y) =( γ(y,˜y)/γ(y) (ify∈ Y P) u(x,˜y) 1−P y′∈YPγ(y′)(ify∈ Y I, y={x}).(94) Here, ify∈ Y Psatisfiesγ(y) = 0, then we setT(·|y)to be an arbitrary PMF. From (92) andP ˜y∈˜Y˜Q˜y=1 w, it is straightforward to see that aboveγsatisfies (48) to induce a valid extremal (v, ϵ)-ULDP mechanism, and aboveTis a valid conditional distribution satisfyingP ˜y∈˜YT(˜y|y) = 1. This finishes the proof. E. Derivation of (57) in Proposition 9 The block matrix inversion formula [36] implies that for a positive definite matrixAwithA=\u0014 B CT C D\u0015 , where bothB andDare square submatrices,A−1has the following block form A−1=\u0014 (B−CTD−1C)−1 ˜CT ˜CT(D−CB−1CT)−1\u0015 ,(95) where bothB−CTD−1CandD−CB−1CTare positive definite by the Schur complement lemma [36].",
    "proof. E. Derivation of (57) in Proposition 9 The block matrix inversion formula [36] implies that for a positive definite matrixAwithA=\u0014 B CT C D\u0015 , where bothB andDare square submatrices,A−1has the following block form A−1=\u0014 (B−CTD−1C)−1 ˜CT ˜CT(D−CB−1CT)−1\u0015 ,(95) where bothB−CTD−1CandD−CB−1CTare positive definite by the Schur complement lemma [36]. This result, together withA⪯B⇒A−1⪰B−1[39], we obtain tr−1(A)≥tr −1(B) + tr −1(D).(96) (96) also holds whenAis non-invertible positive semidefinite, by the definition thattr −1(A) =∞. Now, (57) follows directly from applying (96) twice. F . Derivation of (58) in Proposition 9 For eachi= 1,2,3, we have tr(B i) =diX j=1JP(α),Q(h(j;i), h(j;i))(97) for an orthonormal basis{h(j;i)}di j=1ofH i. By definition of Fisher information and the standard fact [32] that the score has zero mean, we have tr(B i) =diX j=1VarY∼QP(α)h ⟨ηP(α),Q(Y), h(j;i)⟩i (98) =diX j=1EY∼QP(α)h ⟨ηP(α),Q(Y), h(j;i)⟩2i (99) =EY∼QP(α) Πi\u0000 ηP(α),Q(Y)\u0001 2.(100) 16 Now, suppose thatQis induced fromγ:Y P→R ≥0as in Definition 8. Also, for simplicity, lett:=t(Q). It is straightforward to see that for eachk∈[v], we have tk=X y∈Y(k) Pkeϵ+v−k vγ(y).(101) Also, we have QP(α)(Y(k) P) =X y∈Y(k) Pαk(eϵ−1) +v vγ(y)(102) =αk(eϵ−1) +v keϵ+v−ktk (103) and QP(α)(YI) = 1−vX k=1QP(α)(Y(k) P)(104) =vX k=1(1−α)(eϵ−1)k keϵ+v−ktk.(105) Furthermore, referring Table I, we obtain that •Ify∈ Y(k) P, then Πi\u0000 ηP(α),Q(y)\u0001 2=A i,k, where (A1,k, A2,k, A3,k) =\u0012vk(v−k)(eϵ−1)2 (αk(eϵ−1) +v)2,0,k2(eϵ−1)2v(w−v) w(αk(eϵ−1) +v)2\u0013 .(106) •Ify∈ Y I, then Πi\u0000 ηP(α),Q(y)\u0001 2=A′ i, where (A′ 1, A′ 2, A′ 3) =\u0012 0,(w−v)(w−v−1) (1−α)2,v(w−v) w(1−α)2\u0013 .(107) Thus, we have tr(B i) = vX k=1QP(α)(Y(k) P)Ai,k! +QP(α)(YI)A′ i.(108) The desired result (58) follows from the direct calculation. APPENDIXD DETAILEDPROOF FORSECTIONVI A. Proof of Proposition 10 We show (62) separately for each ofi= 1,2,3. 1)i= 1:For eachx∈ X Sandk∈[v], let Y(k,x) P :={y∈ Y(k) P:x∈y},(109) Y(k,¬x) P :=Y(k) P\\Y(k,x) P.(110) We have (Π1(P−P(α)))x=( Px−1 vP(X S) (ifx∈ X S) 0 (ifx∈ X N).(111) Hence, from Table I, we deduce that showing (62) is equivalent to showing the equality below for allx∈ X S: vX k=1(eϵ−1)\u0010 (v−k)Q P(Y(k,x) P)−kQ P(Y(k,¬x) P )\u0011 αk(eϵ−1) +v =\u0012 Px−1 vP(X S)\u0013v−1 M1(α, t).(112)Using symmetries of block designs and Lemma 1, we can see that for everyx, x′∈ XSandx′′∈ XNsuch thatx̸=x′, we have Q(Y(k,x) P|x) =t krkeϵ rk(eϵ−1) +b k=tkkeϵ keϵ+v−k,(113) Q(Y(k,x) P|x′) =t kλkeϵ+rk−λk rk(eϵ−1) +b k(114) =tkk((k−1)eϵ+v−k) (v−1)(keϵ+v−k),(115) Q(Y(k,x) P|x′′) =t krk rk(eϵ−1) +b k=tkk keϵ+v−k,(116) and Q(Y(k,¬x) P|x) =t k−Q(Y(k,x) P|x),(117) Q(Y(k,¬x) P|x′) =t k−Q(Y(k,x) P|x′),(118) Q(Y(k,¬x) P|x′′) =t kbk−rk rk(eϵ−1) +b k(119) =tkv−k keϵ+v−k.(120) By direct calculation, we can see that for everyx∈ X Sand x′∈ X, (112) holds forP=δ(x′;v). Since both sides of (112) are linear inP, we conclude that (112) holds for everyx∈ X S andP∈ P(X). 2)i= 2:We have (Π2(P−P(α)))x=( 0 (ifx∈ X S) Px−P(X N) w−v(ifx∈ X N).(121) Similarly to thei= 1case, we deduce that showing (62) is equivalent to showing the equality below for allx∈ X N: (w−v−1)Q P({x})−Q P(YI\\{{x}}) 1−α =\u0012 Px−1 w−vP(X N)\u0013w−v−1 M2(α, t).(122) We can see that for eachx∈ X N, QP({x}) =P x 1−vX k=1tkbk rk(eϵ−1) +b k! (123) =PxvX k=1tkk(eϵ−1) keϵ+v−k(124) =Px·(1−α)(w−v−1) (w−v)M 2(α, t).(125) This also implies QP(YI\\{{x}}) = (P(X N)−P x)·(1−α)(w−v−1)",
    "showing the equality below for allx∈ X N: (w−v−1)Q P({x})−Q P(YI\\{{x}}) 1−α =\u0012 Px−1 w−vP(X N)\u0013w−v−1 M2(α, t).(122) We can see that for eachx∈ X N, QP({x}) =P x 1−vX k=1tkbk rk(eϵ−1) +b k! (123) =PxvX k=1tkk(eϵ−1) keϵ+v−k(124) =Px·(1−α)(w−v−1) (w−v)M 2(α, t).(125) This also implies QP(YI\\{{x}}) = (P(X N)−P x)·(1−α)(w−v−1) (w−v)M 2(α, t).(126) Thus, (122) clearly holds for everyx∈ X N. 3)i= 3:We have (Π3(P−P(α)))x=(P(X S)−α v(ifx∈ X S) −P(X S)−α w−v(ifx∈ X N).(127) 17 Similarly, we deduce that showing (62) is equivalent to show- ing the equality below: vX k=1k(eϵ−1)Q P(Y(k) P) w(αk(eϵ−1) +v)! −QP(YI) w(1−α) =P(X S)−α v(w−v)M 3(α, t).(128) From (124), we have QP(YI) = (1−P(X S))vX k=1tkk(eϵ−1) keϵ+v−k.(129) Also, we have QP(Y(k) P) =t k\u0012 P(X S) +bk rk(eϵ−1) +b kP(X N)\u0013 (130) =tk·k(eϵ−1)P(X S) +v keϵ+v−k.(131) Then (128) follows from the direct calculation. B. Derivation of (66) in Proposition 11 By continuity, it suffices to show (66) forα∈(0,1). Hence, we assumeα∈(0,1)from now on. LetX∼PandY∼Q(·|X). Since ˆPis unbiased, we have R1(Q,ˆP;P) = VAR( ˆP1(Y))(132) = VAR( ˆP1(Y)−P(α))(133) =E∥ ˆP1(Y)−P(α)∥2− ∥P−P(α)∥2.(134) By the definition of the proposed estimator and the law of total expectation, we have E∥ˆP1(Y)−P(α)∥2(135) =3X i=1(Mi(α, t))2 d2 iE∥Π i(ηP(α),Q(Y))∥2(136) =3X i=1X x∈X(Mi(α, t))2 d2 iPxE[∥Π i(ηP(α),Q(Y))∥2|X=x]. (137) As we presented in (106) and (107),∥Π i(ηP(α),Q(y))∥2de- pends onyonly by (i) whethery∈ Y PorY I, and (ii) the value of|y|. By the regularity of block designs, the conditional distributions of∥Π i(ηP(α),Q(Y))∥2givenX=x are the same for allx∈ X S. Also, by the definition of extremal ULDP mechanisms, the conditional distributions of ∥Πi(ηP(α),Q(Y))∥2givenX=xare the same for allx∈ X N. Thus, we can write E∥ˆP1(Y)−P(α)∥2=ν1P(X S) +ν 2P(X N)(138) =ν1β+ν 2(1−β),(139) whereν 1, ν2≥0are given by ν1=3X i=1(Mi(α, t))2 d2 iE[∥Π i(ηP(α),Q(Y))∥2|X=x],(140) ν2=3X i=1(Mi(α, t))2 d2 iE[∥Π i(ηP(α),Q(Y))∥2|X=x′],(141)for any ofx∈ X Sandx′∈ XN. Thus,E∥ ˆP1(Y)−P(α)∥2is a constant among allP∈ P(X)such thatP(X S) =β. Moreover, we have ∥P−P(α)∥2(142) =X x∈X S\u0010 Px−α v\u00112 +X x∈X N\u0012 Px−1−α w−v\u00132 (143) =α2 v2+(1−α)2 (w−v)2−2α vβ−2(1−α) w−v(1−β) +X x∈X SP2 x+X x∈X NP2 x.(144) By the Cauchy-Schwarz inequality, we have X x∈X SP2 x≥β2 v2,X x∈X NP2 x≥(1−β)2 (w−v)2,(145) and the equality holds ifP=P(β). From this, we conclude that among allP∈ P(X)such thatP(X S) =β,R 1(Q,ˆP;P) is maximized atP=P(β), which shows (66). C. Derivation of (68) in Proposition 11 Note that except for the case ofv≥2andt=δ(v;v), M(α, t)is continuously differentiable with respect toαin a neighborhood of[0,1]. Thus, again by continuity, it suffices to show (68) forα∈(0,1). Hence, we assumeα∈(0,1)from now on. From (134), we have R1(Q,ˆP;P(β))(146) =EY∼QP(β)∥ˆP1(Y)−P(α)∥2− ∥P(β)−P(α)∥2.(147) It is straightforward to see that ∥P(β)−P(α)∥2=w v(w−v)(β−α)2.(148) Hence, it remains to show that EY∼QP(β)∥ˆP1(Y)−P(α)∥2 = (β−α)F(α, t) +M(α, t).(149) Since both sides of the above equation are affine inβ, it suffices to show that EY∼QP(α)∥ˆP1(Y)−P(α)∥2=M(α, t),(150) ∂ ∂βEY∼QP(β)∥ˆP1(Y)−P(α)∥2=F(α, t).(151) a) Proof of(150):It is easy to see thatt(Q)in (58) is equal tot. Hence, by (136), (100), and (58), we have EY∼QP(α)∥ˆP1(Y)−P(α)∥2(152) =3X i=1(Mi(α, t))2 d2 iEY∼QP(α)∥Πi(ηP(α),Q(Y))∥2(153) =3X i=1Mi(α, t) =M(α, t),(154) which shows (150). 18 b) Proof of(151):EachQP(β)(y)is affine inβ, and hence we can write QP(β)(y) =f 1(y)β+f 2(y)(155) forf 1, f2: supp(Q)→R. Then, ∂ ∂βEY∼QP(β)∥ˆP1(Y)−P(α)∥2",
    "see thatt(Q)in (58) is equal tot. Hence, by (136), (100), and (58), we have EY∼QP(α)∥ˆP1(Y)−P(α)∥2(152) =3X i=1(Mi(α, t))2 d2 iEY∼QP(α)∥Πi(ηP(α),Q(Y))∥2(153) =3X i=1Mi(α, t) =M(α, t),(154) which shows (150). 18 b) Proof of(151):EachQP(β)(y)is affine inβ, and hence we can write QP(β)(y) =f 1(y)β+f 2(y)(155) forf 1, f2: supp(Q)→R. Then, ∂ ∂βEY∼QP(β)∥ˆP1(Y)−P(α)∥2 =X yf1(y)∥ˆP1(y)− ˆP(α)∥2.(156) Also, from (150), we have F(α, t) =X y∂ ∂α\u0010 QP(α)(y)∥ˆP1(y)−P(α)∥2\u0011 (157) =X yf1(y)∥ˆP1(y)−P(α)∥2 +X yQP(α)(y) ˆP1(y)−P(α),∂P(α) ∂α .(158) Since ˆPis unbiased,P yQP(α)(y)ˆP1(y) =P(α). Thus, combining with (156), we obtain (151). This ends the derivation of (68). APPENDIXE PROOF OFTHEOREM4 Let(α∗, t∗)be as specified in Theorem 4, witht∗=δ(k∗;v). Here,k∗= 1for Case (a), andk∗∈K∗(v, ϵ)∩[2 :v−1]for Case (b). We separately show two equalities, M(α∗, t∗) = inf t∈∆vM(α∗, t),(159) M(α∗, t∗) = sup α∈[0,1]M(α, t∗).(160) A. Proof of (159) Case (a)-Condition (i):This case is trivial, since when v= 1,∆ vis a singleton. Case (a)-Condition (ii):In this case, we have eϵ≥w−v+r (w−1)(w−2) 2≥1 +w−v,(161) and thusα∗=v(eϵ−1−w+v) w(eϵ−1). First, observe that by a calculation, we have Mi(α∗, t∗) =d i(eϵ+v−1)2 w(eϵ−1)2,∀i= 1,2,3.(162) Recall that(d 1, d2, d3) = (v−1, w−v−1,1). Motivated from this, we apply the arithmetic mean-harmonic mean inequality, obtaining the following. M(α∗, t)(163) =(d1+d2+d3)3X i=1di d1+d2+d3(Mi(α∗, t)/d i)(164) ≥(d1+d2+d3)2 P3 i=1d2 i/Mi(α∗, t).(165)This becomes an equality whent=t∗=δ(1;v). Also, we can see thatP3 i=1d2 i/Mi(α∗, t) =Pv k=1gktk, whereg k≥0are given by gk=kv(v−k)(eϵ−1)2 (α∗k(eϵ−1) +v)(keϵ+v−k) +k(w−v)(w−v−1)(eϵ−1) (1−α∗)(keϵ+v−k) +vk(w−v)(eϵ−1) w(1−α∗)(α∗k(eϵ−1) +v).(166) Now, we claim that g1≥gk,∀k∈[2 :v],(167) because this claim deduces (159) as inf t∈∆vM(α∗, t) =M(α∗, t∗) =(d1+d2+d3)2 g1.(168) To show (167), we calculateg 1−gk, and the result is g1−gk=(169) vk(k−1)w(eϵ−1)2 ζ2(keϵ+v−k)(k(eϵ−1−w+v) +w) ×\u0012 ζ2−2(w−1)ζ+w(w−1)k−1 k\u0013 ,(170) whereζ:=eϵ+v−1. Note thatk−1 kincreases ink. Hence, to showg 1−gk≥0for allk∈[2 :v], it suffices to show that ζ2−2(w−1)ζ+w(w−1) 2≥0.(171) The LHS of the above equation is a quadratic polynomial in ζwith rootsw−1±q (w−1)(w−2) 2. Hence, (171) holds for ζ≥w−1+q (w−1)(w−2) 2, that is,eϵ≥w−v+q (w−1)(w−2) 2. This shows (167), and hence ends the proof. Case (a)-Condition (iii):In this case, we have eϵ≤1 +r 2(w−2) w−1≤1 +w−v=w−1,(172) and thusα∗= 0. By a direct calculation, witht 2= 1−t 1, we obtain M(α∗, t) =f(t 1)(173) :=eϵ+ 1 (eϵ−1)21 t1+eϵ(eϵ+ 1) (w−2)(eϵ−1)w−3 eϵ+ 1−t 1 +w (w−2)(eϵ−1)1 2−t 1.(174) Sincef(t 1)is convex int 1, to show the optimality oft∗= δ(1;v), it suffices to show thatf′(1)≤0. A straightforward calculation shows that f′(1) =w−1 eϵ(eϵ−1)2(w−2)\u0012 e2ϵ−2eϵ−w−3 w−1\u0013 .(175) Here,e2ϵ−2eϵ−w−3 w−1is a quadratic polynomial ineϵwith roots1±q 2(w−2) w−1. Thus,f′(1)≤0foreϵ≤1 +q 2(w−2) w−1, which ends the proof. 19 Case (b):SinceM 2(1, t) =M 3(1, t) = 0, we have inf t∈∆vM(α∗, t) = inf t∈∆vM1(1, t)(176) = inf t∈∆v(v−1)2 v(eϵ−1)21 Pv k=1tkk(v−k) (keϵ+v−k)2 (177) = min k∈[v−1](v−1)2 v(eϵ−1)2(keϵ+v−k)2 k(v−k)(178) = min k∈[v−1]RBD(v, k, ϵ),(179) whereRBDis in (18), and optimalkin (179) and optimalt in (176) are related byt=δ(k;v). Thus, by the definition of K∗,t∗=δ(k∗;v)is an optimal solution to (176). B. Proof of (160) Let(Q, ˆP)be a(w, v, ϵ, k∗)-simple uBD scheme. By Propo- sition 13, Equation (68) witht=t∗=δ(k∗;v)holds for all α∈[0,1]. Settingα=βin (68) gives R1(Q,ˆP;P(β)) =M(β, δ(k∗;v)),∀β∈[0,1].(180) Thus, it suffices to show thatsupβ∈[0,1] R1(Q,ˆP;P(β))is attained atβ=α∗. For Case (a), it directly follows from the",
    "an optimal solution to (176). B. Proof of (160) Let(Q, ˆP)be a(w, v, ϵ, k∗)-simple uBD scheme. By Propo- sition 13, Equation (68) witht=t∗=δ(k∗;v)holds for all α∈[0,1]. Settingα=βin (68) gives R1(Q,ˆP;P(β)) =M(β, δ(k∗;v)),∀β∈[0,1].(180) Thus, it suffices to show thatsupβ∈[0,1] R1(Q,ˆP;P(β))is attained atβ=α∗. For Case (a), it directly follows from the worst-case analysis of uRR [15, Propositions 16, 17]. For Case (b), we claim that F(1, δ(k;v))≥0,∀k∈[2 :v−1].(181) Then, by settingα= 1in (68), we obtain the desired result. To show (181), we calculateF(1, δ(k;v)), and the result is F(1, δ(k;v)) =keϵ+v−k v(eϵ−1)\u0012(v−1)2 v−k−v+ 1 k\u0013 .(182) Hence, it suffices to show that (v−1)2k−(v+ 1)(v−k)≥0,∀k∈[2 :v−1].(183) Since the LHS of the above inequality is increasing ink, it suffices to show only fork= 2, that is, 2(v−1)2−(v+ 1)(v−2)≥0.(184) We have 2(v−1)2−(v+ 1)(v−2) =v2−3v+ 4.(185) As a quadratic polynomial inv, above equation has the discriminant32−4·4<0, and therefore it is non-negative for everyv. Thus, we show (181), and hence we end the proof of (160). APPENDIXF PROOF OFCOROLLARY6 Referring [27, Equation (35)], the estimation error of uSS is n·R n(QuSS,k,ˆPuSS,k;P)(186) =L1(k) +P(X S)L2(k) +P(X N)L3(k) + 1−X x∈XP2 x,(187)where L1(k) =v(keϵ−eϵ+v−k)(keϵ−k+v−1) k(v−k)(eϵ−1)2,(188) L2(k) =k(1−k)(eϵ−1) + (v−1)(v−2k) k(v−k)(eϵ−1),(189) L3(k) =v k(eϵ−1).(190) Especially, lettingPbe the uniform distribution onX S, we have n·R n(QuSS,k,ˆPuSS,k)(191) ≥n·R n(QuSS,k,ˆPuSS,k;P)(192) =L1(k) +L 2(k) + 1−1 v,(193) and hence R(Q uSS,k,ˆPuSS,k)≥L 1(k) +L 2(k) + 1−1 v.(194) Recall from Remark 3 thatM∗(w, v, ϵ) =M∗(v, v, ϵ) = mink∈[v−1] RBD(v, k, ϵ). By a straightforward calculation, we obtain\u0012 L1(k) +L 2(k) + 1−1 v\u0013 −RBD(v, k, ϵ)(195) =2(k−1) v−k.(196) Hence, we have R(Q uSS,k,ˆPuSS,k)≥RBD(v, k, ϵ) +2(k−1) v−k.(197) Using this, we argue thatR(Q uSS,k,ˆPuSS,k)> M∗(w, v, ϵ) for everyk∈[v−1], as follows. 1) Ifk= 1, then sinceϵ <lnq (v−1)(v−2) 2=E(v,1), we have1/∈K∗(v, ϵ), and henceRBD(v, k, ϵ)> M∗(w, v, ϵ). Thus,R(Q uSS,k,ˆPuSS,k)> M∗(w, v, ϵ). 2) Ifk≥2, thenRBD(v, k, ϵ)≥M∗(w, v, ϵ)and2(k−1) v−k> 0. Thus,R(Q uSS,k,ˆPuSS,k)> M∗(w, v, ϵ). This ends the proof of Corollary 6. Remark 8.Originally, [27] formulated the estimator error in terms of the MSE between the empirical frequency ofXnand the estimated distribution. That is, lettingT(n): [w]n→∆ w, T(n) x′(xn) =1 nPn i=11(xi=x′), they formulated R′ n(Q,ˆP;xn) := EYn∼Qn i=1Q(·|x i)∥ˆPn(Yn)−T(n)(xn)∥2.(198) However, provided that the scheme is unbiased, the MSE for the frequency estimation can be translated into the MSE for the distribution estimation, by using the law of total variance. Specifically, for givenP∈∆ wand anunbiasedscheme (Q,ˆP), letXn∼PnandYn∼Qn i=1Q(·|X i)givenXn. Then Rn(Q,ˆP;P) = VARh ˆPn(Yn)i (199) =E[VAR[ ˆPn(Yn)|Xn]] + VAR[E[ ˆPn(Yn)|Xn]](200) =E[R′ n(Q,ˆP;Xn)] + VAR[T(n)(Xn)].(201) 20 Here, each ofT(n) x(Xn)is a binomial random variable with ntrials and probability of successP x, divided byn. Hence VAR[T(n)(Xn)](202) =1 nX x∈X(Px−P2 x) =1 n 1−X x∈XP2 x! .(203) In conclusion, we have Rn(Q,ˆP;P)(204) =EXn∼Pn[R′ n(Q,ˆP;Xn)] +1 n 1−X x∈XP2 x! .(205) From this, we deduce(187)as follows. In [27, Equation (35)], it is provided that n·R′ n(QuSS,k,ˆPuSS,k;xn)(206) =L1(k) +X x′∈XST(n) x′(xn)L2(k) +X x′∈XNT(n) x′(xn)L3(k).(207) UnderXn∼Pn, it is straightforward thatE[T(n) x′(Xn)] = Px′for everyx′∈ X. Hence n·E Xn∼Pn[R′ n(QuSS,k,ˆPuSS,k;Xn)](208) =L1(k) +P(X S)L2(k) +P(X N)L3(k).(209) Applying(205), we obtain(187).",
    "In [27, Equation (35)], it is provided that n·R′ n(QuSS,k,ˆPuSS,k;xn)(206) =L1(k) +X x′∈XST(n) x′(xn)L2(k) +X x′∈XNT(n) x′(xn)L3(k).(207) UnderXn∼Pn, it is straightforward thatE[T(n) x′(Xn)] = Px′for everyx′∈ X. Hence n·E Xn∼Pn[R′ n(QuSS,k,ˆPuSS,k;Xn)](208) =L1(k) +P(X S)L2(k) +P(X N)L3(k).(209) Applying(205), we obtain(187)."
  ]
}