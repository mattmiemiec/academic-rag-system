{
  "filename": "2509.23672v1.pdf",
  "total_chunks": 18,
  "text_length": 57762,
  "chunks": [
    "SUBMISSION OF IEEE TRANSACTIONS ON MEDICAL IMAGING 1 Token Merging via Spatiotemporal Information Mining for Surgical Video Understanding Xixi Jiang, Chen Y ang, Dong Zhang, Pingcheng Dong, Xin Y ang, Kwang-Ting Cheng,Fellow, IEEE Abstractâ€” Vision Transformer models have shown im- pressive effectiveness in the surgical video understand- ing tasks through long-range dependency modeling. How- ever, current methods suffer from prohibitive computa- tional costs due to processing massive spatiotemporal tokens across video frames. While prior work on token merging has advanced model efficiency, they fail to ad- equately consider the inherent spatiotemporal structure of video data and overlook the heterogeneous nature of information distribution, leading to suboptimal perfor- mance. In this paper, we propose a spatiotemporal in- formation mining token merging (STIM-TM) method, rep- resenting the first dedicated approach for surgical video understanding. STIM-TM introduces a decoupled strategy that reduces token redundancy along temporal and spa- tial dimensions independently. Specifically, the temporal component merges spatially corresponding tokens from consecutive frames using saliency weighting, preserving critical sequential information and maintaining continu- ity. Meanwhile, the spatial component prioritizes merging static tokens through temporal stability analysis, protecting dynamic regions containing essential surgical information. Operating in a training-free manner, STIM-TM achieves sig- nificant efficiency gains with over65%GFLOPs reduction while preserving competitive accuracy across comprehen- sive surgical video tasks. Our method also supports effi- cient training of long-sequence surgical videos, addressing computational bottlenecks in surgical applications. Code is available at https://github.com/xjiangmed/STIM-TM. Index Termsâ€” Token merging, Surgical video under- standing, Efficient transformer I. INTRODUCTION Surgical video understanding aims to extract meaningful insights about instrument interactions, anatomical structures, and procedural dynamics [1]. This task plays a pivotal role in advancing surgical practice, encompassing tasks ranging from coarse-level workflow recognition (e.g., phase recognition [2], This work was supported in part by the Hong Kong SAR RGC General Research Fund under Grant 16208823, the National Key R&D Program of China (2024YFE0217700), National Natural Science Foundation of China (62472184), and the Fundamental Research Funds for the Cen- tral Universities. X. Jiang, C. Y ang, D. Zhang, P . Dong, and K.-T. Cheng are with the Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong, China. E- mail: xjiangbh@connect.ust.hk, eechengyang@ust.hk, dongz@ust.hk, pingcheng.dong@connect.ust.hk, timcheng@ust.hk X. Y ang is with the School of Electronic Information and Communica- tions, Huazhong University of Science and Technology, Wuhan, China. E-mail: xinyang2014@hust.edu.cn. Corresponding author: Kwang-Ting Cheng. (a) Long-Range Context Dependency ïƒ¼Phase: Suturing ïPhase: Preparation (b) Sequential Action Composition (c) Existing Methods vs. Our STIM-TM Joint Merging Our Temporal MergingOur Spatial Merging SSSS SDDS SSDS SSDD SSSSSSSS SDSS SDDS DSSS DSSSSSSS SDDS SDDS SDSS SSSSSSSS SDDS SSDS SSDD SSSSSSSS SDDS SDDS SDSS SSSSSSSS SDSS SDDS DSSS DSSS S: static token D: dynamic token M: merged token object node edge merging DD D DDM4M1 M2 M5M7 M3M6DD DD DM1M7 M2 M3M4M5M6DD DD DM1 M2 M3 M4M5M6M7 D DD D DM1 M2 M3 M4M5M6M7D D D D DM1 M2 M3 M4M5M6M7 DD D DDM1M2 M3 M4 M5 M6M7InvisibleFig. 1. Characteristics of surgical videos and illustration of STIM-TM. (a) Long-term temporal context is crucial as short clips",
    "M5M7 M3M6DD DD DM1M7 M2 M3M4M5M6DD DD DM1 M2 M3 M4M5M6M7 D DD D DM1 M2 M3 M4M5M6M7D D D D DM1 M2 M3 M4M5M6M7 DD D DDM1M2 M3 M4 M5 M6M7InvisibleFig. 1. Characteristics of surgical videos and illustration of STIM-TM. (a) Long-term temporal context is crucial as short clips struggle to dis- tinguish similar frames across phases, causing misclassifications such as confusing â€œsuturingâ€ with â€œpreparationâ€. (b) Surgical videos exhibit multi-scale sequential actions with strong interdependencies between actions. (c) Existing methods (e.g., joint merging) may merge similar objects across distant frames, disrupting action continuity (the left in (c) shows object-centric graph representation with cross-frame merging). Our STIM-TM decouples spatiotemporal merging: temporal merging operates between adjacent frames to preserve action sequences, while spatial merging (the right in (c)) retains more tokens in dynamic regions. [3], step recognition [4]) to fine-grained scene understanding (e.g., instrument segmentation [5], action triplet detection [6]). Thanks to the self-attention mechanismâ€™s ability to capture long-range dependencies, Vision Transformers (ViTs) [7] have been increasingly adopted for surgical video understanding, demonstrating remarkable effectiveness [3], [6]. However, ViT- based methods suffer from a severe computational bottleneck as self-attention complexity scales quadratically with the num- ber of tokens [8]. This challenge is particularly severe for video data, and becomes even more pronounced for high- resolution and long-duration surgical videos [2], [9]. This bottleneck impedes the real-time deployment in resource- limited clinical environments. To address quadratic complexity within ViT models, reduc- ing computational burden of attention mechanisms has become a primary solution [2], [3], [10]. Prior research has developed strategies such as decoupling self-attention across temporal and spatial dimensions [11], or restricting attention to local spatiotemporal windows [10]. Surgical-specific optimizations include hierarchical two-stage temporal learning [2], [12], sparse sampling [3], and Mamba-based architectures [5], [6]. While effective, these approaches typically require architec- tural modifications and training from scratch. In contrast, token merging [8], [13], [14] methods offer a flexible, training- free alternative which reduce sequence length by mergingarXiv:2509.23672v1 [cs.CV] 28 Sep 2025 2 SUBMISSION OF IEEE TRANSACTIONS ON MEDICAL IMAGING redundant tokens, decreasing computational overhead. The representative work ToMe [8] proposes Bipartite Soft Match- ing (BSM) for efficiently merging highly similar tokens. Sub- sequent studies [13]â€“[17] have built upon BSM with various improvements across image and video domains. However, existing video token merging methods apply uniform merging strategies across all tokens and dimensions, without considering dimensional context or semantic impor- tance. These uniform strategies lead to two critical limitations. First, they ignore the inherent spatiotemporal structure of video data, risking disruption of critical semantic relation- ships. Second, they fail to account for varying information density across different visual regions, potentially degrading the discriminative information of merged tokens. To achieve efficient token merging, two fundamental challenges should be addressed. The first one is:How should tokens be merged to preserve the distinct characteristics of spatial and tempo- ral dimensions?Existing unidimensional methods eliminate either spatial redundancy [15], [16] within individual frames or temporal redundancy [17] across sequences, resulting in asymmetric compression and information imbalance across dimensions. Joint merging methods [8], [18]â€“[20] simulta- neously consider both dimensions but",
    "be merged to preserve the distinct characteristics of spatial and tempo- ral dimensions?Existing unidimensional methods eliminate either spatial redundancy [15], [16] within individual frames or temporal redundancy [17] across sequences, resulting in asymmetric compression and information imbalance across dimensions. Joint merging methods [8], [18]â€“[20] simulta- neously consider both dimensions but uniformly process all tokens without dimensional distinction. However, as shown in Fig. 1 (a-b), to extract discriminative features from surgical videos, different dimensions have distinct requirements: the temporal dimension requires preserving long-term context and action sequence integrity, while the spatial dimension requires retaining local structure to preserve distinctive action characteristics. Unfortunately, as illustrated in Fig. 1 (c) (left), joint merging may merge similar objects across distant frames, disrupting temporal sequence coherence while compromising spatial feature preservation. Since temporal adjacency and spatial locality carry different semantic meanings in surgical videos, this necessitates a decoupled framework that leverages dimension-specific characteristics. The second one is:Where should tokens be merged to max- imize redundancy reduction while preserving discriminative information?We argue that merging decisions should consider information density and leverage the spatiotemporal structure of video. Our analysis in Sec. III-B reveals two video-specific redundancy patterns: (1)Temporal Redundancy Pattern: Spatially corresponding tokens between adjacent frames ex- hibit significantly higher similarities than token pairs between non-adjacent frames or at different spatial positions. This reflects procedural continuity in surgical videos, indicating that temporal merging should be constrained to spatially aligned tokens from adjacent frames to reduce redundancy effec- tively. (2)Spatial Redundancy Pattern: Within individual frames, information distribution is highly non-uniform.Static regions (e.g., undisturbed peripheral tissues) exhibit consistent features, while dynamic regions (e.g., moving instruments, deforming tissues) show significant variation. This suggests that spatial merging should target low-entropy static regions while preserving informative dynamic elements. Based on these observations, we propose SpatioTemporal Information Mining Token Merging (STIM-TM), a novel training-free module for video transformers. From an infor-mation bottleneck perspective, STIM-TM strategically selects merging candidates to maximize task-relevant information retention while minimizing input redundancy. Specifically, as illustrated in Fig. 1 (c), STIM-TM decouples the merging process into independent temporal and spatial dimensions, preserving both temporal continuity and spatial structure. For temporal merging, we restrict merging candidates to spatially corresponding positions across adjacent frames, and iteratively merge the most similar token pairs. Temporal attention scores serve as saliency measures to weight the merge operation, ensuring more informative tokens contribute more to the merged result. For spatial merging, we employ segment- based consistency scores to prioritize merging static regions while protecting dynamic objects. The main contributions are summarized as follows: â€¢We introduce STIM-TM, the first dedicated token merg- ing framework for surgical video transformers, leveraging the unique spatiotemporal redundancy patterns. â€¢We demonstrate the broad applicability of STIM-TM by achieving significant computational reduction with minimal accuracy degradation across diverse tasks. â€¢STIM-TM is a plug-and-play module that can be seam- lessly integrated into existing video transformer architec- tures during inference, or used during training to enable long-sequence modeling under memory constraints. II. RELATEDWORK A. Surgical Video Understanding with Vision Transformer Vision Transformers (ViTs) [7] have recently outperformed traditional CNNs in surgical video understanding by ef- fectively",
    "that can be seam- lessly integrated into existing video transformer architec- tures during inference, or used during training to enable long-sequence modeling under memory constraints. II. RELATEDWORK A. Surgical Video Understanding with Vision Transformer Vision Transformers (ViTs) [7] have recently outperformed traditional CNNs in surgical video understanding by ef- fectively modeling complex spatiotemporal dependencies. LoViT [2] and SKiT [12] employ two-stage, fully transformer- based architecture with spatial feature extraction followed by temporal transformers for long-range dependency modeling in phase recognition. Surgformer [3] improves phase recognition using hierarchical temporal attention to capture local and global dynamics. TAPIS [4] extends transformer architectures to broader workflow analysis, integrating a video-wise encoder with a region proposal network for multiple tasks. With the advent of large-scale surgical video datasets, transformer- based video foundation models [9], [21], [22] have emerged as a transformative paradigm for surgical video understanding, exhibiting exceptional robustness and cross-domain gener- alization capabilities. Nevertheless, the high computational cost of transformer-based approaches remains a significant challenge. To address this, our STIM-TM merges redundant tokens across spatial and temporal dimensions to decrease the token length of transformer blocks, significantly reducing computational cost. B. Token Reduction Token reduction methods decrease computational overhead through token pruning and merging.Token pruningmeth- ods [23]â€“[25] dynamically remove uninformative tokens based on attention scores or learnable modules. Recent works such as STTS [24] and STA [25] extend pruning to video tasks. However, most pruning methods require additional training, AUTHORet al.: SUBMISSION OF IEEE TRANSACTIONS ON MEDICAL IMAGING 3 Temporal AttentionSpatial Attention Temporal Merging Spatial MergingMLP Patch Embedding [CLS] +Transformer Block àµˆLHead ð‘¥à¯œ ð‘¥à¯œà¯ à¯˜à¯¥à¯šà¯˜à¯—0.62 0.48 0.90 0.73 0.88kà¯œà¯§ simà¯œà¯§ Merging Candidate Time MHSA Saliency-Weighted Merging Hierarchical Segment Partition Segment 1 Segment 2 Static tokens Dynamic tokens Partition Merge Match Static-Prioritized MergingSpatial Token Merging Temporal Token MergingEncoder Layer ð¿à¯  Encoder Layer ð¿à¬µ Encoder Layer ð¿à­¬Temporal Redundancy Token SaliencySaliency Map â€¦ â€¦ Negative Entropy Attention Map ð‘‡àµˆð‘‡ Video tokensâ€¦ â€¦ tà¬µ ð‘¡à¬¶ ð‘¡à¬· ð‘¡à¬¸ ð‘¡à¬µ ð‘¡à¬¶ ð‘¡à¬· ð‘¡à¬¸ Fig. 2. (Left) Baseline architecture and the insertion position of STIM-TM. STIM-TM is a plug-and-play module that reduces temporal and spatial redundancy independently. (Upper right) Temporal merging: We calculate temporal redundancy for each spatial position based on adjacent-frame similarity. The most similar adjacent frames are identified as candidates and aggregated using saliency weights from attention maps. (Lower right) Spatial merging: Video features are hierarchically partitioned into temporal segments across encoder layers. Within each segment, spatial positions are assigned static scores based on cross-frame similarity. High static-score tokens are prioritized and merged via bipartite soft matching. and aggressive pruning may discard critical features.Token merginghas emerged as an effective, often training-free method for reducing token redundancy. Among these methods, ToMe [8] pioneers this direction by merging semantically simi- lar tokens via Bipartite Soft Matching. Subsequent works [13], [14] have explored various merging strategies, such as local- then-global merging [13] and multi-criteria merging [14]. While these methods are effective, most focus on spatial redundancy in images. Recently, token merging has been applied to video tasks [15]â€“[18], [26], [27] to address temporal redundancy in long video understanding and diffusion models. VidToMe [27] improves",
    "merging strategies, such as local- then-global merging [13] and multi-criteria merging [14]. While these methods are effective, most focus on spatial redundancy in images. Recently, token merging has been applied to video tasks [15]â€“[18], [26], [27] to address temporal redundancy in long video understanding and diffusion models. VidToMe [27] improves temporal consistency in video gener- ation by matching and merging tokens across frames within the self-attention module. VTM [19] introduces a learnable merging method that dynamically merges tokens based on their saliency. vid-TLDR [18] uses attention score sharpness for saliency-aware merging. TESTA [26] performs token merging along temporal and spatial dimensions, aggregating similar frames and tokens. TempMe [20] progressively merges tokens across neighboring clips. STPM [28] integrates pruning and merging with semantic importance and attention-based strate- gies. Despite these advances, most methods inadequately han- dle spatiotemporal characteristics and overlook video-specific information patterns. STIM-TM overcomes these limitations by identifying low-information tokens across temporal and spatial dimensions, enabling targeted merging that preserves key structural information.III. METHODOLOGY A. Preliminaries Video Transformers.Video transformers extend the standard transformer architecture [7] to spatiotemporal data. The core self-attention mechanism remains: Attention(Q, K, V) =softmax\u0012QKâŠ¤ âˆš C\u0013 V,(1) whereQ, K, VâˆˆRNÃ—Care projections of inputX. A T-frame clip (HÃ—Wresolution) is tokenized intoN= T tÃ—H PÃ—W Pspatiotemporal tokens. All tokens interact through either: (1) Joint space-time attention [7] with full cross-token interactions. (2) Divided space-time attention [11] that se- quentially computes in two dimensions: Attention(Q t, Kt, Vt) along temporal dimension and Attention(Q s, Ks, Vs)within each frame. The divided approach reduces complexity from O(N2)toO(N tÂ·N2 s+N sÂ·N2 t)whereN tandN sare the number of temporal and spatial tokens respectively. However, both paradigms remain computationally prohibitive due to quadratic token scaling for long videos. Token Merging.ToMe [8] serves as the foundation for most subsequent token merging methods. It progressively reduces token redundancy by mergingRtokens at each transformer layer through Bipartite Soft Matching (BSM). BSM operates through three sequential steps: (1) Partitioning: Input tokens are alternately divided into two disjoint sets,AandB. (2) Matching: Each source token inAis assigned to its most 4 SUBMISSION OF IEEE TRANSACTIONS ON MEDICAL IMAGING (a) Temporal token similarities (b) Spatial token similarities Fig. 3. Token similarity analysis across temporal and spatial dimen- sions. (a) Temporal similarities at varying inter-frame distances on Auto- Laparo [29] dataset (ninth transformer block). (b) Block-wise intra-frame similarity of static vs. dynamic tokens on CholecSeg8k [30] dataset. similar target token inB, typically based on cosine similarity between their attention key representations. (3) Merging: The top-Rmost similar matched pairs are merged via average pooling. This process occurs between the attention and MLP blocks, reducing the token count while preserving feature propagation. This plug-in approach is appealing for video transformers to enhance efficiency. However, ToMe overlooks the varying informativeness of spatiotemporal tokens. B. Theoretic Inspiration Token Similarity Analysis.To identify optimal token merg- ing candidates, we analyze features of Surgformer (trained on Autolaparo [29]) using cosine similarity. (1) Temporal Information Pattern. As shown in Fig. 3 (a), we compute similarity between: (i) tokens at identical spatial positions across varying inter-frame intervals, and (ii) tokens at different spatial positions",
    "Analysis.To identify optimal token merg- ing candidates, we analyze features of Surgformer (trained on Autolaparo [29]) using cosine similarity. (1) Temporal Information Pattern. As shown in Fig. 3 (a), we compute similarity between: (i) tokens at identical spatial positions across varying inter-frame intervals, and (ii) tokens at different spatial positions withinW=kÃ—kneighborhood windows across equivalent temporal separations. We find that spatially corresponding tokens in adjacent frames exhibit the highest similarity. Following principles from representation learning theory [31], this suggests that such token pairs encode more shared information, consistent with higher mutual information: I(xi t, xi t+1)> I(xi t, xW t+d),âˆ€d >1. (2) Spatial Information Pattern. As shown in Fig. 3 (b), tokens within each frame are classified as static or dynamic based on their temporal consistency in segmentation masks (dynamic: the mask la- bel changes between two consecutive frames). The analysis reveals that static tokens consistently have higher intra-frame similarity across transformer blocks, which suggests that static tokens exhibit lower conditional entropy given their spatial context,i.e.,H(X static|Context)< H(X dynamic |Context). This implies that static regions are highly predictable based on their surroundings, while dynamic regions contain more unpredictable and task-relevant information. Information Bottleneck Formulation.LetZrepresent the original output tokens,Z mdenote the merged tokens,Xbe the input image,Ybe the task label. The information bottleneck (IB) principle [32] seeks to maximize the mutual information betweenZ mandYwhile minimizing the mutual information betweenZ mandX. In other words, effective token merging should maximize task-relevant information while minimizing input redundancy. We formulate STIM-TM as approximately optimizing:min O IB=I(Z m, X)âˆ’I(Z m, Y). Directlyoptimizing this objective faces challenges such as the non- separability of mutual information and computational com- plexity [33]. Our training-free approach implicitly follows the IB principle and decomposesO IBinto temporal and spatial components:O STIM-TM =Î±O temporal + (1âˆ’Î±)O spatial . Here, both temporal and spatial components contribute to the IB objective.O temporal minimizesI(Z m, X)by merging high mutual information tokens (spatially aligned adjacent frames) while maximizingI(Z m, Y)through saliency-weighted fu- sion. Meanwhile,O spatial minimizesI(Z m, X)by prioritizing merging static tokens while maximizingI(Z m, Y)by preserv- ing more tokens in task-relevant dynamic regions. In practice, we employ computationally efficient heuristics to approximate the IB objectives. These practical implementations are detailed in the following sections. C. Overview of STIM-TM As illustrated in Fig.2, STIM-TM processes densely sam- pled input frames and progressively merges semantically simi- lar tokens during encoding to reduce computational overhead. Given an input videoVâˆˆRTÃ—HÃ—WÃ—3, we tokenize each frame intoN Spatches and prepend a[CLS]token for global video representation. Using TimeSformer [11] as our example encoder, we insert temporal token merging (TIM-TM) in Sec. III-D after temporal attention and spatial token merging (SIM-TM) in Sec. III-E after spatial attention. After each merging, TIM-TM reducesR Tframes while SIM-TM reduces RStokens per frame. These components can be deployed: either sequentially across different transformer blocks (e.g., temporal merging in earlier layers, spatial merging in later layers), or jointly within the same blocks (with temporal merging preceding spatial merging). Both strategies achieve effective token reduction with different efficiency-accuracy trade-offs, as detailed in the experimental section. D. Temporal Token Merging Our analysis in Sec. III-B",
    "transformer blocks (e.g., temporal merging in earlier layers, spatial merging in later layers), or jointly within the same blocks (with temporal merging preceding spatial merging). Both strategies achieve effective token reduction with different efficiency-accuracy trade-offs, as detailed in the experimental section. D. Temporal Token Merging Our analysis in Sec. III-B reveals that spatially correspond- ing tokens between adjacent frames exhibit significantly higher similarities, suggesting that effective temporal merging should respect this temporal structure. However, existing methods apply uniform strategies without considering temporal proxim- ity, potentially disrupting temporal coherence. To address this limitation, we propose adjacent-frame spatial-aligned merging that compresses consecutive frames while preserving essential procedural dynamics and temporal sequence integrity. To achieve this goal, our temporal merging restricts merging can- didates to consecutive frames and iteratively merges the most similar temporal pairs. We first calculate temporal redundancy as the similarity of features at the same spatial location across consecutive video frames. Formally, we compute the cosine similarity between the attention keyskt iandkt+1 iat thei-th spatial location in framestandt+ 1: simt i=kt iÂ·kt+1 i |kt i||kt+1 i|.(2) For each spatial positioniâˆˆ {1, ..., N S}acrossTframes, we compute similarities for all adjacent pairs(t, t+ 1). Next, we AUTHORet al.: SUBMISSION OF IEEE TRANSACTIONS ON MEDICAL IMAGING 5 identify the pair of adjacent frames( Ë†t,Ë†t+ 1)with the highest similarity as merging candidates: (Ë†t,Ë†t+ 1) = arg max t(simt i).(3) Finally, we merge the selected frame pair using saliency- weighted fusion to minimize task-relevant information loss. We compute the negative entropy of temporal attention scores At,tâ€² ifrom the current transformer block as token saliency, which serves as an information-theoretic measure of token importance. For spatial tokeniat timet, the negative entropy Ht imeasures attention concentration: Ht i=TX tâ€²=1At,tâ€² ilogAt,tâ€² i.(4) HigherHt ivalues indicate focused attention (salient tokens, typically foreground actions), while lower values indicate dispersed attention (background regions). The saliency weights are obtained by normalizing the entropy values through min- max scaling: Î±t i=Ht iâˆ’min Ï„(HÏ„ i) max Ï„(HÏ„ i)âˆ’min Ï„(HÏ„ i).(5) We fuse tokens from the selected pair using these saliency weights: xmerged i =Î±Ë†t iâˆ—xË†t i+Î±Ë†t+1 iâˆ—xË†t+1 i,(6) whereÎ±Ë†t iandÎ±Ë†t+1 iare the weights of the selected frames Ë†tand Ë†t+1respectively. This entropy-based weighting embodies the information bottleneck principle by dynamically preserving high-information content during merging. We achieveR T-frame reduction throughR Tsuccessive merging iterations, merging the most similar adjacent pair at each step. This position-wise, adjacent-frame merging strat- egy preserves coherent spatial structures across compressed frames, without interfering with subsequent spatial modeling. E. Spatial Token Merging Section III-B highlights that spatial redundancy exhibits high non-uniformity: static regions have low information vari- ance, while dynamic regions are rich in task-relevant content. To exploit this asymmetry, we propose static-prioritized merg- ing, which aggressively compresses static areas and preserves more tokens for dynamic regions. To identify stable static regions, we first partitionTframes intoKtemporal segments. We compute the average cosine similarityS(t)between cor- responding spatial tokens in adjacent framestandt+ 1: S(t) =1 NSNSX i=1kt iÂ·kt+1 i |kt i||kt+1 i|,(7) wherekt idenotes the attention key of spatial tokeniat timet. To detect temporal boundaries, we calculate a depth scoring [34] function: D(t) = max",
    "first partitionTframes intoKtemporal segments. We compute the average cosine similarityS(t)between cor- responding spatial tokens in adjacent framestandt+ 1: S(t) =1 NSNSX i=1kt iÂ·kt+1 i |kt i||kt+1 i|,(7) wherekt idenotes the attention key of spatial tokeniat timet. To detect temporal boundaries, we calculate a depth scoring [34] function: D(t) = max Ï„<tS(Ï„) + max Ï„>tS(Ï„)âˆ’2S(t).(8) Intuitively,D(t)measures how distinct frametis from its neighbors; local maxima indicate transitions and are selectedas segment boundaries. Specifically, we choose theb=Kâˆ’ 1largest local maxima ofD(t)as boundaries. Within each temporal segment, we quantify the spatial stability of each spatial positioniby computing its static scoreÂµ ibased on cross-frame token similarity: Âµi=2 TS(TSâˆ’1)TSX t=1X tâ€²>tkt iÂ·ktâ€² i |kt i||ktâ€² i|,(9) whereT Sis the segment length and{kt i}TS t=1denotes the token sequence for positioniacross all frames in the segment. A higherÂµ iindicates that positioniremains stable throughout the segment, while a lower value reflects dynamic behavior. We prioritize merging tokens with high static scores to retain informative regions. Specifically, for each merging operation that aims to removeR Stokens per frame, we first select the topmÃ—R Sspatial positions with the highest static scores Âµin a single frame as merging candidates. Following the ToMe framework, these candidates are partitioned into two disjoint sets,AandB. Cross-set matching is then performed to identify the matched pairs, after which the topR Smost similar pairs are merged via size-weighted averaging. This ap- proach accounts for spatial information density by restricting merging to low-information stable regions, while preserving more unmerged tokens for dynamic areas. To identify static regions at multiple temporal scales, tem- poral segmentation is performed at multiple granularity levels across encoder layers: K=( 1ifâ„“â‰¤L/2 2ifâ„“ > L/2,(10) whereLis the total number of encoder layers, andâ„“is the layer index. Coarse segmentation in shallow layers identifies broadly stable regions across longer sequences, while finer segmentation in deeper layers captures more nuanced temporal variations. This hierarchical approach allows static redundancy of different granularities to be identified. IV. EXPERIMENT A. Experiment setup Baselines and Datasets. We apply STIM-TM as a training- free plug-in module to three strong surgical video under- standing baselines: (1)Surgformer[3]: A TimeSformer- based model for surgical phase recognition, evaluated on Cholec80 [35] (80 cholecystectomy videos, 7 phases) and Autolaparo [29] (21 laparoscopic hysterectomy videos, 7 phases). We use temporal resolution ofT= 16 and a frame rate of 4 for both datasets. During testing,T= 16 is used for Autolaparo, andT= 24 is used for Cholec80. (2) TAPIS[4]: A hybrid framework fusing global spatiotempo- ral features from MViT [36] with instrument-aware regional features from Mask2Former [37]. We apply temporal-only token merging (TIM-TM) within MViT, as spatial pooling is inherent to its architecture. TAPIS model is trained on the GraSP [4] dataset (13 robot-assisted radical prostatec- tomy videos) using 16-frame clips. (3)EndoFM-LV[9]: A self-supervised TimeSformer-based framework pre-trained on 6 SUBMISSION OF IEEE TRANSACTIONS ON MEDICAL IMAGING TABLE I COMPARISON OF TOKEN REDUCTION METHODS ONSURGFORMER(AUTOLAPARO DATASET),INCLUDING ABLATIONS ON REDUCTION METHODS, MERGING DIMENSION,MERGING STRATEGIES,AND ORDER. THE UNRELAXED EVALUATION IS USED. TANDSREPRESENT TEMPORAL AND SPATIAL MERGING. SYMBOLS INDICATE INSERTION LOCATIONS WITHIN THE12-LAYER TRANSFORMER BACKBONE:â™£T: BLOCKS1-6, S: BLOCKS7-12;â€  S: BLOCKS1-6, T: BLOCKS7-12;â™¢T: BLOCKS1-6, S: BLOCKS1-6. Method Type RTRSGFLOPsâ†“Memory(GB)â†“Video-level",
    "MEDICAL IMAGING TABLE I COMPARISON OF TOKEN REDUCTION METHODS ONSURGFORMER(AUTOLAPARO DATASET),INCLUDING ABLATIONS ON REDUCTION METHODS, MERGING DIMENSION,MERGING STRATEGIES,AND ORDER. THE UNRELAXED EVALUATION IS USED. TANDSREPRESENT TEMPORAL AND SPATIAL MERGING. SYMBOLS INDICATE INSERTION LOCATIONS WITHIN THE12-LAYER TRANSFORMER BACKBONE:â™£T: BLOCKS1-6, S: BLOCKS7-12;â€  S: BLOCKS1-6, T: BLOCKS7-12;â™¢T: BLOCKS1-6, S: BLOCKS1-6. Method Type RTRSGFLOPsâ†“Memory(GB)â†“Video-level Metric Phase-level Metric Accuracyâ†‘Precisionâ†‘Recallâ†‘Jaccardâ†‘ Baseline (w/o token reduction) - - 459.39 4.75 85.88Â±5.82 83.45 74.33 65.13 (1) Token Merging v.s. Pruning STTS [24] Decoupled Pruning 1 12 291.70 3.83 79.63Â±7.31 72.65 67.02 56.83 STA [25] Joint Pruning - - 303.67 4.03 80.30Â±4.52 78.52 69.22 55.71 DynamicViT [23] Decoupled Pruning 1 12 300.37 3.12 84.82Â±6.43 78.01 73.68 63.82 ToMe [8] Joint Merging - - 304.47 3.78 80.99Â±6.24 69.59 70.65 58.57 TempMe [20] Joint Merging - - 289.21 3.66 84.09Â±7.15 77.79 73.31 63.08 DyCoke [17] Temporal Merging - - 314.26 2.85 81.64Â±5.75 70.47 69.71 58.75 VisionZip [15] Spatial Merging - 12 292.61 3.54 84.52Â±7.09 79.16 73.11 63.50 T-TESTA [26] Temporal Merging 1 - 281.02 3.59 84.34Â±5.67 76.59 73.92 62.99 S-TESTA [26] Spatial Merging - 12 292.64 3.66 84.36Â±7.51 76.97 73.02 63.34 ST-TESTA [26] Decoupled Merging 1 12 300.40 3.74 85.23Â±6.08 78.44 74.59 64.69 STIM-TM(Ours) Decoupled Merging 1 12 300.37 3.77 86.04Â±5.58 80.47 74.32 65.20 (2) Merging dimension Only Temporal 1 - 281.02 3.70 85.67Â±5.24 78.34 73.85 64.51 Only Spatial - 12 299.61 3.55 85.96Â±5.93 81.77 74.49 65.29 Both Temporal and Spatial 1 12 300.37 3.77 86.04Â±5.58 80.47 74.32 65.20 (3) Temporal Merging Strategy Adjacent-Frame Merging 1 - 281.02 3.70 85.47Â±5.26 80.40 73.79 64.34 w/ Saliency-Weighted Merging 1 - 281.02 3.70 85.67Â±5.24 78.34 73.85 64.51 (4) Spatial Merging Strategy Intra-Frame Merging - 12 292.64 3.66 84.36Â±7.51 76.97 73.02 63.34 w/ Static-prioritized Merging (1 segment) - 12 292.61 3.55 85.64Â±5.79 80.62 74.28 64.86 w/ Static-prioritized Merging (2 segments) - 12 292.60 3.55 85.38Â±6.21 77.82 73.70 64.22 w/ Hierarchical Static-prioritized Merging - 12 299.61 3.55 85.96Â±5.93 81.77 74.49 65.29 (5) Merging Order Temporal First, then Spatialâ™£ 1 12 300.37 3.77 86.04Â±5.58 80.47 74.32 65.20 Spatial First, then Temporalâ€  1 12 304.05 3.72 85.13Â±5.95 84.86 73.52 64.14 Parallel Temporal-Spatialâ™¢ 1 3 305.02 3.81 85.93Â±5.64 78.44 73.99 64.80 Parallel Temporal-Spatialâ™¢ 1 12 243.06 3.30 85.81Â±5.95 82.20 74.49 65.09 6,469 endoscopy videos. We evaluate the effectiveness of STIM-TM on two downstream tasks: disease diagnosis using the PolypDiag dataset [38] with fixed 32-frame clips, and polyp segmentation using the CVC-12K dataset [39] with variable-length sequences (6â€“25 frames). Implementation details. We follow the default settings of the three baselines [3], [4], [9] to train the models. All experiments are performed on NVIDIA RTX 3090 GPUs. For fair comparison, we adjust the merging number (R Tand RS) to ensure comparable computational overhead. For spatial merging, the hyperparametermis set to 2 for training-free inference, whereas it is set to 4 during model training. We evaluate the computational efficiency by measuring GFLOPs and Memory. To quantify IB performance, we compute the IB scoreI(Z m, X)âˆ’I(Z m, Y)following LTM [33], where the feature distributions ofZ mandXare estimated using class centroids and softmax-normalized distances. B. Effectiveness on Different Baselines Results on Surgical Phase Recognition.As shown in Table",
    "evaluate the computational efficiency by measuring GFLOPs and Memory. To quantify IB performance, we compute the IB scoreI(Z m, X)âˆ’I(Z m, Y)following LTM [33], where the feature distributions ofZ mandXare estimated using class centroids and softmax-normalized distances. B. Effectiveness on Different Baselines Results on Surgical Phase Recognition.As shown in Table I and Table II, we evaluate token merging on Surgformer trained on Autolaparo and Cholec80 datasets, comparing baseline, TESTA, and STIM-TM. On Autolaparo (Table I), STIM- TM demonstrates significant advantages over TESTA whileachieving performance comparable to the baseline atR T= 1, RS= 12(Accuracy: 86.04%; Jaccard: 65.20). On Cholec80 (Table II), atR T= 1,R S= 12, STIM-TM exhibits minimal degradation (-0.37 relaxed accuracy, -0.6 relaxed Jaccard), while TESTA suffers substantial drops (-3.55 relaxed accu- racy, -13.81 relaxed Jaccard). Notably, TESTA exhibits larger degradation at lower merging rates (13.81 relaxed Jaccard drop atR T= 1,R S= 12) compared to higher rates (5.65 drop at RT= 2,R S= 24). This reflects the instability of TESTAâ€™s global merging strategy, which may disrupt critical spatiotem- poral structures. In contrast, STIM-TM maintains consistent superior performance, demonstrating exceptional preservation of discriminative features under aggressive reduction. Results on Holistic Surgical Scene Understanding.Table III shows the performance comparison of token merging on TAPIS. Key observations reveal differential task sensitivity to information loss from merging: Instrument segmentation and action detection, which leverage spatial-temporal features indirectly via cross-attention mechanisms, exhibit significantly smaller performance degradation compared to phase recogni- tion and step recognition. Critically, STIM-TM consistently outperforms TESTA across phase/step recognition and action detection, while performing comparably to TESTA on instru- ment segmentation. This demonstrates the robust adaptability AUTHORet al.: SUBMISSION OF IEEE TRANSACTIONS ON MEDICAL IMAGING 7 TABLE II COMPARISON OFTOKENMERGINGMETHODS ONSURGFORMER(CHOLEC80DATASET)WITHDIFFERENTMERGINGRATES. BOTH RELAXED AND UNRELAXED EVALUATIONS ARE USED. WE APPLY TOKEN MERGING TOTEMPORAL: BLOCKS1-6, SPATIAL: BLOCKS7-12. Evaluation RTRSGFLOPsâ†“Memory(GB)â†“ MethodVideo-level Metric Phase-level Metric Accuracyâ†‘Precisionâ†‘Recallâ†‘Jaccardâ†‘ Relaxed- - 689.97 6.56 Baseline 93.33Â±6.33 91.79 92.09 84.01 1 12510.09 5.43 ST-TESTA 89.78Â±6.10 84.79 82.71 70.20 510.05 5.47 STIM-TM(Ours) 92.96Â±6.70 91.97 91.28 83.41 2 24366.68 4.30 ST-TESTA 91.64Â±6.30 89.16 89.90 78.36 366.66 4.46 STIM-TM(Ours) 92.35Â±6.91 91.30 90.27 81.72 Unrelaxed- - 689.97 6.56 Baseline 92.30Â±6.31 87.82 89.27 79.79 1 12510.09 5.43 ST-TESTA 88.11Â±6.27 77.04 76.29 64.28 510.05 5.47 STIM-TM(Ours) 91.97Â±6.72 88.04 88.29 79.35 2 24366.68 4.30 ST-TESTA 90.28Â±6.31 83.42 85.12 73.27 366.66 4.46 STIM-TM(Ours) 91.21Â±6.89 86.99 86.48 77.07 TABLE III COMPARISON OF TOKEN MERGING METHODS ONTAPIS (GRASPDATASET). FOR THE16-LAYER BACKBONEMVIT,WE APPLY TEMPORAL MERGING TOBLOCKS7-13. RTRSGFLOPsâ†“ Memory(GB)â†“ MethodPhases Steps Instruments Actions mAPâ†‘ F1 scoreâ†‘ mAPâ†‘ F1 scoreâ†‘ mAP@0.5IOU boxâ†‘ mAPâ†‘ - - 70.80 3.38 Baseline 73.75 65.55 51.38 47.01 86.59 39.50 1 - 47.98 2.69 T-TESTA 68.73 59.17 46.03 39.47 86.59 37.48 1 - 47.98 2.69 TIM-TM(Ours) 70.14 60.37 46.39 40.24 86.56 38.13 and task-agnostic effectiveness of our proposed method. Results on Surgical Foundation Model.Table IV and Ta- ble V present the results on polyp classification and polyp segmentation tasks using EndoFM-LV , respectively. For the classification task, STIM-TM incurs no performance degra- dation at both merging rates. The segmentation task requires predicting dense, frame-wise segmentation maps, necessitating a token unmerging operation to restore the featuresâ€™ original resolution before inputting",
    "present the results on polyp classification and polyp segmentation tasks using EndoFM-LV , respectively. For the classification task, STIM-TM incurs no performance degra- dation at both merging rates. The segmentation task requires predicting dense, frame-wise segmentation maps, necessitating a token unmerging operation to restore the featuresâ€™ original resolution before inputting to the decoder. We implement token unmerging following the ALGM [13] method, which replicates merged token embeddings at their original source token positions. As can be seen, STIM-TM consistently out- performs TESTA on both tasks. For the segmentation task, the performance is slightly reduced, with Dice decreases of 0.1 and 0.8 at the two settings, respectively. This demonstrates the broad applicability of our STIM-TM across different tasks. C. Extension to Training Stage STIM-TM is also applicable during training for reducing to- ken length. We compare STIM-TM and TESTA across tempo- ral (TIM-TM vs. T-TESTA), spatial (SIM-TM vs. S-TESTA), and combined dimensions (STIM-TM vs. ST-TESTA) on the Surgformer. In the temporal dimension, TIM-TM substantially outperforms TESTA. TESTAâ€™s global frame merging disrupts the procedural coherence, while our method preserves tempo- ral continuity. In the spatial dimension, SIM-TM demonstrates superior performance over TESTA. TESTAâ€™s spatial merging fails to consider the heterogeneous distribution of informative regions, while SIM-TM retains more task-relevant information. When combining temporal and spatial merging (applying par- allel STIM-TM operations for optimal efficiency), STIM-TMTABLE VI COMPARISON OFIBSCORES FOR TOKEN REDUCTION METHODS. Method Accuracyâ†‘ I(Zm, X)â†“I(Z m, Y)â†‘IB scoreâ†“ STTS [24] 79.63 0.0020570.209915 -0.207858 STA [25] 80.30 0.002192 0.207727 -0.205534 DynamicViT [23] 84.82 0.002814 0.270769 -0.267955 ToMe [8] 80.99 0.003223 0.242318 -0.239095 TempMe [20] 84.09 0.002590 0.253951 -0.251361 DyCoke [17] 81.64 0.002680 0.183614 -0.180935 VisionZip [15] 84.52 0.003264 0.275990 -0.272726 ST-TESTA [26] 85.23 0.003182 0.274063 -0.270882 STIM-TM(Ours) 86.04 0.0025180.277328 -0.274810 achieves a 53% reduction in GFLOPs and 64% reduction in Memory, while maintaining minimal performance loss (0.1% accuracy). Remarkably, TIM-TM alone exceeds baseline per- formance, suggesting that merging redundant tokens enables the model to concentrate on more informative features. D. Ablation Analysis We perform ablation studies on Surgformer trained on AutoLaparo to explore the answer of the following questions: (Q-1): Which token reduction strategy is more effective? (Q- 2): Which dimension should be merged? (Q-3): Which tem- poral strategy performs better? (Q-4): Which spatial strategy performs better? (Q-5): What is the optimal merging order? (Q-6): How do merging rates affect performance? Ans-1: Results on Token Reduction Strategies.We compare token pruning and merging methods in Table I (1). STTS [24] estimates the token importance through a learnable scoring network and requires fine-tuning, while other methods are 8 SUBMISSION OF IEEE TRANSACTIONS ON MEDICAL IMAGING TABLE IV COMPARISON OF MERGING METHODS ON THE ENDOFM-LVCLASSIFICATION TASK. TEMPORAL(BLOCKS1-6), SPATIAL(BLOCKS 7-12). RTRSGFLOPsâ†“Mem.â†“ Method F1â†‘ - - 786.82 7.48 Baseline 97.5 1 12615.26 6.49 ST-TESTA 97.5 615.21 6.78 STIM-TM 97.5 2 24471.31 5.29 ST-TESTA 96.7 471.28 5.74 STIM-TM 97.5TABLE V COMPARISON OF MERGING METHODS ON THEENDOFM-LVSEGMENTATION TASK. TEMPORAL(BLOCKS1-3), SPATIAL(BLOCKS4-12). RTRSGFLOPsâ†“ Method Diceâ†‘ - - 690.52 Baseline 83.6 1 12543.82 ST-TESTA 82.7 543.78 STIM-TM 83.5 1 24467.69 ST-TESTA 82.5 467.68 STIM-TM 82.8 Fig. 4. Performance comparison of different candi- date sizemin",
    "2 24471.31 5.29 ST-TESTA 96.7 471.28 5.74 STIM-TM 97.5TABLE V COMPARISON OF MERGING METHODS ON THEENDOFM-LVSEGMENTATION TASK. TEMPORAL(BLOCKS1-3), SPATIAL(BLOCKS4-12). RTRSGFLOPsâ†“ Method Diceâ†‘ - - 690.52 Baseline 83.6 1 12543.82 ST-TESTA 82.7 543.78 STIM-TM 83.5 1 24467.69 ST-TESTA 82.5 467.68 STIM-TM 82.8 Fig. 4. Performance comparison of different candi- date sizemin training and training-free mode. TABLE VII TRAINING WITH TOKEN MERGING METHODS. WE PLUGST-TESTAOR STIM-TMINTO THESURGFORMER DURING BOTH TRAINING AND INFERENCE. Method RTRSGFLOPsâ†“Mem.â†“Video-level Phase-level Acc.â†‘Prec.â†‘Rec.â†‘Jacc.â†‘ SV-RCNet [40] - - - - 75.6 64.0 59.7 47.2 TMRNet [41] - - - - 78.2 66.0 61.5 49.6 TeCNO [42] - - - - 77.3 66.9 64.6 50.7 Trans-SVNet [43] - - - - 78.3 64.2 62.1 50.7 A VT [44] - - - - 77.8 68.0 62.2 50.7 LoViT [2] - - - - 81.4Â±7.6 85.1 65.9 56.0 SKiT [12] - - - - 82.9Â±6.8 81.8 70.1 59.9 Surgformer [3] - - 918.78 21.90 85.9Â±5.8 83.4 74.3 65.1 +T-TESTA 1 - 562.04 15.65 83.4Â±7.7 77.6 72.2 62.5 +S-TESTA - 12 585.28 15.78 85.0Â±7.0 78.3 71.5 62.9 +ST-TESTA 1 12 486.18 14.03 81.8Â±7.4 75.0 70.5 59.5 +TIM-TM(Ours) 1 - 562.04 16.47 86.7Â±7.2 81.4 78.0 69.3 +SIM-TM(Ours) -12 599.22 14.88 86.0Â±4.5 81.8 74.7 65.8 +STIM-TM(Ours) 112 486.12 14.08 85.8Â±6.5 76.8 76.7 66.3 training-free. Both pruning and merging methods reduce com- putation costs compared to the baseline. Most pruning methods (STTS, STA) perform worse than merging methods, indicat- ing that token merging better retains valuable information. Decoupled merging (STIM-TM, TESTA) outperforms single- dimension merging (Dycoke, VisionZip) and joint merg- ing (ToMe, TempMe), confirming that disentangled process- ing better preserves video structural information. STIM-TM achieves the best results among all methods. Compared with the baseline, STIM-TM maintains Accuracy and Jaccard while reducing computational overhead to 65.38% GFLOPs and 79.36% GPU memory. Table VI shows the comparison of IB scores. Our STIM-TM delivers superior performance on both IB score and accuracy. Specifically, our method achieves the highestI(Z m, Y)for task-relevant information while main- taining reasonableI(Z m, X)for compression, resulting in the optimal IB score. This validates that our approach better approximates the IB objective by achieving superior balance between information compression and preservation. Ans-2: Results on Merging Dimensions.In Table I (2), we compare token merging across different dimensions: temporal only, spatial only, and both. STIM-TM achieves superior per- formance when merging across both dimensions compared to single-dimension approaches. This aligns with TESTAâ€™s results in Table I (1). Single-dimension merging creates unbalanced compression, while dual-dimension merging achieves optimalTABLE VIII ABLATION STUDY ON MERGING RATE. RESULTS ONSURGFORMER TRAINED ONAUTOLAPARO DATASET. RTRSGFLOPsâ†“Mem.â†“ MethodVideo-level Phase-level Acc.â†‘Prec.â†‘Rec.â†‘Jacc.â†‘ - - 459.39 4.75 Baseline 85.9Â±5.8 83.4 74.3 65.1 1 12300.40 3.74 ST-TESTA 85.2Â±6.1 78.4 74.6 64.7 300.37 3.77 STIM-TM 86.0Â±5.6 80.5 74.3 65.2 2 12187.79 2.84 ST-TESTA 76.0Â±7.1 66.8 69.0 52.6 187.77 2.94 STIM-TM 82.6Â±5.6 72.9 71.6 60.0 2 24177.62 2.78 ST-TESTA 75.7Â±7.0 67.3 69.2 52.7 177.61 2.86 STIM-TM 82.6Â±5.4 73.9 71.9 60.3 results by integrating temporal and spatial compression. Ans-3: Results on Temporal Merging Strategies.We com- pare different temporal strategies in Table I (3). Compared with T-TESTA in Table I (1), which globally merges features of",
    "60.0 2 24177.62 2.78 ST-TESTA 75.7Â±7.0 67.3 69.2 52.7 177.61 2.86 STIM-TM 82.6Â±5.4 73.9 71.9 60.3 results by integrating temporal and spatial compression. Ans-3: Results on Temporal Merging Strategies.We com- pare different temporal strategies in Table I (3). Compared with T-TESTA in Table I (1), which globally merges features of similar frames, our position-wise similar adjacent frame merging strategy achieves better performance, with an accu- racy improvement of 1.13%. Using saliency-weighted merging further improves the performance, indicating that saliency helps us preserve temporally critical features. Ans-4: Results on Spatial Merging Strategies.(1) Table I (4) compares different spatial strategies. We evaluate the impact of segment quantityKthrough four configurations: intra-frame token merging (performing merging independently within each frame), single-segment (K=1), dual-segment (K=2), and hi- erarchical segmentation (1 segment in initial 6 blocks, 2 segments in final 6 blocks). Among all strategies, hierarchical segmentation achieves the best performance, outperforming intra-frame merging by 1.6% accuracy improvement. This confirms that integrating global with local temporal context optimally identifies spatial redundancy. (2) In Fig. 4, we ana- lyze the impact of merging candidate sizemon performance. We find thatm= 2achieves optimal performance in training- free mode, whilem= 4delivers the best results in training mode. For training-free, the frozen pre-trained model requires conservative merging of only the most certain static regions to avoid disrupting critical features, while for training, a larger candidate pool enables more flexible merging strategies. Ans-5: Results on Merging Order.We evaluated three merging orders (Table I (5)): (1) Temporal first, then Spatial, (2) Spatial first, then Temporal, and (3) Parallel Temporal- Spatial. Results revealed a performance hierarchy: Strategy (1)>Strategy (3)>Strategy (2). This ranking reflects the AUTHORet al.: SUBMISSION OF IEEE TRANSACTIONS ON MEDICAL IMAGING 9 Original Merged Fig. 5. Visualization of temporal merging results from the 11th transformer layer of Surgformer, reducing the temporal dimension from 16 to 5 frames. We highlight merging results at three representative spatial locations, where different colors indicate distinct merged groups and black denotes unmerged tokens. Our TIM-TM preserves coherent temporal structure while retaining essential inter-frame dynamics. Original S-TESTA SIM-TM Fig. 6. Spatial merging comparison between S-TESTA and SIM-TM at Surgformerâ€™s 12th transformer layer. Patches sharing identical interior and border colors represent merged groups. In the third row, the dotted line indicates the boundary between temporal segments. SIM-TM demonstrates segment-wise consistent merging and superior dynamic region preservation compared to S-TESTAâ€™s inconsistent frame-wise merging. progressive nature of feature learning: early layers contain low-level spatial details, while deeper layers hold semantically abstracted representations with higher token similarities (as shown in Fig. 3). Strategy (1) achieves the best performance by aligning with this progression. Temporal merging first removes adjacent-frame redundancy while preserving spatial details, then spatial merging operates on abstracted features where merging is safer. Conversely, Strategy (2) prematurely dis- cards vital spatial details, while Strategy (3)â€™s aggressive dual merging degrades both dimensions before sufficient feature extraction, despite its computational efficiency. Ans-6: Results on Merging rates.Table VIII compares TESTA and STIM-TM across varying merging rates. Higher merging rates reduce computational cost but degrade perfor- mance. The configurationR T= 1,R S= 12achieves optimal efficiency-accuracy trade-off. Further",
    "Strategy (3)â€™s aggressive dual merging degrades both dimensions before sufficient feature extraction, despite its computational efficiency. Ans-6: Results on Merging rates.Table VIII compares TESTA and STIM-TM across varying merging rates. Higher merging rates reduce computational cost but degrade perfor- mance. The configurationR T= 1,R S= 12achieves optimal efficiency-accuracy trade-off. Further merging causes more severe loss of spatiotemporal information and greater per- formance drops. Notably, STIM-TM consistently outperforms TESTA at all merging rates, with its advantage becoming more pronounced as compression increases. E. Visualizations We visualize the temporal merging results on the Auto- Laparo dataset in Fig. 5. WithR T= 1per layer, temporal compression accumulates to reduce 16 frames to 5 by the 11th layer, yielding 5 groups per spatial position (colored highlights show merged groups and black tokens represent individual unmerged groups). The visualization shows that merged groups exhibit strong temporal clustering according to underlying surgical structures. At positions with significant inter-frame changes, tokens remain unmerged, ensuring tem- porally critical information is retained. The results validate that TIM-TM successfully maintains temporal structural continuity and avoids disrupting essential temporal dependencies. We illustrate spatial merging withR S= 12per layer in Fig. 6, cumulatively reducing tokens from 196 to 52 per frame by layer 12. TESTA performs frame-independent merging without considering spatial information heterogeneity,leading to information loss in critical regions and inconsistent patterns across frames. SIM-TM distinguishes between static and dynamic tokens through cross-frame stability analysis, prioritizing the merging of static tokens. The visualization demonstrates SIM-TMâ€™s effectiveness: more tokens are allo- cated to dynamic regions requiring fine-grained details (e.g., surgical instruments), while reducing tokens in static areas. Additionally, SIM-TM maintains superior motion flow consis- tency and spatiotemporal structural coherence across frames. V. CONCLUSION This work presents STIM-TM, the first dedicated token merging framework for surgical video understanding. Lever- aging information-theoretic insights into video redundancy patterns, our method introduces a decoupled approach that in- dependently addresses temporal and spatial redundancy while preserving task-relevant information. Extensive validation demonstrates STIM-TMâ€™s effectiveness, achieving substantial computational efficiency gains with minimal performance degradation during training and inference. As a training-free, plug-and-play solution, STIM-TM enables practical deploy- ment of vision transformers in resource-constrained surgical environments, advancing the development of intelligent surgi- cal systems for real-world clinical applications. REFERENCES [1] U. Khan, U. Nawaz, A. Qayyum, S. Ashraf, M. Bilal, and J. Qadir, â€œSurgical scene understanding in the era of foundation ai models: A comprehensive review,â€arXiv preprint arXiv:2502.14886, 2025. [2] Y . Liu, M. Boels, L. C. Garcia-Peraza-Herrera, T. Vercauteren, P. Das- gupta, A. Granados, and S. Ourselin, â€œLovit: Long video transformer for surgical phase recognition,â€Medical Image Analysis, vol. 99, p. 103366, 2025. [3] S. Yang, L. Luo, Q. Wang, and H. Chen, â€œSurgformer: Surgical transformer with hierarchical temporal attention for surgical phase recognition,â€ inInternational Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, 2024, pp. 606â€“616. [4] N. Ayobi, S. Rodr Â´Ä±guez, A. P Â´erez, I. Hern Â´andez, N. Aparicio, E. Des- sevres, S. Pe Ëœna, J. Santander, J. I. Caicedo, N. Fern Â´andezet al., â€œPixel-wise recognition for holistic surgical scene understanding,â€arXiv preprint arXiv:2401.11174, 2024. 10 SUBMISSION OF IEEE TRANSACTIONS ON MEDICAL",
    "Springer, 2024, pp. 606â€“616. [4] N. Ayobi, S. Rodr Â´Ä±guez, A. P Â´erez, I. Hern Â´andez, N. Aparicio, E. Des- sevres, S. Pe Ëœna, J. Santander, J. I. Caicedo, N. Fern Â´andezet al., â€œPixel-wise recognition for holistic surgical scene understanding,â€arXiv preprint arXiv:2401.11174, 2024. 10 SUBMISSION OF IEEE TRANSACTIONS ON MEDICAL IMAGING [5] H. Liu, M. Gao, X. Luo, Z. Wang, G. Qin, J. Wu, and Y . Jin, â€œResurgsam2: Referring segment anything in surgical video via cred- ible long-term tracking,â€International Conference on Medical Image Computing and Computer Assisted Intervention, 2025. [6] J. Pei, J. Zhang, G. Qin, K. Wang, Y . Jin, and P.-A. Heng, â€œInstrument- tissue-guided surgical action triplet detection via textual-temporal trail exploration,â€IEEE Transactions on Medical Imaging, 2025. [7] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gellyet al., â€œAn image is worth 16x16 words: Transformers for image recognition at scale,â€International Conference on Learning Representations, 2021. [8] D. Bolya, C.-Y . Fu, X. Dai, P. Zhang, C. Feichtenhofer, and J. Hoffman, â€œToken merging: Your vit but faster,â€ inThe Eleventh International Conference on Learning Representations, 2023. [9] Z. Wang, C. Liu, L. Zhu, T. Wang, S. Zhang, and Q. Dou, â€œImprov- ing foundation model for endoscopy video analysis via representation learning on long sequences,â€IEEE Journal of Biomedical and Health Informatics, 2025. [10] Z. Liu, J. Ning, Y . Cao, Y . Wei, Z. Zhang, S. Lin, and H. Hu, â€œVideo swin transformer,â€ inProceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022, pp. 3202â€“3211. [11] G. Bertasius, H. Wang, and L. Torresani, â€œIs space-time attention all you need for video understanding?â€ inInternational Conference on Machine Learning, vol. 2, no. 3, 2021, p. 4. [12] Y . Liu, J. Huo, J. Peng, R. Sparks, P. Dasgupta, A. Granados, and S. Ourselin, â€œSkit: a fast key information video transformer for online surgical phase recognition,â€ inProceedings of the IEEE/CVF Interna- tional Conference on Computer Vision, 2023, pp. 21 074â€“21 084. [13] N. Norouzi, S. Orlova, D. De Geus, and G. Dubbelman, â€œAlgm: Adaptive local-then-global token merging for efficient semantic segmentation with plain vision transformers,â€ inProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 15 773â€“15 782. [14] S. Lee, J. Choi, and H. J. Kim, â€œMulti-criteria token fusion with one- step-ahead attention for efficient vision transformers,â€ inProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 15 741â€“15 750. [15] S. Yang, Y . Chen, Z. Tian, C. Wang, J. Li, B. Yu, and J. Jia, â€œVisionzip: Longer is better but not necessary in vision language models,â€ inPro- ceedings of the Computer Vision and Pattern Recognition Conference, 2025, pp. 19 792â€“19 802. [16] Y . Shang, M. Cai, B. Xu, Y . J. Lee, and Y . Yan, â€œLlava-prumerge: Adaptive token reduction for efficient large multimodal models,â€ in ICCV, 2025. [17] K. Tao, C. Qin, H. You, Y . Sui, and H. Wang, â€œDycoke: Dynamic compression of tokens for fast video large language models,â€The IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2025. [18]",
    "and Y . Yan, â€œLlava-prumerge: Adaptive token reduction for efficient large multimodal models,â€ in ICCV, 2025. [17] K. Tao, C. Qin, H. You, Y . Sui, and H. Wang, â€œDycoke: Dynamic compression of tokens for fast video large language models,â€The IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2025. [18] J. Choi, S. Lee, J. Chu, M. Choi, and H. J. Kim, â€œvid-tldr: Training free token merging for light-weight video transformer,â€ inProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 18 771â€“18 781. [19] S.-H. Lee, J. Wang, Z. Zhang, D. Fan, and X. Li, â€œVideo token merging for long video understanding,â€Advances in Neural Information Processing Systems, vol. 37, pp. 13 851â€“13 871, 2024. [20] L. Shen, T. Hao, T. He, S. Zhao, Y . Zhang, P. Liu, Y . Bao, and G. Ding, â€œTempme: Video temporal token merging for efficient text- video retrieval,â€The Thirteenth International Conference on Learning Representations, 2025. [21] Z. Wang, C. Liu, S. Zhang, and Q. Dou, â€œFoundation model for endoscopy video analysis via large-scale self-supervised pre-train,â€ in International Conference on Medical Image Computing and Computer- Assisted Intervention. Springer, 2023, pp. 101â€“111. [22] T. J. Jaspers, R. L. de Jong, Y . Li, C. H. Kusters, F. H. Bakker, R. C. van Jaarsveld, G. M. Kuiper, R. van Hillegersberg, J. P. Ruurda, W. M. Brinkmanet al., â€œScaling up self-supervised learning for improved surgical foundation models,â€arXiv preprint arXiv:2501.09436, 2025. [23] Y . Rao, W. Zhao, B. Liu, J. Lu, J. Zhou, and C.-J. Hsieh, â€œDynam- icvit: Efficient vision transformers with dynamic token sparsification,â€ Advances in neural information processing systems, vol. 34, pp. 13 937â€“ 13 949, 2021. [24] J. Wang, X. Yang, H. Li, L. Liu, Z. Wu, and Y .-G. Jiang, â€œEfficient video transformers with spatial-temporal token selection,â€ inEuropean Conference on Computer Vision. Springer, 2022, pp. 69â€“86. [25] S. Ding, P. Zhao, X. Zhang, R. Qian, H. Xiong, and Q. Tian, â€œPrune spatio-temporal tokens by semantic-aware temporal accumulation,â€ in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 16 945â€“16 956.[26] S. Ren, S. Chen, S. Li, X. Sun, and L. Hou, â€œTesta: Temporal- spatial token aggregation for long-form video-language understanding,â€ inThe 2023 Conference on Empirical Methods in Natural Language Processing, 2023. [27] X. Li, C. Ma, X. Yang, and M.-H. Yang, â€œVidtome: Video token merging for zero-shot video editing,â€ inProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 7486â€“7495. [28] Y . Su, J. Zhang, R. Yan, P. Li, G. Xie, and X. Shu, â€œStpm: Spatial- temporal token pruning and merging for complex activity recognition,â€ IEEE Transactions on Circuits and Systems for Video Technology, 2024. [29] Z. Wang, B. Lu, Y . Long, F. Zhong, T.-H. Cheung, Q. Dou, and Y . Liu, â€œAutolaparo: A new dataset of integrated multi-tasks for image- guided surgical automation in laparoscopic hysterectomy,â€ inInterna- tional Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, 2022, pp. 486â€“496. [30] W.-Y . Hong, C.-L. Kao, Y .-H. Kuo, J.-R. Wang, W.-L. Chang, and C.-S. Shih, â€œCholecseg8k: a semantic segmentation dataset for laparoscopic cholecystectomy based",
    "of integrated multi-tasks for image- guided surgical automation in laparoscopic hysterectomy,â€ inInterna- tional Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, 2022, pp. 486â€“496. [30] W.-Y . Hong, C.-L. Kao, Y .-H. Kuo, J.-R. Wang, W.-L. Chang, and C.-S. Shih, â€œCholecseg8k: a semantic segmentation dataset for laparoscopic cholecystectomy based on cholec80,â€arXiv preprint arXiv:2012.12453, 2020. [31] A. v. d. Oord, Y . Li, and O. Vinyals, â€œRepresentation learning with contrastive predictive coding,â€arXiv preprint arXiv:1807.03748, 2018. [32] N. Tishby and N. Zaslavsky, â€œDeep learning and the information bottleneck principle,â€ in2015 ieee information theory workshop (itw). Ieee, 2015, pp. 1â€“5. [33] Y . Wang and Y . Yang, â€œEfficient visual transformer by learnable token merging,â€IEEE Transactions on Pattern Analysis and Machine Intelli- gence, 2025. [34] Y . Wang, C. Xie, Y . Liu, and Z. Zheng, â€œVideollamb: Long-context video understanding with recurrent memory bridges,â€arXiv preprint arXiv:2409.01071, 2024. [35] A. P. Twinanda, S. Shehata, D. Mutter, J. Marescaux, M. De Mathelin, and N. Padoy, â€œEndonet: a deep architecture for recognition tasks on laparoscopic videos,â€IEEE transactions on medical imaging, vol. 36, no. 1, pp. 86â€“97, 2016. [36] H. Fan, B. Xiong, K. Mangalam, Y . Li, Z. Yan, J. Malik, and C. Feichten- hofer, â€œMultiscale vision transformers,â€ inProceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 6824â€“6835. [37] B. Cheng, I. Misra, A. G. Schwing, A. Kirillov, and R. Girdhar, â€œMasked-attention mask transformer for universal image segmentation,â€ inProceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022, pp. 1290â€“1299. [38] Y . Tian, G. Pang, F. Liu, Y . Liu, C. Wang, Y . Chen, J. Verjans, and G. Carneiro, â€œContrastive transformer-based multiple instance learning for weakly supervised polyp frame detection,â€ inInternational Confer- ence on Medical Image Computing and Computer-Assisted Intervention. Springer, 2022, pp. 88â€“98. [39] J. Bernal, F. J. S Â´anchez, G. Fern Â´andez-Esparrach, D. Gil, C. Rodr Â´Ä±guez, and F. Vilari Ëœno, â€œWm-dova maps for accurate polyp highlighting in colonoscopy: Validation vs. saliency maps from physicians,â€Comput- erized medical imaging and graphics, vol. 43, pp. 99â€“111, 2015. [40] Y . Jin, Q. Dou, H. Chen, L. Yu, J. Qin, C.-W. Fu, and P.-A. Heng, â€œSv-rcnet: workflow recognition from surgical videos using recurrent convolutional network,â€IEEE transactions on medical imaging, vol. 37, no. 5, pp. 1114â€“1126, 2017. [41] Y . Jin, Y . Long, C. Chen, Z. Zhao, Q. Dou, and P.-A. Heng, â€œTemporal memory relation network for workflow recognition from surgical video,â€ IEEE Transactions on Medical Imaging, vol. 40, no. 7, pp. 1911â€“1923, 2021. [42] T. Czempiel, M. Paschali, M. Keicher, W. Simson, H. Feussner, S. T. Kim, and N. Navab, â€œTecno: Surgical phase recognition with multi- stage temporal convolutional networks,â€ inMedical Image Computing and Computer Assisted Interventionâ€“MICCAI 2020: 23rd International Conference, Lima, Peru, October 4â€“8, 2020, Proceedings, Part III 23. Springer, 2020, pp. 343â€“352. [43] X. Gao, Y . Jin, Y . Long, Q. Dou, and P.-A. Heng, â€œTrans-svnet: Accurate phase recognition from surgical videos via hybrid embedding aggregation transformer,â€ inMedical Image Computing and Computer Assisted Interventionâ€“MICCAI 2021: 24th International Conference, Strasbourg, France, September 27â€“October 1, 2021, Proceedings, Part IV 24.",
    "2020, pp. 343â€“352. [43] X. Gao, Y . Jin, Y . Long, Q. Dou, and P.-A. Heng, â€œTrans-svnet: Accurate phase recognition from surgical videos via hybrid embedding aggregation transformer,â€ inMedical Image Computing and Computer Assisted Interventionâ€“MICCAI 2021: 24th International Conference, Strasbourg, France, September 27â€“October 1, 2021, Proceedings, Part IV 24. Springer, 2021, pp. 593â€“603. [44] R. Girdhar and K. Grauman, â€œAnticipative video transformer,â€ inPro- ceedings of the IEEE/CVF international conference on computer vision, 2021, pp. 13 505â€“13 515."
  ]
}