{
  "filename": "2509.10671v1.pdf",
  "total_chunks": 12,
  "text_length": 37449,
  "chunks": [
    "A Linear Programming Framework for Optimal Event-Triggered LQG Control Zahra Hashemi and Dipankar Maity Abstract—This paper explores intelligent scheduling of sensor- to-controller communication in networked control systems, par- ticularly when data transmission incurs a cost. While the optimal controller in a standard linear quadratic Gaussian (LQG) setup can be computed analytically, determining the optimal times to transmit sensor data remains computationally and analyt- ically challenging. We show that, through reformulation and the introduction of auxiliary binary variables, the scheduling problem can be cast as a computationally efficient mixed-integer linear program (MILP). This formulation not only simplifies the analysis but also reveals structural insights and provides clear decision criteria at each step. Embedding the approach within a model predictive control (MPC) framework enables dynamic adaptation, and we prove that the resulting scheduler performs at least as well as any deterministic strategy (e.g., periodic strat- egy). Simulation results further demonstrate that our method consistently outperforms traditional periodic scheduling. Index Terms—Networked Control Systems, Event-Triggered Control, Mixed-Integer Linear Programming, Model Predictive Control, Communication Scheduling. I. INTRODUCTION Networked control systems (NCS) play a vital role in today’s cyber–physical systems, where sensors, actuators, and controllers interact through shared communication networks. This architecture offers significant advantages in terms of flexibility and scalability. However, it also presents key chal- lenges, particularly due to limited communication resources. Continuously transmitting sensor data at every time step can be inefficient or even impractical, especially under energy [1]– [3], bandwidth [4]–[6], and latency constraints [7], [8]. A substantial body of research has explored event-triggered control (ETC) as a way to balance control performance with communication efficiency [9]. Early approaches focused on deterministic triggering rules that guarantee stability by trans- mitting whenever a predefined state or error threshold is exceeded [10]–[13]. More recent work has shifted toward stochastic and optimization-based methods, where transmis- sion policies are co-designed with control laws to mini- mize objective functions that explicitly penalize communica- tion [14]–[19]. Even with recent progress, there is still no widely ap- plicable and computationally efficient algorithm for tackling the scheduling problem, especially under the stochastic error dynamics. The main challenge lies in the bilinear nature of This research is supported by the National Science Foundation CAREER Award 2443349. The authors are with the Department of Electrical and Computer Engi- neering at the University of North Carolina at Charlotte, NC, USA, 28223. (e-mails:{zahrahashemi1, dmaity}@charlotte.edu).the error covariance recursion, combined with a decentralized setup where the scheduler and controller have access to different information. This mismatch makes classical dynamic programming techniques ineffective and results in a complex, nonconvex stochastic optimization problem with high dimen- sionality [14]. Several alternative approaches have been explored. Among these,value-of-information(V oI) based approaches attempt to quantify the benefit of each transmission, but depend on dynamic programming in continuous state-space, which quickly becomes impractical as system size grows [20], [21]. In continuous-time settings, partial differential equations and Hamilton-Jacobi-Bellman based formulations have been pro- posed [22], but solving these equations is computationally intensive, even for relatively small systems. These limitations highlight the need for reformulations that retain optimality while offering better scalability. A recent study has",
    "system size grows [20], [21]. In continuous-time settings, partial differential equations and Hamilton-Jacobi-Bellman based formulations have been pro- posed [22], but solving these equations is computationally intensive, even for relatively small systems. These limitations highlight the need for reformulations that retain optimality while offering better scalability. A recent study has also considered deep reinforcement learning based methods to overcome this analytical and computational intractability [23]. To tackle these issues, we propose a scheduler-centric reformulation that retains optimality while enabling efficient computation. The key idea is to eliminate matrix-valued de- cision variables arising from the covariance recursion and instead represent the nonconvex switching behavior using auxiliary binary monomials. This leads to an equivalent mixed- integer linear program (MILP) with a linear objective and linear constraints—making it solvable with modern integer programming tools. This paper makes two primary contributions. First, we present an exact reformulation of the stochastic schedul- ing problem with bilinear error dynamics. Building on the certainty-equivalent reduction introduced by Molinet. al[14], [15], we show that the problem can be transformed into a de- terministic mixed-integer nonlinear program (MINP). By un- folding the covariance recursion, we eliminate matrix-valued decision variables, reducing the problem to an unconstrained MINP that depends solely on binary scheduling decisions. Through the introduction of binary monomials and the applica- tion of McCormick relaxation, we construct a MILP that is ex- actly equivalent to the original formulation. This reformulation enables efficient computation. Second, we develop a real-time method for transmission decisions using one-step send/skip certificates. By comparing local and future Gramians, the certificate determines whether sending or skipping is optimal at each step, often allowing the scheduler to avoid solving the MILP. This mechanism integrates efficiently into a model predictive control (MPC) framework and enables fast, adaptivearXiv:2509.10671v1 [eess.SY] 12 Sep 2025 scheduling. We prove that the resultingMPC-based scheduler strictly outperforms all deterministic causal policies, including periodic strategies. In numerical simulations across varying system sizes, our MILP reformulation achieves speedups of up to five orders of magnitude—reducing optimization time by factors of105compared to solving the original MINP directly. Moreover, in a double-integrator case study, we demonstrate that theMPCscheduler consistently achieves lower control cost with fewer transmissions than baseline methods. The paper is organized as follows. We describe the prob- lem formulation in Section II and the optimal controller in Section III. The event-triggered communication policy and the linear programming reduction is presented in IV. The efficiency and optimality certificates of the proposed method are addressed in Section V. Section VI reports the simulation results and we conclude the work in Section VIII. Notation:LetRandN 0denote the set of real numbers and non-negative integers, respectively. All vectors and matrices are real valued with compatible dimensions. For a symmetric matrixQ, define the weighted norm∥x∥2 Q:=x⊤Qx. We use the Loewner order:A⪯BmeansB−A⪰0; in particular Q⪰0denotes positive semidefinite matrices. The operator tr(·)denotes the trace. Expectation and probability are denoted byE[·]andPr(·), respectively. Unless stated otherwise, expec- tations are taken with respect to the initial statex 0and the process-noise sequence{w k}k∈N 0. Conditional expectations use the controller’s information set, written asE[· | Z k]. II. PROBLEMDESCRIPTION Let us",
    "positive semidefinite matrices. The operator tr(·)denotes the trace. Expectation and probability are denoted byE[·]andPr(·), respectively. Unless stated otherwise, expec- tations are taken with respect to the initial statex 0and the process-noise sequence{w k}k∈N 0. Conditional expectations use the controller’s information set, written asE[· | Z k]. II. PROBLEMDESCRIPTION Let us consider a discrete-time stochastic networked control system following the dynamics: xk+1=Ax k+Bu k+wk,(1) zk=( xk θk= 1, ∅θ k= 0,(2) whereA∈Rn×nandB∈Rn×mare constant system matrices. The state vectorx k∈Rnand control inputu k∈Rm are updated at each discrete timestepk. In (2), the variablez k denotes the measurementreceivedby the controller at time k, which depends on the communication decision variable θk∈ {0,1}. The sensor measurement isx k, which may or may not be transmitted, depending on the communication decision. This formulation enables event-triggered communi- cation, where the sensor transmits measurements selectively, conditioned on the occurrence of a triggering event. The initial statex 0is modeled as a random variable with finite mean and covariance. The process noisew k∈Rnis an independent and identically distributed (i.i.d.) sequence of zero-mean random variables with finite covarianceΣw. Additionally,x 0and the noise variablesw kare assumed to be statistically independent for everyk. To formalize the controller’s access to measurement data over time, we introduce theinformation setavailable to the controller at each time step. We assume that the con- troller also remembers its past control actions. LetZ k= {z0:k, θ0:k, u0:k−1}denote the information available to theInformation Set Controller AgentScheduler NetworkZk uk xkθk zk Fig. 1:System architecture illustrating the interaction among the agent, scheduler, network, and controller. At each time stepk, the scheduler selects a binary transmission decisionθ k. Based on this decision, the corresponding measurementz k∈ {x k,∅}is transmitted to the controller which computes the inputu k. controller at time stepk≥1, with the initial information set given byZ 0={z 0, θ0}. At the next timestep, the information set is updated as: Zk+1=Z k∪ {z k+1, θk+1, uk}.(3) We consider the system’s evolution over a finite horizon of Tsteps. Fig. 1 depicts the overall system architecture, which consists of the following components: •Scheduler:Observes the statex kat each discrete time step k, and makes a binary decisionθ k∈ {0,1}indicating whether the controller should receive the current measure- ment via the resource limited network. •Network:Transmits the packet{z k, θk}from the aggregator to the controller. For simplicity, we assume an ideal network with no delays, quantization, or packet loss. •Information Set Module:Maintains and updates the in- formation setZ kat the controller using the latest received packet. This ensures that the controller always has access to all past transmissions. •Controller:Based on the information setZ k, the controller computes the control inputu k. •Agent:Applies the control inputu kto the plant and generates the new statex k+1according to dynamics (1). The primary objective is to jointly design a control policy µand a communication scheduling policyΘ =θ 0:T−1 to minimize the expected cost functional: J(µ,Θ) =E\"T−1X k=0\u0000 ∥xk∥2 Q+∥u k∥2 R+λθ k\u0001 +∥x T∥2 QT# ,(4) where the expectationE[·]is taken with respect to the stochas- ticity in the initial state, process noise, and—if randomized policies are considered—the scheduling and control decisions. Here,Q⪰0andQ",
    "a communication scheduling policyΘ =θ 0:T−1 to minimize the expected cost functional: J(µ,Θ) =E\"T−1X k=0\u0000 ∥xk∥2 Q+∥u k∥2 R+λθ k\u0001 +∥x T∥2 QT# ,(4) where the expectationE[·]is taken with respect to the stochas- ticity in the initial state, process noise, and—if randomized policies are considered—the scheduling and control decisions. Here,Q⪰0andQ T⪰0are state weighting matrices,R≻0 is the control weighting matrix, andλ >0is a designer- specified regularization parameter that penalizes communi- cation. The termPT−1 k=0θkquantifies the total number of transmissions, directly impacting network usage. Therefore, the cost function (4) captures a trade-off between control performance and communication effort, accumulated over the horizon[0, T]. Formally, we aim to perform the following joint optimiza- tion over the control and communication variables min µ,ΘJ(µ,Θ). III. OPTIMALCONTROLLER Solving the joint optimization problem of control law and event-triggering schedule in (4) is a central challenge in event- triggered control. The fact that the scheduler and controller operate with different information sets introduces a decen- tralized information structure. As a result, classical dynamic programming techniques are not directly applicable, prompting the development of alternative solution methods. This chal- lenge—optimizing control under limited and decentralized in- formation—has been widely studied, with several formulations and solution strategies proposed in the literature. Recent work shows that the problem can sometimes be decomposed into simpler subproblems, making it more tractable. Notably, some parts of the formulation resemble the standard LQR problem, which can be efficiently solved using the Riccati equation. We begin by examining the event-triggered control consid- erted by Molinet al.[14], [15], which proved that the optimal control policy in this discrete-time setting has the form: uk=−L kE[xk| Zk], k∈ {0, . . . , T−1},(5) where recall thatZ kdenotes the information set available to the controller up to timek. The control gainL kand the associated Riccati recursion are given by: Lk=S−1 kB⊤Pk+1A,(6a) Sk=R+B⊤Pk+1B,(6b) Pk=A⊤Pk+1A+Q−A⊤Pk+1BS−1 kB⊤Pk+1A,(6c) with terminal conditionP T=Q T, and whereP k∈Rn×nis nonnegative definite for allk∈ {0, . . . , T}. By defining,ˆx k:= E[xk| Zk]we may verify [15] ˆxk=( xk θk= 1, Aˆxk−1+Bu k−1 θk= 0. As further shown in [15], substituting the optimal control law (5) into the cost functional reduces the problem to minimizing a new cost that depends solely on the transmission decision vectorΘ: J(µ∗,Θ) =J const+E\u0014T−1X k=0∥ek∥2 L⊤ kSkLk\u0015 +λE\"T−1X k=0θk# ,(7) wheree k=xk−E[x k| Zk]denotes the state estimation error at timek. Here,J const:=E\u0002 x⊤ 0P0x0\u0003 +PN−1 k=0E\u0002 w⊤ kPk+1wk\u0003 denotes the part of the cost that depends only on the initial statex 0and the process noise sequence, and is therefore independent of the transmission vectorΘ[15]. Following the analysis in [14, Theorem 1], the error sequence{e k}k∈N 0 evolves recursively as: ek+1=¯θk+1(Aek+wk),(8)with initial conditione −1= 0, where ¯θk+1:= 1−θ k+1. That is, the estimation error resets to zero whenever a fresh measurement is received (θ k+1= 1), and otherwise propagates forward under the system dynamics with additive process noise. This formulation serves as the foundation for the event-triggered estimator used in (7). For further details and theoretical foundations, we refer the reader to [14]. IV. OPTIMALCOMMUNICATIONPROTOCOL In this section, we investigate the optimal scheduling policy.",
    "k+1= 1), and otherwise propagates forward under the system dynamics with additive process noise. This formulation serves as the foundation for the event-triggered estimator used in (7). For further details and theoretical foundations, we refer the reader to [14]. IV. OPTIMALCOMMUNICATIONPROTOCOL In this section, we investigate the optimal scheduling policy. To this end, we define the error on the scheduler’s side at time kas: ˜ek:=x k−Aˆx k−1−Bu k−1=Ae k−1+wk−1 ˜e0:=x 0−E[x 0]. One may verify that the estimation error at the controller side followse k=βk˜ekfor allk. At any givenk, the scheduler optimizes for the remaining horizon: min βk:(T−1)E\"T−1X t=k∥et∥2 Γt+λ(1−β t)# subject toe t+1=βt+1(Aet+wt)(9) ek=βk˜ek. Here,Γ t:=L⊤ tStLtdenotes the weighting matrix appearing in the cost functional (7). Notice that the scheduler error˜e k affects the scheduling policy for the remaining horizon[k,(T− 1)]. Given˜e k, we find the optimal schedulingΘ∗ k(˜ek) :={β∗ k|k, β∗ k+1|k , . . . , β∗ T−1|k}and apply the first one, i.e.,β∗ k|k, and then repeat the same process at timek+ 1with the new realized error˜e k+1. To develop an efficient algorithm for solving the stochastic optimization (9), we proceed as follows. For any given deci- sion roll-outΘ k={β k|k, . . . , β T−1|k}, we define ¯J(Θ k, k|˜ek) to be the objective value of the optimization (9). That is, ¯J(Θ k, k|˜ek) =T−1X t=ktr(Γ tΣt) +λ(1−β t|k),(10) whereΣ t=E[e te⊤ t|Θk,˜ek]for allt=k, . . . ,(T−1). Consequently, Σt+1=E[e t+1e⊤ t+1|Θk,˜ek] =E[β2 t+1|k(Aet+wt)(Ae t+wt)⊤|Θk,˜ek] (∗)=βt+1|kE[Ae te⊤ tA⊤+Ae tw⊤ t+wte⊤ tA⊤+wtw⊤ t|Θk,˜ek] (†)=βt+1|k(AΣ tA⊤+ Σw), where(∗)follows from the fact thatβ2 t+1|k =β t+1|k and (†)follows from the definition ofΣ tand the fact thatw t is independent ofes kand the choice ofΘ kfor allt. We have also defined and usedE[w tw⊤ t] := Σw. Thus, the original optimization under stochastic dynamics can be reformulated as a deterministicmixed-integer nonlinear programwith bi- linear matrix equality constraints: Matrix Recursion Form min Θk,Σk:T−1¯J(Θ, k|es k) subject toΣ t+1=βt+1|k(AΣ tA⊤+ Σw), ∀t=k, . . . , T−2, Σk=βk|kes k(es k)⊤.(11) Even though the objective function is linear (both inΣ k:T−1 andβ k:T−1 ), the bi-linear matrix equalities pose a significant challenge in solving the optimization. These matrix variables also increase the dimensionality of the optimization problem— (T−1−k)(n(n−1) 2+1)variables for thek-th timestep optimiza- tion. The major bottleneck (i.e., the(T−1−k)n(n−1) 2part) comes from the matrix variables, which scales quadratically with the dimension of the state spacen. A. Mixed Integer Linear Program Reformulation To overcome the mentioned computational challenge, we re- formulate this optimization problem by completely eliminating the matrix variables. Unfolding the bi-linear matrix dynamics yields the following closed-form expression: Σt= tY s=k¯θs|k! At−kΣs kAt−k⊤ +tX τ=k+1 tY s=τ¯θs|k! At−τΣwAt−τ⊤.(12) Let us further define Gt,τ:=( At−kΣs kAt−k⊤, τ=k, At−τΣwAt−τ⊤, τ≥k+ 1,(13) gt,τ:= tr\u0000 ΓtGt,τ\u0001 .(14) With these definitions, the closed-form covariance expression in (12) can be rewritten as the scalar-weighted expansion: Σt=tX τ=k\u0010tY s=τ¯θs|k\u0011 Gt,τ, and the instantaneous stage cost simplifies to tr(Γ tΣt) =tX τ=k\u0010tY s=τ¯θs|k\u0011 gt,m, where all coefficientsg t,τcan be precomputedoffline. Upon substituting (12) into the objective function, we obtain an unconstrained mixed-integer nonlinearoptimization. Unconstrained Reformulation min ¯θk:T−1T−1X t=ktX τ=k\u0010tY s=k¯θs|k\u0011 gt,τ+λT−1X",
    "be rewritten as the scalar-weighted expansion: Σt=tX τ=k\u0010tY s=τ¯θs|k\u0011 Gt,τ, and the instantaneous stage cost simplifies to tr(Γ tΣt) =tX τ=k\u0010tY s=τ¯θs|k\u0011 gt,m, where all coefficientsg t,τcan be precomputedoffline. Upon substituting (12) into the objective function, we obtain an unconstrained mixed-integer nonlinearoptimization. Unconstrained Reformulation min ¯θk:T−1T−1X t=ktX τ=k\u0010tY s=k¯θs|k\u0011 gt,τ+λT−1X t=k(1− ¯θt|k) (15) Although we have reduced the dimension of the optimization problem to(T−k−1), independent of the state dimension n, the formulation in (15) remains a MINP, which is still challenging to solve in practice. To overcome this difficulty,we further reformulate the problem as a MILP without intro- ducing any suboptimality. We begin by defining unified binary monomials that capture the switching effects of the scheduling decisions: µt,τ:=tY s=τ¯θs|k, τ=k, . . . , t;t=k, . . . , T−1.(16) To enforce the binary product definition in (16), we impose the following linear inequalities for alls∈ {τ, . . . , t}: tX s=τ¯θs|k−(t−τ)≤µ t,τ≤¯θs|k, (17) These constraints guarantee thatµ t,τ= 1if and only if ¯θs|k= 1for alls∈[τ, t]; otherwiseµ t,τ= 0. This con- struction is a special case of the McCormick relaxation [24], which reduces to anexactformulation when applied to binary variables. Consequently, the nonlinear product in (16) can be replaced with its linear counterpart, leading to the following MILP reformulation of the objective: MILP Reformulation min ¯θk:T−1 ,{µt,τ}T−1X t=ktX τ=kµt,τgt,τ+λT−1X t=k\u0000 1−¯θt|k\u0001 s.t. ¯θt|k∈ {0,1}, µ t,τ∈ {0,1}, µt,τ≤¯θs|k,∀s∈ {τ, . . . , t}, µt,τ≥tX s=τ¯θs|k−\u0000 t−τ\u0001 ,(18) The entire control-communication joint optimization process is presented in Algorithm 1. While the MILP formulation (18) introduces auxiliary bi- nariesµ t,τ(fork≤τ≤t≤T−1) and additional constraints from the (exact) McCormick linearization, this modest size increase is outweighed by a key benefit: products of binaries are replaced by an equivalent linear representation. Consequently, the unconstrained MINP in (15) becomes a standard MILP with a linear objective and linear constraints. This conversion enables the use of modern MILP solvers (e.g., GUROBI), which are far more efficient than general MINP solvers. Figure 2 compares the average runtimes of the three formulations: MINP (Matrix Recursion Form) and MILP were solved with GUROBI, while the MINP (Unconstrained Reformulation) was solved with a nonlinear branch-and-bound solver (fminbnb). The Matrix Recursion MINP scales expo- nentially, and the Unconstrained Reformulation suffers from solver inefficiency, whereas the MILP achieves nearly constant solution times. Acrossn= 2–35, the reformulation yields speedups of105–106times, verified over multiple trials on a 3.60 GHz Intel Core i7-12700K CPU (12 cores, 32 GB RAM). V. EFFICIENCY ANDOPTIMALITY OFMPC In this section, we present two main technical results. First, we show that the computational complexity of theMPC Algorithm 1OPTIMALEVENT-TRIGGEREDLQG 1:Initialize:system matricesA, B; cost weightsQ, Q T, R; noise covarianceΣw; penaltyλ; horizonT 2:Compute control gainsL kand weighting matricesΓ kby solving the Riccati recursion fork= 0, . . . , T−1 3:Precompute kernelsG t,τand scalarsg t,τ= tr(Γ tGt,τ)for all0≤τ≤t≤T−1 4:Initialize:Σs 0= (x 0−E[x 0])(x0−E[x 0])⊤ 5:fork= 0toT−1do 6:setdecided←false 7:ifes⊤ kΓkes k≥λthen▷send is weakly optimal 8: ¯θk|k←0;decided←true 9:else ifes⊤ kWkes k≤λthen▷skip is weakly optimal 10: ¯θk|k←1;decided←true 11:end if 12:ifdecided=false then 13:Solve the optimization problem MILP (18) to 14:obtain{ ¯θt|k}T−1 t=k 15:end if 16:if",
    "kernelsG t,τand scalarsg t,τ= tr(Γ tGt,τ)for all0≤τ≤t≤T−1 4:Initialize:Σs 0= (x 0−E[x 0])(x0−E[x 0])⊤ 5:fork= 0toT−1do 6:setdecided←false 7:ifes⊤ kΓkes k≥λthen▷send is weakly optimal 8: ¯θk|k←0;decided←true 9:else ifes⊤ kWkes k≤λthen▷skip is weakly optimal 10: ¯θk|k←1;decided←true 11:end if 12:ifdecided=false then 13:Solve the optimization problem MILP (18) to 14:obtain{ ¯θt|k}T−1 t=k 15:end if 16:if ¯θk|k= 0then 17:Transmitx kand reset estimation error:e k←0 18:Update:E[x k| Zk]←x k 19:else 20:Predict state estimate: 21:E[x k| Zk]←AE[x k−1| Zk−1] +B u k−1 22:end if 23:Apply control input:u k← −L kE[xk| Zk] 24:Measurex kand update covariance: 25:Σs k←(x k−E[x k| Zk])(xk−E[x k| Zk])⊤ 26:end for 0510152025303510-2100102104 MINP-Recursion MINP-Reformulation MILP Fig. 2:Average optimization time (seconds) vs. problem sizenon a logarithmic scale for horizon lengthT= 9. approach can be further reduced by deriving a threshold-based policy to determine whether the optimization at timestepk needs to be solved in the first place. Second, we show that our proposed method outperforms any deterministic policy, including widely used periodic policies. Theorem 1(One–Step Optimality Certificates).Consider(1), the error dynamics(8), and the cost(7)withΓ k:=L⊤ kSkLk⪰ 0. Fork∈ {0, . . . , T−1}define the tail Gramian Wk:=T−1−kX j=0(Aj)⊤Γk+jAj. LetBen k(es k)be the expected cost reduction obtained by sending at timek(i.e., skip minus send). Thenes⊤ kΓkes k−λ≤Ben k(es k)≤es⊤ kWkes k−λ.(19) Consequently, ifes⊤ kΓkes k≥λthen send is optimal, and if es⊤ kWkes k≤λthen skip is optimal. Both bounds are tight. Proof.LetJsend k(resp.Jskip k) be the optimal cost-to-go from timekwhen the action atkis send (θ k= 1) (resp. skip, θk= 0), and define ∆k:=Jskip k−Jsend k= Ben k(es k). Stagek.Under send,e k= 0and the stage-kcost isλ; under skip,e k=es kand the stage-kcost ises⊤ kΓkes k. Therefore, ∆k=es⊤ kΓkes k−λ+ ∆tail k, where wedefinethe tail increment from the initial error by ∆tail k:=E\"T−1−kX j=1e⊤ k+jΓk+jek+j ek=es k# −E\"T−1−kX j=1e⊤ k+jΓk+jek+j ek= 0# . This subtraction removes the pure-noise contribution (indepen- dent ofes k). Unrolling the error dynamicse t+1=¯θt+1(Aet+ wt).we obtain, forj≥1, ek+j=µk+j,k+1 Ajes k+ηk+j, ηk+j=j−1X r=0µk+j, k+1+r Aj−1−rwk+r. Hereη k+j is a linear combination of{w k, . . . , w k+j−1}, henceE[η k+j] = 0; moreoveres kis independent ofη k+j. E\u0002 e⊤ k+jΓk+jek+j ek=es k\u0003 =µk+j,k+1 es⊤ k(Aj)⊤Γk+jAjes k +E\u0002 η⊤ k+jΓk+jηk+j\u0003 . Subtracting thee k= 0expression removes the second term and gives ∆tail k=T−1−kX j=1µk+j,k+1 es⊤ k(Aj)⊤Γk+jAjes k. Since for any deterministic causal schedule µk+j,k+1 =Qk+j s=k+1¯θs∈ {0,1}, 0≤∆tail k≤T−1−kX j=1es⊤ k(Aj)⊤Γk+jAjes k=es⊤ k(Wk−Γk)es k. The upper bound isattainedby the “never send again” schedule (µ k+j,k+1 = 1for allj≥1), while the lower bound is attainedby a schedule that sends at timek+1(soµ k+j,k+1 = 0 for allj). Thus, the stage-kcontribution yields es⊤ kΓkes k−λ≤∆ k≤es⊤ kWkes k−λ, which is (19). SinceBen k=Jskip k−Jsend k, ifes⊤ kΓkes k−λ≥0then Benk≥0andsendis (weakly) optimal;1ifes⊤ kWkes k−λ≤0 1Weakly optimal means the action is not worse than the alternative, even if both yield the same cost. thenBen k≤0andskipis (weakly) optimal. Consequently, we need to solve the MILP at timekonly in the indeterminate regime. es⊤ kΓkes k≤λ≤es⊤ kWkes k Theorem 2(MPCdominates deterministic schedules).Con- sider(1)–(8)with stage costℓ(e k, θk) =e⊤ kΓkek+λθ k. Let Θdetdenote any fixed deterministic schedule. LetΘMPCbe the receding-horizon policy that, at each timek, solves its MILP",
    "k≤0andskipis (weakly) optimal. Consequently, we need to solve the MILP at timekonly in the indeterminate regime. es⊤ kΓkes k≤λ≤es⊤ kWkes k Theorem 2(MPCdominates deterministic schedules).Con- sider(1)–(8)with stage costℓ(e k, θk) =e⊤ kΓkek+λθ k. Let Θdetdenote any fixed deterministic schedule. LetΘMPCbe the receding-horizon policy that, at each timek, solves its MILP subproblem to optimality using the current covariance and appliesθ∗ k|k. Then, for anyΘdet, J(ΘMPC)≤J(Θdet). Proof.LetI kbe the data available to the scheduler at time k. For eachk, the receding-horizon actionθ∗ k|kis defined as the optimizer of the MILP subproblem at timekconditioned onI k. Likewise,θ∗ k|tdenotes the action for timekplanned at timet≤k(under the subproblem solved at timet). We prove the result in two steps. Step 1 (MPC≤open-loop plan). We will show E\"T−1X k=0ℓ\u0000 ek, θ∗ k|k\u0001# | {z } Expected performance under MPC≤E\"T−1X k=0ℓ\u0000 ek, θ∗ k|0\u0001# | {z } expected performance of open-loop plan computed at timet= 0.(20) The proof is by backward induction, using conditional op- timality at each time and the tower property of conditional expectation.Base case (k=T−1).By optimality of the receding-horizon decision at timeT−1, ℓ\u0000 eT−1, θ∗ T−1|T−1\u0001 ≤ℓ\u0000 eT−1, θ∗ T−1|T−2\u0001 for alles T−1 a.s. The equality holds only if the decision computed at timeT−2 (i.e.,θ∗ T−1|T−2) is still optimal at timeT−1. Taking conditional expectations givenI T−2 and using monotonicity of conditional expectation, E\u0002 ℓ\u0000 eT−1, θ∗ T−1|T−1\u0001 IT−2\u0003 ≤E\u0002 ℓ\u0000 eT−1, θ∗ T−1|T−2\u0001 IT−2\u0003 , Applying the tower property yields: E\u0002 ℓ\u0000 eT−1, θ∗ T−1|T−1\u0001\u0003 ≤E\u0002 ℓ\u0000 eT−1, θ∗ T−1|T−2\u0001\u0003 . Inductive step (fromj+ 1down toj).Fixj∈ {0, . . . , T−2} and assume as induction hypothesis that the inequality holds fromj+ 1onward. E T−1X k=j+1ℓ\u0000 ek, θ∗ k|k\u0001 ≤E T−1X k=j+1ℓ\u0000 ek, θ∗ k|j\u0001 . By construction of the receding-horizon step, the pair\u0010 θ∗ j|j,{θ∗ k|j}T−1 k=j+1\u0011 is (by definition) optimal for the one-step decision atjwhen the future plan{θ∗ k|j}k>jis taken as fixed. Hence, almost surely,ℓ\u0000 ej, θ∗ j|j\u0001 +E T−1X k=j+1ℓ(ek, θ∗ k|j) Ij  ≤ℓ\u0000 ej, θ∗ j|j−1\u0001 +E T−1X k=j+1ℓ(ek, θ∗ k|j−1) Ij . Taking unconditional expectations and applying the tower property to both conditional terms gives: E\u0002 ℓ\u0000 ej, θ∗ j|j\u0001\u0003 +E T−1X k=j+1ℓ(ek, θ∗ k|j)  ≤E\u0002 ℓ\u0000 ej, θ∗ j|j−1\u0001\u0003 +E T−1X k=j+1ℓ(ek, θ∗ k|j−1) . Combining this with the induction hypothesis yields: E T−1X k=jℓ\u0000 ek, θ∗ k|k\u0001 ≤E T−1X k=jℓ\u0000 ek, θ∗ k|j−1\u0001 . Pluggingj= 1and then addingEh ℓ\u0000 e0, θ∗ 0|0\u0001i to both sides yields (20). Step 2 (open-loop plan⪯any deterministic schedule). By definition, the open-loop sequence{θ∗ k|0}T−1 k=0minimizes EhPT−1 k=0ℓ\u0000 ek, θk\u0001 I0i over all admissible deterministic sched- ules fixed at time0. Any deterministic scheduleΘdet= {θdet k|0}T−1 k=0is feasible for this problem; therefore, E\"T−1X k=0ℓ\u0000 ek, θ∗ k|0\u0001# ≤E\"T−1X k=0ℓ\u0000 ek, θdet k|0\u0001# . Together, these two steps establish the theorem. Step 1 of the proof shows that, in expectation, the receding- horizonMPCstrategy is at least as good as the offline open- loop plan (the full-horizon MILP solved once at time0). In fact, the open-loop plan can produce the same performance as the MPConly ifthe open-loop plan is optimal at every sub-optimization problem (i.e., the optimization",
    "shows that, in expectation, the receding- horizonMPCstrategy is at least as good as the offline open- loop plan (the full-horizon MILP solved once at time0). In fact, the open-loop plan can produce the same performance as the MPConly ifthe open-loop plan is optimal at every sub-optimization problem (i.e., the optimization from stage konward). If there exists a timeksuch that the solution to the MPC problem is unique andθ∗ k|k̸=θ∗ k|k−1, the MPC outperforms the openloop policy. Step 2 shows that, in expectation, the offline open-loop plan is at least as good as any deterministic fixed schedule. VI. CASESTUDY: DOUBLEINTEGRATOR We evaluate the proposed MILP-based event-triggered scheduling on a double integrator, modeling a robot moving along a line with position–velocity dynamics: xk+1=\u00141T s 0 1\u0015 xk+\" T2 s 2 Ts# uk+wk, wherex k= [p kvk]⊤is the state (position and velocity),u k is the control input (acceleration), andw kis zero-mean noise 0 5 10 15 20 2501(a) Offline transmission schedule. 0 5 10 15 20 2501 (b) MPC transmission schedule. Fig. 3:Transmission schedules under offline vs. MPC strategies for a representative disturbance realization. with covarianceΣw. The sampling period is set toT s= 0.1s. Simulation parameters are: T= 25,Σw= 0.5I, λ= 100. 1)Open-Loop Prediction:No transmissions; the controller relies solely on open-loop predictions. 2)Continuous Communication:State transmitted at every step, i.e.,θ k= 1for allk(ideal baseline). 3)Offline Schedule:Fixed transmission schedule computed once from the full-horizon MILP. 4)MPCScheduler:Receding-horizon MILP solved at each step using the current error covariance. This comparison illustrates the trade-off between communi- cation and control performance across baseline, idealized, and optimized strategies. Figure 3 compares the transmission schedules of the offline andMPCstrategies for a representative disturbance realization. The offline method follows a fixed pattern, transmitting at reg- ular intervals regardless of system behavior or estimation error. In contrast, theMPCstrategy adapts to real-time uncertainty, triggering transmissions only when necessary. This adaptivity enables MPC to substantially reduce communication without sacrificing performance. Figure 4 shows the estimation error trajectories under the three scheduling strategies. Both offline andMPCreset the error to zero upon transmission. The offline policy follows a fixed schedule, making it insensitive to disturbance realiza- tions, whereas theMPCpolicy adapts to real-time uncertainty and transmits only when needed. As a result,MPCachieves error bounds comparable to the offline schedule in bothx(1) andx(2)while using fewer transmissions. By contrast, full communication removes estimation error but requires trans- mission at every step. The efficiency ofMPCin balancing communication and control/estimation is illustrated in Figure 5, which reports contours of theaveragenumber of transmissions as a function of the communication penaltyλand the noise covariance scaleσ(withΣw=σI). Both approaches depend onσ because the offline schedule is designed usingΣw; however, the offline policy is open-loop—once(λ, σ)are fixed it yields a static transmission pattern that does not react to the particular noiserealizationalong a trajectory. In contrast,MPCre- optimizes online using the realized estimation error/innovation and therefore adapts its transmission frequency to both cost 0 5 10 15 20-3-2.5-2-1.5-1-0.500.511.5 MPC(a)x(1)estimation error. 0 5 10 15 20-1.5-1-0.500.511.5 (b)x(2)estimation error. Fig. 4:Estimation error trajectories under offline vs. MPC scheduling. 1011021030.10.20.30.40.50.60.70.80.91 0510152025 (a) Offline optimization. 1011021030.10.20.30.40.50.60.70.80.91 0510152025 (b) MPC scheduling. Fig. 5:Average communication count under",
    "realized estimation error/innovation and therefore adapts its transmission frequency to both cost 0 5 10 15 20-3-2.5-2-1.5-1-0.500.511.5 MPC(a)x(1)estimation error. 0 5 10 15 20-1.5-1-0.500.511.5 (b)x(2)estimation error. Fig. 4:Estimation error trajectories under offline vs. MPC scheduling. 1011021030.10.20.30.40.50.60.70.80.91 0510152025 (a) Offline optimization. 1011021030.10.20.30.40.50.60.70.80.91 0510152025 (b) MPC scheduling. Fig. 5:Average communication count under offline vs. MPC scheduling across different values ofλand noise covariance. and uncertainty; this adaptive effect is most pronounced for low to moderate values ofλ, leading to more efficient use of communication. A similar pattern appears in the cost landscape of Figure 6. TheMPCstrategy consistently achieves lower total cost than the offline baseline, with the advantage most pronounced when communication is expensive and transmissions are limited. These results underscore the value of incorporating real-time information into both control and scheduling decisions. To assess robustness, we ran Monte Carlo simulations over 1000disturbance realizations. Table I summarizes the results. MPCachieves the lowest average cost of5694.34with only on average of4.46transmissions per run, about half that of offline (5893.16, 8.14). Full communication eliminates error but at the highest cost(6956.29, 25.0), while prediction-only minimizes transmissions(1.0)but incurs the largest cost(17860.73). Figure 7 compares control strategies in terms of the trade-off between communication and performance cost. The periodic baseline (dark blue points and fitted curve) shows that higher communication frequency improves performance. In contrast, both offline optimization (brown) andMPCscheduling (yel- low) achieve lower costs than the periodic baseline at com- parable or reduced communication levels. This demonstrates the advantage of optimized and adaptive scheduling over fixed periodic triggering in reducing communication while maintaining performance. Overall, theMPCapproach provides a scalable, efficient solution for resource-aware estimation and control, well suited to bandwidth- or energy-constrained systems. TABLE I: Average cost and communication count under different strategies. Strategy Cost Comm. Avg. Single Case Study Offline Schedule 3420.23 8.00 MPCScheduler 3046.57 5.00 Continuous Communication 5107.83 25.00 Open-Loop Prediction 5988.35 0.00 Monte Carlo Average (1000 Seeds) Offline Schedule 5893.16 8.14 MPCScheduler 5694.34 4.46 Continuous Communication 6956.29 25.00 Open-Loop Prediction 17860.73 0.00 1011021030.10.20.30.40.50.60.70.80.91 020004000600080001000012000140001600018000 (a) Offline optimization. 1011021030.10.20.30.40.50.60.70.80.91 020004000600080001000012000140001600018000 (b) MPC scheduling. Fig. 6:Average total cost under offline vs. MPC scheduling across different values ofλand noise covariance. VII. CONCLUSION We introduced a linear reformulation for event-triggered scheduling in stochastic networked control systems, converting a challenging nonlinear optimization problem into a tractable MILP. This reformulation uncovered structural insights into how estimation error, communication decisions, and control performance interact, and led to the development of intu- itive transmission certificates. Our numerical results show that the method dramatically reduces computational effort while achieving a better trade-off between control cost and communication usage compared to standard approaches. These results point to the potential of this framework as a scalable foundation for more complex applications, such as multi-agent coordination and large-scale networked systems. REFERENCES [1] W. Heemels, K. Johansson, and P. Tabuada, “An introduction to event- triggered and self-triggered control,” vol. 32, no. 5, pp. 50–65, 2012. 0 500 1000 1500 2000 25000.40.60.811.21.41.61.82104 Fig. 7:Comparison of control strategies with the base line periodic strategy[2] D. Maity and J. S. Baras, “Optimal event-triggered control of nonde- terministic linear systems,”IEEE Transactions on Automatic Control,",
    "“An introduction to event- triggered and self-triggered control,” vol. 32, no. 5, pp. 50–65, 2012. 0 500 1000 1500 2000 25000.40.60.811.21.41.61.82104 Fig. 7:Comparison of control strategies with the base line periodic strategy[2] D. Maity and J. S. Baras, “Optimal event-triggered control of nonde- terministic linear systems,”IEEE Transactions on Automatic Control, vol. 65, no. 2, pp. 604–619, 2019. [3] Z. Hashemi, A. Ramezani, and M. P. Moghaddam, “Energy hub manage- ment by using decentralized robust model predictive control,” in2016 4th international conference on control, instrumentation, and automation (ICCIA). IEEE, 2016, pp. 105–110. [4] M. Afshari, D. Maity, and P. Tsiotras, “Communication-and control- aware optimal quantizer selection for multi-agent control,”IEEE Control Systems Letters, vol. 8, pp. 2385–2390, 2024. [5] D. Maity and P. Tsiotras, “Optimal controller synthesis and dynamic quantizer switching for linear-quadratic-Gaussian systems,”IEEE Trans- actions on Automatic Control, vol. 67, no. 1, pp. 382–389, 2021. [6] V . Kostina and B. Hassibi, “Rate-cost tradeoffs in control,” vol. 63, no. 11, pp. 3390–3404, 2018. [7] D. Maity, M. H. Mamduhi, S. Hirche, K. H. Johansson, and J. S. Baras, “Optimal LQG control under delay-dependent costly information,”IEEE control systems letters, vol. 3, no. 1, pp. 102–107, 2018. [8] D. Maity, M. H. Mamduhi, S. Hirche, and K. H. Johansson, “Optimal LQG control of networked systems under traffic-correlated delay and dropout,”IEEE Control Systems Letters, vol. 6, pp. 1280–1285, 2021. [9] K. J. ˚Astr¨om, “Fundamental limitations of control system performance,” inCommunications, Computation, Control, and Signal Processing: a tribute to Thomas Kailath. Springer, 1997, pp. 355–363. [10] M. Rabi, G. V . Moustakides, and J. S. Baras, “Adaptive sampling for linear state estimation,”SIAM Journal on Control and Optimization, vol. 50, no. 2, pp. 672–702, 2012. [11] O. C. Imer and T. Bas ¸ar, “Optimal estimation with limited measure- ments,” vol. 2, no. 1-3, pp. 5–29, 2010. [12] G. M. Lipsa and N. C. Martins, “Remote state estimation with com- munication costs for first-order LTI systems,”IEEE Transactions on Automatic Control, vol. 56, no. 9, pp. 2013–2025, 2011. [13] D. Maity and J. S. Baras, “Event based control of stochastic linear systems,” in2015 International Conference on Event-based Control, Communication, and Signal Processing (EBCCSP). IEEE, 2015, pp. 1–8. [14] A. Molin and S. Hirche, “On LQG joint optimal scheduling and control under communication constraints,” inProceedings of the 48th IEEE CDC. IEEE, 2009, pp. 5832–5838. [15] ——, “On the optimality of certainty equivalence for event-triggered control systems,”IEEE Transactions on Automatic Control, vol. 58, no. 2, pp. 470–474, 2012. [16] D. Maity and J. S. Baras, “Minimal feedback optimal control of linear- quadratic-Gaussian systems: No communication is also a communica- tion,”IFAC-PapersOnLine, vol. 53, no. 2, pp. 2201–2207, 2020. [17] M. H. Mamduhi and D. Maity, “Network-aware optimal sampling for stochastic control systems over dynamic networks,”IEEE Control Systems Letters, vol. 9, pp. 1808–1813, 2025. [18] M. H. Mamduhi, D. Maity, K. H. Johansson, and J. Lygeros, “Regret- optimal cross-layer co-design in networked control systems—part I: General case,”IEEE Communications Letters, vol. 27, no. 11, pp. 2874– 2878, 2023. [19] D. Maity, M. H. Mamduhi, J. Lygeros, and K. H. Johansson, “Regret- optimal cross-layer co-design in",
    "[18] M. H. Mamduhi, D. Maity, K. H. Johansson, and J. Lygeros, “Regret- optimal cross-layer co-design in networked control systems—part I: General case,”IEEE Communications Letters, vol. 27, no. 11, pp. 2874– 2878, 2023. [19] D. Maity, M. H. Mamduhi, J. Lygeros, and K. H. Johansson, “Regret- optimal cross-layer co-design in networked control systems—part II: Gauss-Markov case,”IEEE Communications Letters, vol. 27, no. 11, pp. 2879–2883, 2023. [20] T. Soleymani, J. S. Baras, and S. Hirche, “Value of information in feedback control: Quantification,”IEEE Transactions on Automatic Control, vol. 67, no. 7, pp. 3730–3737, 2021. [21] T. Soleymani, J. S. Baras, S. Hirche, and K. H. Johansson, “Value of information in feedback control: Global optimality,”IEEE Transactions on Automatic Control, vol. 68, no. 6, pp. 3641–3647, 2022. [22] M. Thelander Andr ´en, “On lqg-optimal event-based sampling,” Ph.D. dissertation, Lund University, 2020. [23] S. Aggarwal, D. Maity, and T. Bas ¸ar, “Interq: A DQN framework for optimal intermittent control,”IEEE Control Systems Letters, vol. 9, pp. 607–612, 2025. [24] G. P. McCormick, “Computability of global solutions to factorable nonconvex programs: Part I—convex underestimating problems,”Math- ematical Programming, vol. 10, no. 1, pp. 147–175, 1976."
  ]
}