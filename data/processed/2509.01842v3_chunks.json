{
  "filename": "2509.01842v3.pdf",
  "total_chunks": 18,
  "text_length": 61849,
  "chunks": [
    "A PREPRINT GradES: Significantly Faster Training in Transformers with Grad ient-Based E arly S topping Qifu Wen1,†, *, Xi Zeng1,†, Zihan Zhou1, Shuaijun Liu2, Mehdi Hosseinzadeh3,4, Ningxin Su2, and Reza Rawassizadeh1,5 1Department of Computer Science, Boston University Metropolitan College 2Information Hub, The Hong Kong University of Science and Technology, Guangzhou 3School of Engineering and Technology, Duy Tan University, Da Nang, Vietnam 4Department of AI, School of Computer Science and Engineering, Galgotias University, Greater Noida, India 5Center of Excellence in Precision Medicine and Digital Health, Department of Physiology, Chulalongkorn University, Thailand †These authors contributed equally to this work *Corresponding author: qfwen@bu.edu, rezar@bu.edu, ningxinsu@hkust-gz.edu.cn ABSTRACT Early stopping monitors global validation loss and halts all parameter updates simultaneously, which is computationally costly for large transformers due to the extended time required for validation inference. We proposeGradES, a novel gradient-based early stopping approach that operates within transformer components (attention projections and Feed-Forward layer matrices). We found that different components converge at varying rates during fine-tuning for both language and vision-language models.GradES tracks the magnitude of gradient changes in backpropagation for these matrices during training. When a projection matrix’s magnitude of gradient changes fall below a convergence threshold τ, we exclude that projection matrix from further updates individually, eliminating costly validation passes while allowing slow converging matrices to continue learning.GradESspeeds up training time by 1.57–7.22 ×while simultaneously enhancing generalization through early prevention of overfitting, resulting in 1.2% higher average accuracy in language tasks and 3.88% on multimodal benchmarks. KeywordsTransformer, Vision Transformer, Early Stopping, Fine-tuning, Optimization, Large Language Models, Multimodal Learning 1 Introduction Large language models (LLMs) have remarkable capabilities across diverse tasks, but their training and deployment require substantial computational costs that scale with model size and inference frequency. Due to the high cost of training models with billions of parameters, any effort toward reducing the training cost improves the development of LLMs. This challenge extends to vision-language models (VLMs), where we can observe similar differences in visual and textual modalities. Fine-tuning LLMs requires balancing computational efficiency against downstream task performance. As transformer architectures scale to billions of parameters, the computation becomes increasingly expensive [ 1,2]. While optimizing for memory and computational efficiency remains important [ 3], one key issue is often overlooked, i.e., the common practice of extensive fine-tuning assumes that more gradient updates always improve performance [ 4]. Research has shown that training for excessive epochs leads to overfitting, where models overfit training data and fail to generalize [5]. Conventional early stopping, which is determined based on loss score, is computationally expensive for large language models, as each validation step requires full forward passes through all transformer layers for every sample in the validation set. This overhead scales linearly with both model size and validation set size, forcing practitioners to validate infrequently, typically every few thousand training steps [ 6], thereby creating a fundamental trade-off between computational cost and the risk of overfitting. In vision transformers(ViT), the same problem is present because ViT process both image and text inputs through separate transformer encoders. We detail this overhead in 5. 1arXiv:2509.01842v3 [cs.LG] 16 Oct 2025 A",
    "thousand training steps [ 6], thereby creating a fundamental trade-off between computational cost and the risk of overfitting. In vision transformers(ViT), the same problem is present because ViT process both image and text inputs through separate transformer encoders. We detail this overhead in 5. 1arXiv:2509.01842v3 [cs.LG] 16 Oct 2025 A PREPRINT Furthermore, our experiment shows that Transformer’s components have different convergence patterns, as shown in Figure 1. This difference in convergence is particularly pronounced in ViT, where vision transformers typically converge slower than their language counterparts. and the binary decision of classic early stopping fails to exploit the diverse convergence patterns, where attention and MLP components exhibit fundamentally different learning dynamics during the fine-tuning process [7]. Through analysis of gradient dynamics across transformer architectures, we identified a critical inefficiency in current fine-tuning practices, i.e., varied convergence across the Transformer’s components. By tracking magnitude of gradient changes for attention projections matrix: Wq,Wk,Wvthat compute queries, keys, and values respectively, and Woor output projection. As well as MLP network matrix Wupfor dimension expansion and Wdownfor dimension reduction. We observe that some components reach convergence with magnitude of gradient changes below 10−3, while others maintain substantial magnitude of gradient changes throughout, as shown in Figure 1. This pattern holds across both language models and ViT architectures. Figure 1: Element-wise L1norms for the gradient matrix of components in layer 7 for Qwen3-0.6B [ 8]. Each step consists of processing one training batch through the complete forward pass, loss computation, backpropagation, and parameter update cycle. The seven tracked matrices comprise attention projections ( W(7) q,W(7) k,W(7) v,W(7) o) and MLP components ( W(7) gate,W(7) up,W(7) down). MLP projections exhibit 2 to 3 ×higher gradient magnitudes than attention projections throughout training, with W(7) upandW(7) downmaintaining the largest magnitude of gradient changes. The red dotted line indicates our convergence thresholdτ= 1.183×10−3. This disparity motivatesGradES, our gradient-based early stopping strategy that operates at the matrix level. Rather than requiring expensive validation passes,GradESleverages gradient information already computed during backpropagation to monitor each matrix W(l)∈ {W(l) q,W(l) k,W(l) v,W(l) o,W(l) gate,W(l) up,W(l) down}independently, where ldenotes the layer. As illustrated in Figure 2, our architecture monitors the L1 norm changes for each component within the transformer layers. When a matrix’s magnitude of gradient changes falls below threshold τ, the component is frozen (marked with lock icons in Figure 2), we stop its training while maintaining gradient flow for proper backpropagation, transforming early stopping from a binary termination decision into a continuous regularization mechanism. This selective stopping allows components with higher gradient activity to continue learning while converged components remain fixed. The selection of thresholdτis a critical hyperparameter whose configuration is detailed in Section C. Our experiments across five LLMs, varied from 0.6B to 14B parameters, demonstrate thatGradESreduces fine-tuning time by 50% while maintaining or improving accuracy across eight benchmarks. Attention projections consistently stabilize 2–3 times faster than MLP components, with key and value projections stopping earliest which is a pattern that validates our component specific (Attention or MLP) approach over early stopping.GradEScan be seamlessly integrated with optimizers such as Adam [ 9], SGD [",
    "improving accuracy across eight benchmarks. Attention projections consistently stabilize 2–3 times faster than MLP components, with key and value projections stopping earliest which is a pattern that validates our component specific (Attention or MLP) approach over early stopping.GradEScan be seamlessly integrated with optimizers such as Adam [ 9], SGD [ 10], and parameter-efficient fine-tuning methods such as LoRA [11], while complementing weight decay by preventing overfitting in converged components as weight decay continues, regularizing active components. Contributions.We make the following contributions: 2 A PREPRINT Figure 2:GradESarchitecture. Color indicate gradient magnitude changes for each weight matrix. Components with low gradient changes (below thresholdτ) are stopped (lock icons) while others continue training. •We proposeGradES, a gradient-based early stopping method designed specifically for transformer and vision transformer architectures, eliminating expensive validation inference used for classic early stopping. •We identified that gradient-based early stopping serves as an effective regularization technique, preventing overfitting in fast-converging Transformers’ components while maintaining learning capacity in not yet converged completely, achieving improved accuracy up to 1.2% on language tasks and 3.88% on multimodal benchmarks and training time speed up of 1.57–7.22×. •We validate the compatibility ofGradESwith diverse optimization algorithms (e.g., Adam, SGD) and parameter- efficient fine-tuning methods (e.g., LoRA), showing consistent acceleration across training paradigms while preserving their respective computational and memory efficiency advantages. •We release our implementation as an open-source repository and PyPi package. It requires minimal modifications to existing training pipelines. 2 Related Work 2.1 Transformer and VLM The Transformer architecture [ 12] consists of distinct weight matrices for queries, keys, values, output projections, and MLP components. Vision-Language Models (VLMs) have evolved from contrastive learning frameworks such as CLIP [ 13], which learns joint embeddings for zero-shot visual recognition, to architectures that enable complex multimodal reasoning. LLaV A [ 14] integrated frozen vision encoders with large language models (LLMs) through learnable projection layers, refined by state-of-the-art models like Qwen2.5-VL-7B [ 15]. Concurrently, efforts toward computational efficiency have produced frameworks like nanoVLM [ 16]. Interestingly, language encoders converge faster compared to vision layers which require extended optimization. Even within the language layers, attention and FFN layers converge at disparate rates. This variability motivates ourGradESalgorithm. 2.2 Parameter-Efficient Fine-tuning and Adaptive Training LoRA [ 11] employs low-rank decomposition BA where r≪min(d in, dout)(10,000 ×parameter reduction). A group of promising theoretical works established connections between gradient convergence patterns and optimization dynamics in neural networks. Arora et al. [ 17] analyzes gradient descent convergence in deep linear networks, while Chatterjee et al. [18] extends these results to general deep architectures, demonstrating that different components exhibit distinct convergence trajectories. Nguegnang et al. [ 19] characterizes convergence rates for gradient descent in linear networks, providing theoretical foundations for component convergence monitoring. The role of gradient-based stopping criteria has been formalized by Patel et al. [ 20], who establish strong convergence guarantees when gradients fall below thresholds. Additionally, Gess et al. [ 21] demonstrate exponential convergence rates for momentum methods in over parameterized settings, suggesting that convergence patterns are robust across different optimizers. Building on these theoretical insights, several methods exploit convergence patterns for training efficiency. Aut-",
    "who establish strong convergence guarantees when gradients fall below thresholds. Additionally, Gess et al. [ 21] demonstrate exponential convergence rates for momentum methods in over parameterized settings, suggesting that convergence patterns are robust across different optimizers. Building on these theoretical insights, several methods exploit convergence patterns for training efficiency. Aut- oFreeze [ 22] accelerates BERT fine-tuning by 2.55 ×through automatic layer freezing based on gradient patterns, though it operates only at layer granularity. AFLoRA [ 23] introduces gradual freezing of LoRA adapters using conver- gence scores, achieving 9.5 ×parameter reduction and 1.86 ×speedup. LoRAF [ 24] prevents catastrophic forgetting 3 A PREPRINT by selectively freezing LoRA matrices based on their convergence state, using 95% fewer parameters than standard LoRA. However, these approaches either require coarse layer level decisions (AutoFreeze), additional architectural components like routers or masks (AFLoRA, LoRAF), or coupling with specific fine-tuning methods. GradESadvances this line of work by directly leveraging the heterogeneous convergence phenomenon [ 25] at the granularity of individual weight matrices.GradESuses gradient magnitudes to detect when components converge [ 26]. It then freezes individual components as they reach convergence, adapting to each component’s learning rate without requiring schedules. 2.3 Early Stopping Early stopping represents an established regularization technique with extensive theoretical and practical foundations. Prechelt et al. [ 27] provided practical guidelines for validation-based stopping criteria, while theoretical analyses by Yao et al. [ 28] demonstrated that early stopping acts as implicit regularization in gradient descent learning. Recent theoretical advances by Ji et al. [ 29] have proven the consistency of early stopping in neural networks, establishing rigorous convergence guarantees. Beyond traditional validation-based approaches, gradient-based early stopping by Mahsereci et al. [ 30] and Pflug et al. [ 31] monitors gradient magnitudes to detect convergence without requiring validation data. Recent work has explored more sophisticated stopping strategies: instance dependent early stopping by Yuan et al. [ 32] adapts termination per training example to reduce overfitting on easy samples, while correlation based methods by Ferro et al. [ 33] combine multiple online indicators for more robust stopping decisions. Methods for noisy settings have also emerged, including strategies for learning with label noise by Bai et al. [ 34] and stopping without validation data by Yuan et al. [35]. All these approaches apply stopping decisions globally, either terminating all training simultaneously or operating at the instance level.GradESdiffers fundamentally by applying gradient-based convergence detection at the individual weight matrix level within transformers, allowing different components to stop at their natural convergence points while others continue learning. This approach will effectively bridge early stopping with fine-grained regularization. 3 Method 3.1 GradES Algorithm We analyzed the magnitude of gradient changes of weight matrices across Transformer layers during fine-tuning and discovered different convergence rates for different components within Transformers’ Layers. For each weight matrix W(l)in layer l∈ {1, . . . , L} , we track the element-wise L1norm of gradients at training step t, as it is presented in Equation 1. G(l) W(t) =∥∇W(l) t− ∇W(l) t−1∥1=mX i=1nX j=1|(∇W(l) t)ij−(∇W(l) t−1)ij|(1) whereW(l)∈ {W(l) q,W(l) k,W(l) v,W(l) o,W(l) gate,W(l) up,W(l) down}represents the attention projection",
    "W(l)in layer l∈ {1, . . . , L} , we track the element-wise L1norm of gradients at training step t, as it is presented in Equation 1. G(l) W(t) =∥∇W(l) t− ∇W(l) t−1∥1=mX i=1nX j=1|(∇W(l) t)ij−(∇W(l) t−1)ij|(1) whereW(l)∈ {W(l) q,W(l) k,W(l) v,W(l) o,W(l) gate,W(l) up,W(l) down}represents the attention projection matrices ( Wq, Wk,Wv,Wo) and MLP weight matrices ( Wgate,Wup,Wdown) within each Transformer layer. The metric G(l) W(t) quantifies the total gradient flow through each component, revealing distinct convergence trajectories that motivate our gradient-based component level early stopping strategy. Algorithm 1 formalizes,GardES, our gradient-based early stopping procedure.GardESalgorithm introduces three key innovations that distinguish it from traditional early stopping approaches: Component-level convergence detection.Unlike conventional methods that monitor global validation accuracy, GradEStracks individual weight matrices W(l)∈ {W(l) q,W(l) k,W(l) v,W(l) o,W(l) gate,W(l) up,W(l) down}within each layer l. We employ the L1gradient norm GW(t) =∥∇W∥ 1as our convergence metric, chosen for its computational efficiency compared to L2norms. When GW(t)< τ , we consider the component converged stop updating the weight matrix (lines 8-14). Adaptive grace period strategy.The initial tgrace period =⌈αT⌉ steps (with α= 0.5 in our experiments) allow all components to escape their pre-trained initialization before convergence monitoring begins. This prevents terminating training of components prematurely that may appear initially converged but require substantial adaptation for the downstream task. The grace period duration depends on the total training examples and scales proportionally with the total training budgetT. Gradient flow preservation.A critical design choice is maintaining gradient computation through Converged matrices (line 12). While converged components do not receive parameter updates (lines 17-22), they continue to propagate 4 A PREPRINT Algorithm 1GradES: Early Stopping based on Absolute Change of Gradient Matrix Input: Pre-trained model MwithLlayers, dataset D, total steps T, grace period ratio α, threshold τ, learning rate η Output:Fine-tuned modelM′ 1:Initialize:All parameters trainable, frozen setF ← ∅ 2:Initialize:Previous gradients∇W t−1←0for allW 3:t grace period ← ⌈α·T⌉▷Start monitoring after grace period 4:The grace period is computed as a fractionαof the total training stepsT. 5:fortraining stept= 1toTdo 6:SampleB ∼ D; compute lossL(B)and gradients 7:ift > t grace period then▷Monitor after grace period 8:foreachW∈all layers :W/∈ Fdo 9:G W(t) =P i,j|(∇W t)ij−(∇W t−1)ij| 10:ifG W(t)< τthenF ← F ∪ {W} 11:foreach projection matrixWdo 12:ifW/∈ Fthen▷Update only active parameters 13:W←W−η· ∇W 14:else 15:Skip update (but gradient still flows through) 16:foreachW∈all layersdo▷Store gradients for next step 17:∇W t−1← ∇W t 18:ifall parameters frozenthen▷i.e., allW∈ F 19:break ▷Early stopping 20:M′←Update model with modified projection matricesW 21:returnFine-tuned modelM′ gradients to earlier layers. This ensures that active components receive proper gradient signals throughout training, preventing the gradient flow disruption that would occur with complete component removal, like pruning. The algorithm terminates when all components satisfy the convergence criterion (Line 24), eliminating unneces- sary computation on converged parameters. We provide formal convergence guarantees and theoretical analysis in Appendix B. 3.2GradESfor Low-Rank Adaptation LoRA is one of the most common approaches used for fine-tuning. Since LoRA constrains parameters to a low- dimensional subspace, gradient dynamics in this reduced space exhibit fundamentally different convergence properties than full fine-tuning. When applyingGradESto LoRA [ 11] fine-tuning, we",
    "and theoretical analysis in Appendix B. 3.2GradESfor Low-Rank Adaptation LoRA is one of the most common approaches used for fine-tuning. Since LoRA constrains parameters to a low- dimensional subspace, gradient dynamics in this reduced space exhibit fundamentally different convergence properties than full fine-tuning. When applyingGradESto LoRA [ 11] fine-tuning, we monitor gradient magnitudes in the low-rank space rather than the full parameter space. Let l∈ {1, . . . , L} denote the layer index where Lis the total number of layers. Within each transformer layer l, we apply LoRA decomposition to individual weight matrices W(l)∈ {W(l) q,W(l) k,W(l) v,W(l) o,W(l) gate,W(l) up,W(l) down}, where the first four correspond to attention projections and the latter three to MLP components. For each weight matrix W(l)∈Rdout×d in, where dout, dinare the output and input dimensions, in layer l, the LoRA weight is: W(l) adapted =W(l) frozen +B(l) WA(l) W (2) whereB(l) W∈Rdout×randA(l) W∈Rr×d inare the trainable low-rank matrices with rankr≪min(d out, din). For each individual LoRA matrix Win layer l, we track convergence by monitoring the combined gradient magnitude: G(l) W(t) =∥∇A(l) W∥1+∥∇B(l) W∥1 (3) where ∇A(l) Wand∇B(l) Wdenote the gradients of the low-rank matrices at training step t, and∥ · ∥ 1denotes the element-wiseL 1norm. The freezing operates at the matrix level, enabling precise control. After the grace period period t > t grace period , we independently freeze each LoRA matrix when: G(l) W(t)< τ rforW∈ {W q,Wk,Wv,Wo,W gate,W up,W down}(4) 5 A PREPRINT where τris the convergence threshold adjusted for the reduced parameter count. Once a specific matrix reaches convergence, we stop updating its corresponding A(l) WandB(l) Wwhile continuing to compute gradients through them for backpropagation. Training terminates when all LoRA matrices across all layers are frozen. 4 Experimental Setup 4.1 Models We evaluateGradESon the 5 most popular language models and 2 vision language model on huggingface [ 36] (at the time of conducting this experiment) to represent different parameter scales, quantization strategies, and architectural designs. Our language model selection spans three orders of magnitude in parameter count (0.6B to 14B) to investigate the scalability of our approach. We conducted all experiments using 4-bit quantized models, due to hardware limitations of the experimental platform. Specifically, we employ:Qwen3-14B[ 8];Microsoft Phi4-14B[ 37];Llama-3.1-8B- Instruct[ 38];Mistral-7B-Instruct-v0.3[ 39]; and the compactQwen3-0.6B[ 8]. To assess performance on multimodal tasks, we also evaluate two vision-language models:Qwen2.5-VL-7B[15] and the smallernanoVLM[16]. 4.2 Benchmark and Evaluation Metrics We evaluateGradESon eight widely used benchmarks covering different aspects of language understanding. For reasoning tasks, we use BoolQ [ 40] for boolean question answering requiring complex reasoning, PIQA [ 41] for physical commonsense reasoning about everyday situations, SIQA [ 42] for social commonsense reasoning about human interactions, and HellaSwag [ 43] for commonsense inference about plausible continuations. For knowledge-intensive task completion, we employ OpenBookQA [ 44] for science questions requiring multi-hop reasoning, ARC-Easy and ARC-Challenge [ 45] for grade school science questions at two difficulty levels, and WinoGrande [ 46] for pronoun resolution requiring commonsense knowledge. We measure both task accuracy and computational efficiency to provide a rounded view ofGradES’s benefits. Average task accuracy",
    "[ 44] for science questions requiring multi-hop reasoning, ARC-Easy and ARC-Challenge [ 45] for grade school science questions at two difficulty levels, and WinoGrande [ 46] for pronoun resolution requiring commonsense knowledge. We measure both task accuracy and computational efficiency to provide a rounded view ofGradES’s benefits. Average task accuracy is measured by accuracy on each benchmark’s test set. For computational efficiency metrics, we track training time on identical hardware and floating point operations (FLOPs) computed using PyTorch profiler [47]. We evaluate vision transformers using the LMMs-Eval harness [ 48,49] to assess nanoVLM training accuracy. We conduct fine-tuning experiments on Qwen2.5-VL-7B and evaluate performance across three established benchmarks: GQA [ 50] for compositional visual reasoning, VQAv2 [ 51] for robust visual question answering, and COCO Cap- tions [52] for image captioning capabilities. 4.3 PEFT and Early Stopping Methods We evaluateGradESagainst established fine-tuning paradigms to demonstrate its benefits. Our baseline methods comprise Full Parameter Fine-tuning (FP), which updates all model parameters without constraints; and LoRA [ 11] to assess the composability of our method. In particular, we applyGradESto both full fine-tuning (FP+GradES) and LoRA (LoRA+GradES), yielding six distinct configurations for comprehensive evaluation. For validation-based early stopping baselines (FP+ES and LoRA+ES), we perform validation checks at 5% intervals throughout training. Terminating training when validation loss fails to improve for consecutive checkpoints. We presents the overhead of early stopping compared toGradESin 4. More detailed experiment settings and hyperparameter selection are provided in Appendix D. 5 Results 5.1 Accuracy on Benchmarks We evaluateGradESagainst standard full-parameter (FP) fine-tuning and LoRA across five language models ranging from 0.6B to 14B parameters on eight commonsense reasoning benchmarks. As shown in Table 1,GradESconsistently improves upon the baseline early stopping (ES) method across both fine-tuning methods (FP and LoRA). For full- parameter fine-tuning on larger models (14B), full-parameter withGradESachieves the highest average accuracy on Qwen3 (90.81%) and Phi4 (91.94%), demonstrating consistent improvements over full-parameter fine-tuning (90.80% and 91.93% respectively). Its impact is more significant on smaller models, where LoRA (ES) and LoRA (GradES) on Qwen3 0.6B achieve 67.37% and 67.30% average accuracy, substantially outperforming standard full-parameter methods ( ∼66.5%). Notably, the choice of base fine-tuning method (Full-parameter vs. LoRA) exhibits its model- independent behavior, while full-parameter fine-tuning methods perform competitively on larger models (Qwen3 14B, Phi4 14B), LoRA variants showed higher accuracy on mid-sized models, with LoRA (ES) achieving 86.27% 6 A PREPRINT on Mistral-7B compared to 75.80% for standard full-parameter fine-tuning, a remarkable 10.47 percentage point improvement.GradESshows particular strength on specific benchmarks, achieving best results on Winograde compared to other methods (Qwen3 14B: 84.77%), PIQA (Phi4 14B: 92.60%). Table 1: Comparison of the accuracy for different fine-tuning methods on five different language models. Values are reported in percentages, and the best one in each category is highlighted in bold. Model Method BoolQ PIQA SIQA HellaSwag Winograde OpenBookQA ARC-C ARC-E Avg. Qwen3 14BFull Parameter 91.07 91.29 81.93 95.11 83.03 91.60 94.31 98.07 90.80 FP+ES 91.07 91.08 81.99 95.05 83.03 91.40 93.98 98.07 90.71 FP+GradES91.2291.19 82.04 94.97 83.0391.80 94.3197.8990.81 LoRA 90.8691.2481.68 95.21 83.35 91.40 94.31 97.19 90.65 LoRA+ES 90.64 91.08 81.9995.4081.53 91.40",
    "Method BoolQ PIQA SIQA HellaSwag Winograde OpenBookQA ARC-C ARC-E Avg. Qwen3 14BFull Parameter 91.07 91.29 81.93 95.11 83.03 91.60 94.31 98.07 90.80 FP+ES 91.07 91.08 81.99 95.05 83.03 91.40 93.98 98.07 90.71 FP+GradES91.2291.19 82.04 94.97 83.0391.80 94.3197.8990.81 LoRA 90.8691.2481.68 95.21 83.35 91.40 94.31 97.19 90.65 LoRA+ES 90.64 91.08 81.9995.4081.53 91.40 93.65 97.54 90.40 LoRA+GradES 90.67 91.1382.2995.2284.7791.20 93.31 97.02 90.70 Phi4 14BFull Parameter 90.49 91.95 83.27 95.49 88.71 93.00 94.31 98.25 91.93 FP+ES 90.34 92.11 82.55 95.50 88.56 93.00 94.31 98.07 91.80 FP+GradES 90.3192.60 83.0695.4388.9592.60 94.31 98.2591.94 LoRA 90.31 92.00 82.45 95.36 87.53 91.8094.6598.07 91.52 LoRA+ES 90.61 92.22 82.6095.4487.37 92.00 94.3198.2591.60 LoRA+GradES90.4992.60 82.40 95.38 87.37 91.60 94.65 98.07 91.57 Qwen3 0.6BFull Parameter 77.28 69.31 66.99 65.09 50.28 61.20 61.54 80.53 66.53 FP+ES 77.06 68.99 67.09 65.16 49.57 61.00 63.21 81.23 66.66 FP+GradES 77.0371.3866.84 64.2651.6262.40 61.20 79.65 66.80 LoRA79.1469.1569.1467.94 49.0966.2060.54 77.19 67.30 LoRA+ES 79.11 69.10 68.8368.1849.64 65.00 61.20 77.8967.37 LoRA+GradES 78.56 69.42 68.17 68.20 49.41 65.4061.5477.72 67.30 Llama-3.1-8BFull Parameter 89.27 88.08 81.01 94.40 81.93 85.00 83.61 92.46 86.97 FP+ES 89.24 87.81 80.86 94.42 82.56 85.80 82.61 92.11 86.93 FP+GradES 88.8788.2580.45 94.23 82.79 84.8083.95 92.81 87.02 LoRA 87.98 87.65 79.73 94.31 80.3587.2083.28 91.40 86.49 LoRA+ES 88.62 88.19 79.32 94.20 80.35 87.20 83.28 91.75 86.62 LoRA+GradES88.7888.0379.68 94.43 81.6987.00 83.95 90.53 86.76 Mistral-7BFull Parameter 85.26 80.25 77.33 83.71 66.30 75.60 62.54 75.44 75.80 FP (ES) 85.32 80.09 76.20 83.95 57.54 74.60 64.21 74.39 74.54 FP+GradES 85.38 78.56 75.79 84.14 66.47 76.40 61.20 75.10 75.38 LoRA89.3387.43 79.7994.9579.72 84.20 76.25 88.42 85.01 LoRA+ES 88.9388.30 81.0194.6982.24 85.0079.60 90.3586.27 LoRA+GradES 89.36 88.03 80.71 94.69 80.90 84.4081.61 89.6586.17 Table 2 presentsGradESperformance on vision-language tasks using Qwen2.5-VL-7B.GradESconsistently improves both full-parameter and LoRA fine-tuning, with LoRA+GradES achieving the highest average accuracy (70.6%). LoRA methods substantially outperform full-parameter approaches on image captioning (54.44% vs. 41.61% for COCO Captions), while LoRA+GradES attains best performance on VQAv2 (81.24%). These results demonstrate thatGradES effectively enhances vision-language model performance, particularly when combined with PEFT methods. Table 2: Performance comparison ofGradESagainst standard fine-tuning methods on vision-language benchmarks. Results show accuracy (%) for Qwen2.5-VL-7B across visual reasoning (GQA), question answering (VQAv2), and image captioning (COCO Cap) tasks. Model Method GQA VQAv2 COCO Cap Avg. Qwen2.5-VL-7BFull Parameter 75.69 81.0 41.38 66.08 FP+GradES 76.08 80.81 41.61 66.20 LoRA 76.49 81.01 53.22 70.24 LoRA+GradES 76.13 81.24 54.44 70.6 7 A PREPRINT Table 3: Performance ofGradESon nanoVLM training across multimodal reasoning benchmarks. Results show accuracy (%) for full-parameter fine-tuning with and withoutGradESacross perception, reasoning, and knowledge- based tasks. Benchmark Training Training+GradES Coarse Perception 38.87 42.42 Fine-grained Perception 22.40 29.82 Instance Reasoning 36.07 36.42 Logical Reasoning 28.86 37.21 Math 27.60 28.60 Science & Technology 31.10 33.74 Avg. 30.82 34.70 Table 3 demonstratesGradES’s effectiveness on nanoVLM training across diverse multimodal benchmarks. Training withGradESachieves substantial improvements over standard training, with an average accuracy increase of 3.88 percentage points (34.70% vs. 30.82%). Most notably,GradESyields significant gains on Fine-grained Perception (+7.42% and Logical Reasoning (+8.35%), while maintaining consistent improvements across all evaluated domains. These results confirmGradES’s ability to enhance vision-language model training, particularly for tasks requiring detailed visual understanding and complex reasoning. 5.2 Training Efficiency Tables 4 and 5 present",
    "(34.70% vs. 30.82%). Most notably,GradESyields significant gains on Fine-grained Perception (+7.42% and Logical Reasoning (+8.35%), while maintaining consistent improvements across all evaluated domains. These results confirmGradES’s ability to enhance vision-language model training, particularly for tasks requiring detailed visual understanding and complex reasoning. 5.2 Training Efficiency Tables 4 and 5 present computational efficiency metrics across language and vision-language models.GradES demonstrates consistent efficiency improvements over both baseline and traditional early stopping (ES) approaches. For language models, FP+GradES achieves speedups of 1.32–1.64 ×while reducing FLOPs by 29–45%, with the most significant gains on larger models (Qwen3-14B: 1.51 ×speedup, 0.55 ×FLOPs). In contrast, traditional ES paradoxically slows training (0.59–0.76 ×) despite FLOPs reduction, highlighting the overhead of frequent validation and convergence monitoring. Vision-language models show similar trends, with FP+GradES achieving 1.17 ×speedup and 12% FLOPs reduction on Qwen2.5-VL-7B. The efficiency benefits extend to parameter-efficient methods, where LoRA+GradES consistently outperforms standard LoRA across all models. For language models, LoRA+GradES achieves speedups of 2.66–2.87 ×over full-parameter baselines while reducing LoRA’s computational overhead from 2.34–2.43 ×to 1.88–2.29 ×FLOPs. On vision-language tasks, LoRA+GradES delivers the highest speedup (1.80 ×), reducing training time by 44%. Remarkably, these computational savings preserve model quality—GradESvariants achieve the highest accuracies while requiring substantially fewer FLOPs than standard fine-tuning. These results establish gradient-guided early stopping as a practical solution for resource-constrained deployment, demonstrating consistent efficiency gains across model architectures (Qwen, Phi, Llama, Mistral), scales (0.6B–14B parameters), and modalities (language and vision-language). 8 A PREPRINT Table 4: Training time and computational cost comparison of different fine-tuning methods on 5 different language models. Training time in seconds, FLOPs in floating point operations. Speedup and FLOPs ratios are computed relative to Full Parameter(Base) for all methods. The best one in each category is highlighted in bold. Model Method Training Time (s) Speedup FLOPs FLOPs Ratio Qwen3-14BFull Parameter(Base) 16,202 1.00×1.17×10181.00× FP+ES 22,466 0.72×8.76×10170.75× FP+GradES 10,721 1.51×6.43×10170.55× LoRA 6,387 2.54×2.74×10182.34× LoRA+ES 23,932 0.68×2.74×10182.34× LoRA+GradES 5,6432.87×2.43×10182.08× Phi4-14BFull Parameter(Base) 14,627 1.00×1.12×10181.00× FP+ES 23,040 0.63×9.49×10170.85× FP+GradES 9,218 1.59×6.15×10170.55× LoRA 6,030 2.43×2.69×10182.40× LoRA+ES 24,394 0.60×2.69×10182.40× LoRA+GradES 5,5062.66×2.38×10182.13× Qwen3-0.6BFull Parameter(Base) 6,550 1.00×3.68×10161.00× FP+ES 8,569 0.76×2.76×10160.75× FP+GradES 4,018 1.63×2.03×10160.55× LoRA 8927.34×8.94×10162.43× LoRA+ES 6,155 1.06×8.94×10162.43× LoRA+GradES 907 7.22×8.43×10162.29× Llama-3.1-8BFull Parameter(Base) 9,541 1.00×7.45×10171.00× FP+ES 16,129 0.59×6.70×10170.90× FP+GradES 5,832 1.64×4.10×10170.55× LoRA 3,737 2.55×1.58×10182.12× LoRA+ES 15,499 0.62×1.58×10182.12× LoRA+GradES 3,3702.83×1.40×10181.88× Mistral-7BFull Parameter(Base) 9,256 1.00×6.38×10171.00× FP+ES 14,521 0.64×6.38×10171.00× FP+GradES 6,996 1.32×4.51×10170.71× LoRA 3,752 2.47×1.51×10182.37× LoRA+ES 11,159 0.83×1.51×10182.37× LoRA+GradES 3,2592.84×1.32×10182.07× Table 5: Time and FLOPs comparison ofGradESon vision-language model for Qwen2.5-VL-7B across different fine-tuning methods. Model Method Training Time (s) Speedup FLOPs FLOPs Ratio Qwen2.5-VL-7BFull Parameter 157,239 1.00×2.27×10191.00× FP+GradES 134,686 1.17×1.99×10190.88× LoRA 103,466 1.52×2.80×10191.23× LoRA+GradES 87,5721.80×2.52×10191.11× 5.3 Ablation Studies We first investigate the necessity of the grace period α. When we remove the grace period entirely ( α= 0 ), training terminates immediately after initialization. As illustrated in Figure 1, gradient norms across all components remain small during the warm-up phase, falling below the convergence threshold τ. This observation confirms that the grace period is crucial for our method. Then we evaluate the impact of the grace period α∈ {0.1,0.2,0.3,0.4,0.5,0.6} and convergence threshold τ∈ {1.5,3.0,4.5,6.0,7.5,9.0} on both model performance and training efficiency. All experiments",
    "components remain small during the warm-up phase, falling below the convergence threshold τ. This observation confirms that the grace period is crucial for our method. Then we evaluate the impact of the grace period α∈ {0.1,0.2,0.3,0.4,0.5,0.6} and convergence threshold τ∈ {1.5,3.0,4.5,6.0,7.5,9.0} on both model performance and training efficiency. All experiments are conducted on 9 A PREPRINT the Qwen-14B model across eight benchmark tasks: BoolQ, PIQA, SIQA, HellaSwag, WinoGrande, OpenBookQA, ARC-Challenge, and ARC-Easy. Table 6 reports the average evaluation accuracy across these benchmarks, while Table 7 presents the corresponding training times. Our results reveal that optimal performance (92.81% average accuracy) is achieved with τ= 1.5 andα= 0.5 , suggest- ing that a moderate grace period combined with a convergence threshold yields the best generalization. Conversely, the most significant speedup occurs at τ= 9.0 andα= 0.1 , where the large threshold enables early stopping of most components and the minimal grace period starts of freezing. Table 6: Accuracy values are shown as percentages for threshold ( τ) and grace period ( α) parameters, with the best result highlighted in bold. τ/α0.1 0.2 0.3 0.4 0.5 0.6 1.5 90.64 90.59 90.77 90.6890.9390.83 3.0 90.58 90.70 90.51 90.61 90.54 90.83 4.5 90.50 90.69 90.62 90.61 90.74 90.80 6.0 90.46 90.68 90.67 90.58 90.68 90.78 7.5 90.24 90.27 90.58 90.61 90.70 90.78 9.0 90.01 90.35 90.34 90.66 90.71 90.71 Table 7: Fine-tuning time for threshold (τ) and grace period (α) parameters, with the fastest time highlighted in bold. τ/α0.1 0.2 0.3 0.4 0.5 0.6 1.5 10726.74 10592.54 10629.99 10840.85 11187.48 11774.73 4.5 8925.71 9269.45 9317.26 9789.08 10273.89 11318.94 7.5 8038.52 8337.48 8599.51 9262.74 9962.62 11087.04 9.07393.147746.00 8036.02 9130.12 9477.76 10916.65 6 Discussion 6.1 Convergence Patterns Across Different Models Figure 3 shows the progression of the matrix that converged across different model scales during training. After a warm-up period of α= 1000 steps, our method begins freezing converged components based on gradient magnitude thresholdsτ(model-specific values detailed in Appendix C). The difference in convergence rate reflects distinct architectural behaviors. Larger models (7B-14B) exhibit rapid convergence, with the majority of components frozen by step 1400—approximately 40% through training. In contrast, the smaller Qwen-0.6B model demonstrates delayed convergence, with no components meeting the freezing criteria until step 1600. Notably, the threshold τvaries significantly between training methods, and full fine-tuning requires larger thresholds compared to LoRA adaptation. 10 A PREPRINT Figure 3: Cumulative frozen components during training across model scales. Fraction of weight matrices frozen over time for five different LLMs. 6.2 Attention versus MLP components Figure 4a presents the gradient norms |∇W| across all weight matrices during fine-tuning of the Qwen-0.6B model. We compute the element-wise L1 norm for each weight matrix and report averaged values for two architectural components: MLP matrices (W up,W down; orange) and attention projection matrices (W q,Wk,Wv,Wo; blue). We have two key observations. First, the gradient trend reflects the cosine learning rate schedule: initial warmup drives increasing gradient magnitudes as the model adapts to the task distribution, while the subsequent cosine decay produces monotonically decreasing gradient norms, enabling smooth convergence from exploration to refinement. Second, and",
    "q,Wk,Wv,Wo; blue). We have two key observations. First, the gradient trend reflects the cosine learning rate schedule: initial warmup drives increasing gradient magnitudes as the model adapts to the task distribution, while the subsequent cosine decay produces monotonically decreasing gradient norms, enabling smooth convergence from exploration to refinement. Second, and more critically for our method, MLP weight matrices consistently maintain larger gradient norms compared to attention projection matrices throughout training. This persistent gap indicates that MLP parameters require more steps to converge, suggesting inefficiency under uniform training strategies. This observation directly motivates our approach: by dynamically allocating computational resources proportional to gradient magnitudes, we can accelerate convergence of slower learning components while maintaining overall model accuracy. (a) Gradient norm evolution during Qwen-0.6B fine-tuning. Element-wise L1 norms of weight gradients averaged across lay- ers for MLP matrices (orange) and attention projections (blue). MLP matrices consistently exhibit larger gradient magnitudes throughout training, indicating slower convergence and motivat- ing targeted computational allocation. (b) Gradient norm evolution during NanoVLM training. Element-wise L1 norms of weight gradients averaged across Vision layers matrices (blue) and attention projections (orange). ViT matrices consistently exhibit larger gradient magnitudes throughout training. Figure 4: Comparison of gradient norm evolution patterns across different model architectures during fine-tuning. 11 A PREPRINT 6.3 LLMs vs VLMs Perhaps a similarly interesting discovery we made is with the vision language model. Figure 4 illustrates the training dynamics of the vision and language transformer components separately. Our analysis reveals an difference in convergence pattern like that between attention and MLP components: while the language transformer reaches convergence early in training, the vision transformer converges slower. However, extending training with a decreasing learning rate schedule leads to overfitting and subsequent accuracy degradation. Our approach demonstrates significant accuracy improvements over full parameter fine-tuning. With only 8,000 training examples, we achieve 34.7% accuracy—representing a 3.92% absolute improvement over the baseline full fine- tuning approach. Remarkably, the original Nano-VLM requires approximately 17,000 examples to reach comparable performance levels[ 16], indicating that our method reduces the required training data by more than 50% while maintaining competitive accuracy. 6.4 Combination with Parameter-Efficient Fine-Tuning Methods GradEScombines particularly well with parameter-efficient fine-tuning methods such as LoRA. As shown in Table 4, LoRA+GradES achieves the fastest training times across all models, reducing training time to just 0.14–0.38 ×of standard full-parameter fine-tuning. This dramatic speedup comes from optimizing two independent dimensions, (i) LoRA reduces the parameter space through low-rank weight decomposition, (ii) whileGradESreduces training iterations by detecting convergence through gradient monitoring. For example, on Qwen3 0.6B, LoRA+GradES completes training in just 907 seconds compared to 6,550 seconds for standard fine-tuning, a 7.22 ×speedup, while achieving better accuracy (67.30% vs. 66.53%). The benefits of combiningGradESwith LoRA become especially clear when compared to standard early stopping. While LoRA+ES actually increases training time by 2.97–6.90 ×due to validation overhead, LoRA+GradES maintains or improves LoRA’s efficiency. This difference suggests that gradient-based early stopping criteria work particularly well in LoRA’s constrained optimization landscape, where the reduced parameter space provides clearer convergence signals. Although LoRA methods inherently require more FLOPs per iteration than full-parameter training",
    "time by 2.97–6.90 ×due to validation overhead, LoRA+GradES maintains or improves LoRA’s efficiency. This difference suggests that gradient-based early stopping criteria work particularly well in LoRA’s constrained optimization landscape, where the reduced parameter space provides clearer convergence signals. Although LoRA methods inherently require more FLOPs per iteration than full-parameter training (2.07– 2.43×),GradEScompensates by reducing the total number of iterations needed. The result is a practical for deployment due to the extremely fast training times with reasonable computational costs, making LoRA+GradES the optimal choice for fine-tuning. 6.5 GradES versus Classic Early Stopping A fundamental limitation of classic early stopping is the computational expense of requiring complete forward passes for every validation step. In contrast,GradESreuses gradient information from backpropagation, yielding substantial computational savings. As shown in Table 4, it achieves up to 6.79 ×speedup compared to classic early stopping on Qwen3-0.6B (907s vs 6,155s for LoRA fine-tuning), while maintaining comparable accuracy. The minimal accuracy difference—67.30% forGradESversus 67.37% for classic early stopping as shown in Table 1—does not justify the significant computational overhead. Across all five models tested,GradESconsistently reduces training time by 35–66% while achieving a 45–71% reduction in FLOPs compared to baseline full fine-tuning, making it particularly valuable for limited resource settings. Furthermore, classic early stopping employs a model-wide convergence criterion that is not suitable for Transformer architectures. As demonstrated in Figure 1, different weight matrices within transformer layers exhibit varying convergence rates. Model-wide early stopping fails to account for this heterogeneity, potentially leading to overfitting in some parameters while underfitting in others.GradESaddresses this limitation by enabling component-specific convergence criteria, allowing MLP and attention components to be trained independently until each reaches its optimal stopping point. This ensures all weight matrices converge according to appropriate criteria rather than being halted or unnecessarily extended based on global validation metrics. 7 Limitation WhileGradESdemonstrates substantial efficiency gains, some limitations exists. First, gradient monitoring incurs approximately 3% computational overhead, though this is negligible compared to the 1.57–7.22×training time speed up achieved. Second, the convergence threshold τrequires manual tuning across different models and tasks, with full- parameter fine-tuning requiring larger thresholds than LoRA and model scale affecting optimal values. For VLMs, we observe that vision and language components convergence at a different rate, motivating component specific thresholds for optimal performance. Third, our current implementation employs static freezing without patience mechanisms common in traditional early stopping, potentially leading to premature convergence decisions. Additionally, while 12 A PREPRINT we validateGradESon transformer architectures, its applicability to other neural architectures, such as convolutional networks, graph neural networks, and emerging architectures like state space models, remains unexplored. 8 Conclusion and Future work Several promising directions emerge for future work. Automatic threshold selection through gradient statistics or meta learning could eliminate manual tuning, while incorporating patience parameters would allow components to temporarily violate convergence criteria before freezing. Dynamic freezing and unfreezing mechanisms could adapt to task complexity and distribution shifts, particularly beneficial when combined with specific thresholds for MLP versus attention components as suggested by Figure 4a. Integration with complementary efficiency techniques like mixed precision training, gradient checkpointing, and structured pruning could yield multiplicative speedups.",
    "before freezing. Dynamic freezing and unfreezing mechanisms could adapt to task complexity and distribution shifts, particularly beneficial when combined with specific thresholds for MLP versus attention components as suggested by Figure 4a. Integration with complementary efficiency techniques like mixed precision training, gradient checkpointing, and structured pruning could yield multiplicative speedups. Extending GradESbeyond transformers to vision models, graph networks, and hybrid architectures would broaden its impact. Most ambitiously, applying gradient stopping to pretraining could significantly reduce the massive computational costs of foundation model development, while theoretical analysis of convergence criteria would provide principled guidelines for threshold selection and accuracy guarantees. The key insight underlying our approach is that different transformer components exhibit distinct convergence behaviors during fine-tuning. In VLMs particularly, we observe that language transformers converge earlier than vision trans- formers. By recognizing and exploiting these diverse convergence patterns,GradESallocates computational resources more efficiently than uniform training strategies. The method’s ability to eliminate costly validation passes while providing precise convergence control represents a practical advancement for deploying large language models with limited resources. As the scale of language models continues to grow, gradient optimization strategies likeGradES will become increasingly critical for making these powerful models accessible to the broader research community and enabling rapid experimentation in real-world applications. References [1]Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. QLoRA: Efficient finetuning of quantized LLMs.arXiv preprint arXiv:2305.14314, 2023. [2]Sadhika Malladi, Tianyu Gao, Eshaan Nichani, Alex Damian, Jason D. Lee, Danqi Chen, and Sanjeev Arora. Fine-tuning language models with just forward passes. InThirty-seventh Conference on Neural Information Processing Systems, 2023. [3]Reza Rawassizadeh.Machine Learning and Artificial Intelligence: Concepts, Algorithms and Models. 1 edition, March 2025. [4]Tianyi Zhang, Felix Wu, Arzoo Katiyar, Kilian Q. Weinberger, and Yoav Artzi. Revisiting few-sample BERT fine-tuning.arXiv preprint arXiv:2006.05987, 2021. [5]Marius Mosbach, Maksym Andriushchenko, and Dietrich Klakow. On the stability of fine-tuning BERT: Miscon- ceptions, explanations, and strong baselines.arXiv preprint arXiv:2006.04884, 2021. [6]Kushal Tirumala, Aram H. Markosyan, Luke Zettlemoyer, and Armen Aghajanyan. Memorization without overfitting: Analyzing the training dynamics of large language models.arXiv preprint arXiv:2205.10770, 2022. [7]Xinhao Yao, Hongjin Qian, Xiaolin Hu, Gengze Xu, Wei Liu, Jian Luan, Bin Wang, and Yong Liu. Theoretical insights into fine-tuning attention mechanism: Generalization and optimization.arXiv preprint arXiv:2410.02247, 2025. [8]Qwen, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, and Others. Qwen2.5 technical report.arXiv preprint arXiv:2412.15115, 2025. [9]Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization.arXiv preprint arXiv:1412.6980, 2017. [10] Herbert Robbins and Sutton Monro. A stochastic approximation method.The Annals of Mathematical Statistics, 22(3):400–407, 1951. [11] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. InInternational Conference on Learning Represen- tations, 2022. 13 A PREPRINT [12] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. InAdvances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. [13] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,",
    "A PREPRINT [12] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. InAdvances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. [13] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021. [14] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023. [15] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, and Others. Qwen2.5-vl technical report, 2025. [16] Luis Wiedmann, Aritra Roy Gosthipaty, and Andrés Marafioti. nanovlm. https://github.com/huggingface/ nanoVLM, 2025. [17] Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu. A convergence analysis of gradient descent for deep linear neural networks. InInternational Conference on Learning Representations (ICLR), 2019. [18] Sourav Chatterjee. Convergence of gradient descent for deep neural networks.arXiv preprint arXiv:2203.16462, 2022. [19] Gabin Maxime Nguegnang, Holger Rauhut, and Ulrich Terstiege. Convergence of gradient descent for learning linear neural networks.arXiv preprint arXiv:2108.02040, 2021. [20] Vivak Patel. Stopping criteria for, and strong convergence of, stochastic gradient descent on bottou-curtis-nocedal functions.arXiv preprint arXiv:2004.00475, 2021. [21] Benjamin Gess and Sebastian Kassing. Exponential convergence rates for momentum stochastic gradient descent in the overparametrized setting.arXiv preprint arXiv:2302.03550, 2024. [22] Yuhan Liu, Saurabh Agarwal, and Shivaram Venkataraman. Autofreeze: Automatically freezing model blocks to accelerate fine-tuning.arXiv preprint arXiv:2102.01386, 2021. [23] Zeyu Liu, Souvik Kundu, Anni Li, Junrui Wan, Lianghao Jiang, and Peter Anthony Beerel. AFLoRA: Adaptive freezing of low rank adaptation in parameter efficient fine-tuning of large models.arXiv preprint arXiv:2403.13269, 2024. [24] Juzheng Zhang, Jiacheng You, Ashwinee Panda, and Tom Goldstein. LoRA without forgetting: Freezing and sparse masking for low-rank adaptation. InSparsity in LLMs (SLLM): Deep Dive into Mixture of Experts, Quantization, Hardware, and Inference, 2025. [25] Dylan J. Foster, Ayush Sekhari, and Karthik Sridharan. Uniform convergence of gradients for non-convex learning and optimization.arXiv preprint arXiv:1810.11059, 2018. [26] Wenzhi Gao, Ya-Chi Chu, Yinyu Ye, and Madeleine Udell. Gradient methods with online scaling.arXiv preprint arXiv:2411.01803, 2024. [27] Lutz Prechelt. Early stopping — but when? In Grégoire Montavon, Geneviève B. Orr, and Klaus-Robert Müller, editors,Neural Networks: Tricks of the Trade. Springer, 2012. [28] Yuan Yao, Lorenzo Rosasco, and Andrea Caponnetto. On early stopping in gradient descent learning.Constructive Approximation, 26(2):289–315, 2007. [29] Ziwei Ji, Justin D. Li, and Matus Telgarsky. Early-stopped neural networks are consistent. InAdvances in Neural Information Processing Systems, 2021. [30] Maren Mahsereci, Lukas Balles, Christoph Lassner, and Philipp Hennig. Early stopping without a validation set. arXiv preprint arXiv:1703.09580, 2017. [31] Georg Ch Pflug. Non-asymptotic confidence bounds for stochastic approximation algorithms with constant step size.Monatshefte für Mathematik, 110(3-4):297–314, 1990. [32] Suqin Yuan, Runqi Lin, Lei Feng, Bo Han, and Tongliang Liu. Instance-dependent early stopping.arXiv preprint arXiv:2502.07547, 2025. 14 A PREPRINT [33] Manuel Vilares Ferro, Yerai Doval Mosquera, Francisco J. Ribadas Pena, and Victor M. Darriba Bilbao. Early stopping by correlating online indicators in neural networks.arXiv preprint arXiv:2402.02513, 2024. [34]",
    "[32] Suqin Yuan, Runqi Lin, Lei Feng, Bo Han, and Tongliang Liu. Instance-dependent early stopping.arXiv preprint arXiv:2502.07547, 2025. 14 A PREPRINT [33] Manuel Vilares Ferro, Yerai Doval Mosquera, Francisco J. Ribadas Pena, and Victor M. Darriba Bilbao. Early stopping by correlating online indicators in neural networks.arXiv preprint arXiv:2402.02513, 2024. [34] Yingbin Bai, Erkun Yang, Bo Han, Yanhua Yang, Jiatong Li, Yinian Mao, Gang Niu, and Tongliang Liu. Understanding and improving early stopping for learning with noisy labels.arXiv preprint arXiv:2106.15853, 2021. [35] Suqin Yuan, Lei Feng, and Tongliang Liu. Early stopping against label noise without validation data.arXiv preprint arXiv:2502.07551, 2025. [36] HuggingFace. Hugging face hub, 2020. AI model and dataset repository platform. [37] Marah Abdin, Jyoti Aneja, Harkirat Behl, Sébastien Bubeck, Ronen Eldan, Suriya Gunasekar, Michael Harrison, Russell J. Hewett, et al. Phi-4 technical report.arXiv preprint arXiv:2412.08905, 2024. [38] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models.arXiv preprint arXiv:2407.21783, 2024. [39] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b.arXiv preprint arXiv:2310.06825, 2023. [40] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions.arXiv preprint arXiv:1905.10044, 2019. [41] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. PIQA: Reasoning about physical commonsense in natural language. InProceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 7432–7439, 2020. [42] Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. SocialIQA: Commonsense reasoning about social interactions.arXiv preprint arXiv:1904.09728, 2019. [43] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence?arXiv preprint arXiv:1905.07830, 2019. [44] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering.arXiv preprint arXiv:1809.02789, 2018. [45] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try ARC, the AI2 reasoning challenge.arXiv preprint arXiv:1803.05457, 2018. [46] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale.arXiv preprint arXiv:1907.10641, 2019. [47] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, et al. PyTorch: An imperative style, high- performance deep learning library.arXiv preprint arXiv:1912.01703, 2019. [48] Kaichen Zhang, Bo Li, Peiyuan Zhang, Fanyi Pu, Joshua Adrian Cahyono, Kairui Hu, Shuai Liu, Yuanhan Zhang, Jingkang Yang, Chunyuan Li, and Ziwei Liu. LMMs-Eval: Reality check on the evaluation of large multimodal models, 2024. [49] Bo Li, Peiyuan Zhang, Kaichen Zhang, Fanyi Pu, Xinrun Du, Yuhao Dong, Haotian Liu, Yuanhan Zhang, Ge Zhang, Chunyuan Li, and Ziwei Liu. LMMs-Eval: Accelerating the development of large multimodal models, March 2024. [50]",
    "Li, and Ziwei Liu. LMMs-Eval: Reality check on the evaluation of large multimodal models, 2024. [49] Bo Li, Peiyuan Zhang, Kaichen Zhang, Fanyi Pu, Xinrun Du, Yuhao Dong, Haotian Liu, Yuanhan Zhang, Ge Zhang, Chunyuan Li, and Ziwei Liu. LMMs-Eval: Accelerating the development of large multimodal models, March 2024. [50] Drew A. Hudson and Christopher D. Manning. Gqa: A new dataset for real-world visual reasoning and compositional question answering, 2019. [51] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa matter: Elevating the role of image understanding in visual question answering, 2017. [52] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollar, and C. Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server, 2015. 15 A PREPRINT A Theoretical Analysis: Selection of Norm The choice of norm for gradient monitoring significantly impacts both computational efficiency and convergence detection reliability. For a gradient matrix G∈Rm×n, we consider four matrix norm candidates: element-wise L1(∥G∥ 1,1=P i,j|gij|), Frobenius ( ∥G∥F=qP i,jg2 ij), spectral ( ∥G∥ 2=σ max(G)), and subordinate L ∞ (∥G∥∞= max iP j|gij|). We select the element-wise L1norm based on computational efficiency and convergence properties. The L1norm requires only O(mn) operations through element-wise summation, avoiding expensive computations such as square roots (Frobenius) or singular value decomposition (spectral, O(mnmin(m, n)) ). Furthermore, the L1norm provides a stronger convergence criterion through the following theorem: Theorem 1.For any matrix A∈Rm×n, the elementwise L1norm provides an upper bound for commonly used matrix norms: ∥A∥ 2≤ ∥A∥ 1,1 (5) ∥A∥F≤ ∥A∥ 1,1 (6) ∥A∥∞≤ ∥A∥ 1,1 (7) ∥A∥ 1≤ ∥A∥ 1,1 (8) where∥A∥ 1= max jP i|aij|denotes the subordinateL 1norm (maximum column sum). Proof. For(5), the spectral norm satisfies ∥A∥ 2≤p ∥A∥ 1· ∥A∥∞by the well-known inequality for induced norms. Since∥A∥ 1= max jP i|aij| ≤P i,j|aij|=∥A∥ 1,1and similarly∥A∥ ∞≤ ∥A∥ 1,1, we have: ∥A∥ 2≤p ∥A∥ 1· ∥A∥∞≤q ∥A∥ 1,1· ∥A∥ 1,1=∥A∥ 1,1 For (6), note thata2 ij≤ |a ij| ·max k,l|akl| ≤ |a ij| · ∥A∥ 1,1for any element. Summing over all indices: ∥A∥2 F=X i,ja2 ij≤X i,j|aij| · ∥A∥ 1,1=∥A∥2 1,1 Therefore,∥A∥ F≤ ∥A∥ 1,1. For(7), the subordinate L ∞norm is the maximum row sum: ∥A∥∞= max 1≤i≤mPn j=1|aij|. Since the maximum row sum cannot exceed the sum of all elements:∥A∥ ∞≤Pm i=1Pn j=1|aij|=∥A∥ 1,1. For(8), similarly, the subordinate L1norm is the maximum column sum: ∥A∥ 1= max 1≤j≤nPm i=1|aij| ≤Pm i=1Pn j=1|aij|=∥A∥ 1,1. These relationships establish that monitoring the L1norm provides a universal upper bound for convergence detection. When∥G∥ 1,1< τ, we guarantee that all other standard matrix norms are also bounded by τ, ensuring robust convergence detection across multiple norm perspectives while maintaining linear computational complexity. B Convergence Properties We provide theoretical guarantees for the convergence of Algorithm 1. Our analysis demonstrates thatGradESconverges to a stationary point of the loss function while ensuring computational efficiency through adaptive parameter freezing. Theorem 2(Convergence of GradES).Consider Algorithm 1 applied to a loss function L:Rd→R that is L-smooth and lower bounded byL∗. With thresholdτ >0, the algorithm satisfies: 1. The loss sequence{L(t)}T t=1is non-increasing after warm",
    "thatGradESconverges to a stationary point of the loss function while ensuring computational efficiency through adaptive parameter freezing. Theorem 2(Convergence of GradES).Consider Algorithm 1 applied to a loss function L:Rd→R that is L-smooth and lower bounded byL∗. With thresholdτ >0, the algorithm satisfies: 1. The loss sequence{L(t)}T t=1is non-increasing after warm up 2. All frozen parameters satisfy∥∇ WL∥1,1< τat convergence 3. The algorithm terminates in finite time withmin t∈[T]∥∇L(t)∥ ≤τ 16 A PREPRINT Proof. Part 1: Monotonic Loss Decrease.After the warmup period ( t >0.05T ), the cosine schedule ensures ηtis decreasing. For any active parameterW/∈ Fat stept, the update rule yields: L(W t+1)≤ L(W t) +⟨∇L(W t),W t+1−W t⟩+L 2∥Wt+1−W t∥2(9) =L(W t)−η t∥∇L(W t)∥2+Lη2 t 2∥∇L(W t)∥2(10) For frozen parameters W∈ F , we have Wt+1=W t, thusL(W t+1) =L(W t). The cosine schedule with maximum valueη 0<2 Lensures: L(t+ 1)≤ L(t)−X W/∈Fηt\u0012 1−Lηt 2\u0013 ∥∇WL(t)∥2(11) Sinceη t≤η0<2 Lthroughout training, the loss sequence is non-increasing. Part 2: Frozen Parameters at Stationary Points.A parameter matrix Wis frozen at step tfwhen∥∇WL(tf)∥1,1< τ. Since frozen parameters receive no further updates: Wt=W tf∀t > t f (12) As shown in Part 1, the gradient magnitudes are non-increasing after warmup. Combined with the continuity of gradients: ∥∇WL(t)∥ 1,1≤ ∥∇WL(tf)∥1,1< τ∀t > t f>0.05T(13) This ensures that once a parameter is frozen, its gradient remains below the threshold. Part 3: Finite-Time Termination.Define the active parameter set at time tasAt={W:W/∈ F t}. The cardinality |At|is non-increasing since parameters can only transition from active to frozen. Under the cosine schedule, as t→T , we have ηt→0. In experiment, gradient magnitudes decrease monotonically after warmup. Therefore, there exists a finiteT∗< Tsuch that either: • All parameters satisfy∥∇ WL∥1,1< τand are frozen, or • The cosine schedule drivesη t∥∇L(t)∥< ϵfor arbitrarily smallϵ In both cases, the algorithm effectively converges withmin t∈[T]∥∇L(t)∥ ≤τ. Corollary 3.GradES achieves an ϵ-stationary point while potentially reducing computational cost by a factor proportional to |F|/d , where dis the total number of parameters. The cosine schedule ensures smooth convergence without oscillations in gradient magnitudes. This analysis establishes thatGradESmaintains convergence guarantees under the practical cosine learning rate schedule used in our experiments. The threshold τcontrols the trade-off between convergence accuracy and computational savings, while the 5% warmup period ensures stable gradient behavior before monitoring begins. C Hyperparameter Configuration We present comprehensive hyperparameter configurations to ensure experimental reproducibility. Tables 8–9 detail the training configurations for five language models—Qwen3-14B, Phi4-14B, Qwen3-0.6B, Llama-3.1-8B, and Mistral- 7B—under both full-parameter (FP) fine-tuning and Low-Rank Adaptation (LoRA). For all experiments, we employ early stopping with a validation loss threshold of δ= 0.0005 , patience of 3 epochs, and validation performed at 5% intervals throughout training. Tables 10–11 additionally present configurations and efficiency metrics for vision- language model experiments. 17 A PREPRINT Table 8: Hyperparameter configuration for language model fine-tuning experiments. FP denotes full-parameter fine- tuning. Model Method Learning Rate Batch Size Grad Accum Max Seq Len LoRA Rank Qwen3 14BFP 2e-5 1 4 4096 - LoRA 2e-4 16 4 4096 32 Phi4 14BFP 2e-5 1 4 4096 - LoRA 2e-4 16 4 4096 32",
    "configuration for language model fine-tuning experiments. FP denotes full-parameter fine- tuning. Model Method Learning Rate Batch Size Grad Accum Max Seq Len LoRA Rank Qwen3 14BFP 2e-5 1 4 4096 - LoRA 2e-4 16 4 4096 32 Phi4 14BFP 2e-5 1 4 4096 - LoRA 2e-4 16 4 4096 32 Qwen3 0.6BFP 2e-5 1 4 4096 - LoRA 2e-4 16 4 4096 32 Llama-3.1-8BFP 2e-5 1 4 4096 - LoRA 2e-4 16 4 4096 32 Mistral-7BFP 2e-5 1 4 4096 - LoRA 2e-4 16 4 4096 32 Table 9:GradESconvergence thresholds for language models. Grace period ratio ( α) determines the fraction of training steps before gradient monitoring begins. Threshold τdefines the gradient magnitude below which components are frozen. Full-parameter fine-tuning requires higher thresholds due to larger gradient magnitudes. Model Method Grace Period Ratio(α) Threshold Tau (τ) Qwen3 14BFP 0.55 6.387926 LoRA 0.55 0.025181 Phi4 14BFP 0.55 3.512882 LoRA 0.55 0.025181 Qwen3 0.6BFP 0.55 1.804456 LoRA 0.55 0.001183 Llama-3.1-8BFP 0.55 2.404167 LoRA 0.55 0.021637 Mistral-7BFP 0.55 2.726866 LoRA 0.55 0.029591 Table 10: Hyperparameter configuration for vision-language model experiments. Component-specific thresholds enable targeted convergence monitoring for vision and language transformers separately. Model Method VisionτLanguageτGrace Period (α) Qwen2.5-VL-7BLoRA 3.3 33.0 0.30 FP (4-bit) 0.13 0.09 0.30 NanoVLMTraining 0.30 6.00 0.28 Training (alt) 6.30 3.90 0.28 Table 11: Computational efficiency ofGradESon vision-language models. FLOPs and training time measurements demonstrate consistent improvements across different fine-tuning methods and precision settings. Model Method FLOPs Time (s) Speedup Qwen2.5-VL-7BLoRA (bf16) 2.80E+19 103,466.02 - LoRA+GradES (bf16) 2.52E+19 87,572.36 1.18× FP (4-bit) 2.27E+19 157,239.03 - FP+GradES (4-bit) 1.99E+19 134,686.05 1.17× NanoVLMTraining - 62,009.00 - Training+GradES - 34,669.00 1.79× 18 A PREPRINT D Code Availability We are committed to ensuring the reproducibility of our research. To facilitate this, we provide comprehensive resources: Implementation.Our complete implementation, including training scripts, evaluation pipelines, and gradient mon- itoring utilities, is publicly available at https://github.com/IXZZZ9/GradES . The repository includes detailed documentation, environment setup instructions, and scripts to reproduce all experimental results presented in this paper. Licensing.All code is released under the MIT License, promoting open scientific collaboration and industrial adoption. Model weights follow the original Qwen license terms. 19"
  ]
}