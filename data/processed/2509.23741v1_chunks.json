{
  "filename": "2509.23741v1.pdf",
  "total_chunks": 35,
  "text_length": 108964,
  "chunks": [
    "ResAD++: Towards Class Agnostic Anomaly Detection via Residual Feature Learning Xincheng Yao1,Chao Shi1,Muming Zhao2,Guangtao Zhai1,Chongyang Zhang1* 1*School of Information Science and Electronic Engineering, Shanghai Jiao Tong University, Shanghai, China. 2School of Information Science and Technology, Beijing Forestry University, Beijing, China. *Corresponding author(s). E-mail(s): sunny zhang@sjtu.edu.cn; Contributing authors: i-dover@sjtu.edu.cn; shichaostone@sjtu.edu.cn; mumingzhao@bjfu.edu.cn; zhaiguangtao@sjtu.edu.cn; Abstract This paper explores the problem of class-agnostic anomaly detection (AD), where the objective is to train one class-agnostic AD model that can generalize to detect anomalies in diverse new classes from different domains without any retraining or fine-tuning on the target data. When applied for new classes, the performance of current single- and multi-class AD methods is still unsatisfactory. One fundamental reason is that representation learning in existing methods is still class-related, namely, feature correlation. Feature correlation represents that the features of each class have many class- related attributes to the class, resulting in the model learned on one class relying on class-related representations, thereby being hard to adapt to other classes. To address this issue, we propose residual features and construct a simple but effective framework, termed ResAD. Our core insight is to learn the residual feature distribution rather than the initial feature distribution. Residual features are formed by matching and then subtracting normal reference features. In this way, we can effectively realize feature decorrelation. Even in new classes, the distribution of normal residual features would not remarkably shift from the learned distribution. In addition, we think that residual features still have one issue: scale correlation. Scale correlation refers that features of different classes may still have significant differences in scale, namely, the numerical value scales in the features of different classes may be remarkably different. To this end, we propose a feature hypersphere constraining approach, which learns to constrain initial normal residual features into a spatial hypersphere for enabling the feature scales of different classes as consistent as possible. Furthermore, we propose a novel log- barrier bidirectional contraction OCC loss and vector quantization based feature distribution matching module to enhance ResAD, leading to the improved version of ResAD (ResAD++). Comprehensive experiments on eight real-world AD datasets demonstrate that our ResAD++ can achieve remarkable AD results when directly used in new classes, outperforming state-of-the-art competing methods and also surpassing ResAD. The code is available at https://github.com/xcyao00/ResAD. Keywords:Class Agnostic Anomaly Detection, Residual Feature Learning, Feature Hypersphere Constraining 1 Introduction Anomaly detection (AD) is a widely studied machine learning task that aims to distinguish 1arXiv:2509.23741v1 [cs.CV] 28 Sep 2025 Class -Agnostic Anomaly Detection Class Agnostic ModelTraining on Known Classes Known Class 1 Known Class 2 Known Class NInput ImageReference Images New Class 1 New Class 2 New Class NReference ImagesInput Image Class Agnostic ModelInference on Diverse New Classes Fig. 1:Intuitive illustration of class-agnostic anomaly detection. We aim to train a class- agnostic AD model that can be directly applied to detect anomalies in new classes with only few- shot normal samples as reference. Class means the category of the object in the image. For industrial scenarios, it refers to the industrial product cate- gory,e.g., carpet, cable, etc. For medical analysis, it",
    "agnostic AD model that can be directly applied to detect anomalies in new classes with only few- shot normal samples as reference. Class means the category of the object in the image. For industrial scenarios, it refers to the industrial product cate- gory,e.g., carpet, cable, etc. For medical analysis, it refers to the body organ category,e.g., head, retina, etc. an instance that substantially deviates from the normal samples in the dataset. In computer vision, image-based anomaly detection typically also requires further localizing those anomalous image regions. In real-world applications, anomaly detection has received widespread attention in diverse domains, such as industrial defect inspec- tion [1 â€“3], video surveillance [4,5], medical lesion detection [6,7], and road anomaly detection [8,9]. Due to the scarcity of anomalies and diversity of normal classes (see Fig.1 for the meaning of class), current anomaly detection studies are mainly devoted to unsupervised one-for-one learn- ing,i.e., learning one specific AD model with only anomaly-free samples for each class. Most of the previously popular AD methods follow this paradigm, such as reconstruction-based meth- ods [10 â€“17], one-class-classification based methods [18â€“22], embedding-based methods [23 â€“27], and knowledge distillation approaches [28â€“33]. Despite these methods having achieved remark- able detection performance on various AD bench- marks, applying anomaly detection algorithms in real-world scenarios still confronts many challenges. A critical challenge is that there are usually diverse classes, and new classes are continually emerging. The previous single-class AD methods and also multi-class AD methods [34 â€“39] are still insufficientto satisfy the requirements of real-world applica- tions. Multi-class AD aims to learn one AD model for multiple classes simultaneously, but the learned model canâ€™t be directly applied to new classes and still requires retraining. In some privacy-sensitive scenarios, a more serious problem is that these methods become infeasible as retraining on the tar- get data is not allowed due to data privacy issues. Therefore, class-agnostic ability is a critical issue in the AD community, but it still hasnâ€™t been well studied in current AD literatures. To tackle the critical AD challenge, this paper aims to study an academy-valuable and application-required task: class-agnostic anomaly detection. As shown in Fig.1,one class-agnostic model is trained with sam- ples from multiple known classes, and the objective is to generalize to detect anomalies in new1classes. When used for new class detection, only few-shot new class normal samples are required, without retraining or any fine-tuning on the target data. Compared to multi-class AD, class-agnostic AD is more challenging because this task also requires one AD model for multiple classes, and even these classes are not trained. Nonetheless, solving such a task is quite chal- lenging. The performance of current single-class and also multi-class AD models usually drops dra- matically when used for new classes (see Tab.3). We attribute this phenomenon to one issue:class- related feature representation,a.k.a,feature correlation. Feature correlation represents that the features of each class generated by well-trained neural networks usually have many class-related attributes [40]. As class-related attributes are an abstract concept (hard to intuitively visualize), we explain them through specific images. For exam- ple, in the left part of",
    "issue:class- related feature representation,a.k.a,feature correlation. Feature correlation represents that the features of each class generated by well-trained neural networks usually have many class-related attributes [40]. As class-related attributes are an abstract concept (hard to intuitively visualize), we explain them through specific images. For exam- ple, in the left part of Fig.1, the weaving texture is one typical attribute for the known class 1. For the known class 2, three different colored insulators, sil- ver and brown wires, and blue cable cross-sections are typical attributes. For the known class 3, four counterclockwise rotating gears and bronze with black colored surface are typical attributes. After feature extraction, the class-related attributes are embedded into features, which are more abstract and hard to describe in language. We can only plot the feature distribution for visualization. As differ- ent classes have distinctive class-related attributes, 1In this paper, we call the classes in training as known classes, others are called as new (or novel, unknown) classes. 2 the feature distributions of different classes will be significantly different (please see Fig.4(a)). In a nutshell, class-related attributes are typical to the class and distinctive from other classes, rep- resenting the discriminative characteristics of the class. This can also explain the failure of previous methods when dealing with novel classes. Because previous methods didnâ€™t consider eliminating the significant â€œnormal-to-normalâ€ discrepancy. The â€œnormal-to-normalâ€ discrepancy causes normal in new classes will also be misjudged as abnormal (see Fig.8). To address the feature correlation issue, our core insight is to utilize residual features to effec- tively realize feature decorrelation. The residual operation includes two steps: matching and sub- tracting. We will first match the nearest normal reference feature to each input feature. As class- related attributes can also be embedded in normal reference features2, the closer the two features are, usually the more similar the embedded attributes will be. Then, the matching process at the patch level can be considered as matching the most sim- ilar class-related attributes to each input feature. Therefore, by further subtracting, it is highly prob- abilistic that the class-related components in the initial features will be mutually eliminated. From the perspective of feature distribution, it can be imagined that residual features will be distributed in a relatively fixed origin-centered region (see Fig.2 and Fig.4(b)). As shown in Fig.2, the main merit of normal residual features is: even in new classes, the distribution of normal residual fea- tures would not remarkably shift from the learned distribution. Regardless of classes, larger residual values are expected for abnormal features than normal features. Therefore, we think that residual features can be regarded as class-invariant3rep- resentations compared to the significantly variant initial features. Based on residual features, we propose a sim- ple but effective class-agnostic AD framework, termed ResAD (i.e.,Residual Feature Learn- ing based Class-AgnosticAnomalyDetection). ResAD is based on one key insight: residual fea- ture learning, and consists of two key designs: 2For example, in Fig.1, each normal reference sample of the known class 2 will also contain three insulators (class-related) with different colors, otherwise itâ€™s not normal. 3Strictly speaking, the residual features are",
    "Class-AgnosticAnomalyDetection). ResAD is based on one key insight: residual fea- ture learning, and consists of two key designs: 2For example, in Fig.1, each normal reference sample of the known class 2 will also contain three insulators (class-related) with different colors, otherwise itâ€™s not normal. 3Strictly speaking, the residual features are not fully invari- ant, while the variations are significantly smaller. Initial Feature Distribution Residual Feature DistributionKnown Class 1Known Class 2Known Class 3 New Class 1Decision BoundaryKnown Classes New Classes Residual Feature LearningMatchingNew Class 2Reference Features Normal Features Abnormal FeaturesFig. 2:Conceptual illustration of residual features. The residual feature space has fewer vari- ations compared to the initial feature space. The decision boundary of the residual feature distribu- tion can more effectively distinguish anomalies in new classes, rather than treating most features of new classes as anomalies. feature hypersphere constraining and feature distri- bution estimating. First, we employ a pre-trained feature extractor to generate normal reference fea- tures from few-shot normal reference samples. Each input feature will match the nearest normal refer- ence feature and subtract it to form the residual feature. Second, we think that residual features still have one issue:class-related scale4variation, a.k.a, scale correlation. Scale correlation refers that features of different classes may also have sig- nificant differences in scale, namely, the numerical value scales in the features of different classes may be remarkably different. This can lead to difficulty in obtaining a unified normal-abnormal decision boundary for different classes,i.e., the scales of decision boundaries in different classes may be sig- nificantly different, a good decision boundary in one class may be poor in another class (please see detailed explanations in Sec.3.2). To address the scale correlation issue, we take the idea from one- class-classification learning [20,41] and propose a feature hypersphere constraining approach to real- ize scale decorrelation. In the scale decorrelation module, the network will learn to constrain initial normal residual features into a spatial hypersphere 4Note that we use the â€œscaleâ€ to represent the size of numer- ical values in features or the size of feature norms, rather than object scale. 3 for enabling the feature scales of different classes as consistent as possible. Third, with the hypersphere-constrained resid- ual features, we can easily utilize a feature distri- bution estimator [42] to learn and estimate the normal residual feature distribution, anomalies can be recognized as out-of-distribution. Further- more, to further reduce distribution mismatch between testing feature distribution and training feature distribution, we propose a vector quanti- zation (VQ) based feature distribution matching approach to inject training distribution into the test samples. In this way, the residual features of new classes are more likely to match certain learned representations, thus being more accurately discriminated. For new classes, as the residual fea- tures have fewer variations or are covered by the learned distribution, the whole framework is more class-agnostic. The idea of residual features was initially intro- duced in our recent NeurIPS 2024 spotlight paper [43]. In this work, we mainly make four improve- ments to ResAD: (1) We rethink the causes of why existing methods are poorly class-agnostic and propose",
    "learned distribution, the whole framework is more class-agnostic. The idea of residual features was initially intro- duced in our recent NeurIPS 2024 spotlight paper [43]. In this work, we mainly make four improve- ments to ResAD: (1) We rethink the causes of why existing methods are poorly class-agnostic and propose a more essential explanation: feature cor- relation. The deeper explanation can better inspire the solution: decorrelation. More importantly, we further numerically evaluate the effectiveness of our method for achieving feature decorrelation and scale decorrelation by statistical values (see Tab.1 and Tab.2). (2) In the feature hypersphere con- straining module, we propose a novel log-barrier bidirectional contraction OCC loss, which is more robust and can achieve better results. (3) In the feature distribution estimating module, we modify the maximum likelihood loss and employ a new loss for optimization. This improvement provides us with a new anomaly scoring way, which is better for AD results. (4) We propose a novel vector quanti- zation based feature distribution matching module, which can make the testing distribution more con- sistent with the training distribution. Aside from these method changes, we conduct more compre- hensive experiments, including comparison on more datasets, adding a new 8-shot setting, comparison with full-shot trained models, and more extensive ablation studies, to clearly illustrate the advantage of our method. The improved method is called as ResAD++ for distinguishing from the name inthe conference paper. Overall, the performance of ResAD++ is better than ResAD in [43]. In summary, we make the following main contributions: 1. We study the class-agnostic anomaly detec- tion task to evaluate the class-agnostic ability of AD methods in identifying anomalies from novel classes without retraining or fine-tuning. To the best of our knowledge, relevant research works about class-agnostic AD are still relatively lacking in the AD community. 2. Based on our previous work, ResAD [43], we further extend the original version of ResAD to ResAD++ with novel improvements, including log-barrier bidirectional contraction OCC loss, VQ- based feature distribution matching module, etc. We think that the idea of residual feature learning is not specific to the proposed framework and can provide inspiration for subsequent class-agnostic AD works. 3. Comprehensive experiments on eight AD datasets are performed to evaluate our ResAD++â€™s class-generalization ability. The results show that our ResAD++ can achieve remarkable AD results when directly used in new classes, outperform- ing state-of-the-art methods and also surpassing ResAD. With only 8-shot normal samples as reference, ResAD++ can achieve 98.6% image- level AUROC and 96.7% pixel-level AUROC on MVTecAD. 2 Related Work Single-class Anomaly Detection.Most AD methods follow the one-for-one paradigm. Reconstruction-based methodsare the most popu- lar AD methods. These methods hold the insight that models trained by normal samples would fail in abnormal image regions. Many previous works utilize auto-encoders [10 â€“12], variational auto-encoders [44] and generative adversarial networks [13, 14] to encode and reconstruct nor- mal data. Other methods [15, 16, 36] accomplish anomaly detection by inpainting, where image patches are masked randomly. Then, neural net- works are trained to predict the masked patches. Distillation-based AD methods [28] can also be",
    "variational auto-encoders [44] and generative adversarial networks [13, 14] to encode and reconstruct nor- mal data. Other methods [15, 16, 36] accomplish anomaly detection by inpainting, where image patches are masked randomly. Then, neural net- works are trained to predict the masked patches. Distillation-based AD methods [28] can also be considered as belonging to the reconstruction type. These methods train student networks to recon- struct the representations of teacher networks on normal samples, and the assumption is that the 4 student would fail in abnormal features. Recent works mainly focus on feature pyramid [29, 30], reverse distillation [31, 32], and asymmetric distillation [33]. OCC-based methodsbelong to another type of classical AD modeling methods. The earliest works are mainly to extend the OCC (i.e., one- class-classification) models such as OC-SVM [45] or SVDD [18,41] for anomaly detection. Recently, in [21], a patch-based SVDD that contains multiple cores rather than a single core in DeepSVDD [18] is proposed to enable anomaly localization. Deep- SAD [18] is the first semi-supervised OCC-based AD framework that extends DeepSVDD to utilize a few abnormal samples during training. FCDD pro- posed in [20] further extends DeepSAD based on the pseudo-Huber loss in [46] to support anomaly localization. In FCDD [20], anomaly maps are generated by the proposed Fully Convolutional Data Description combined with receptive field upsampling. In [22], the authors further extend the PatchSVDD [21] model by the proposed multi-scale patch-based representation learning method. Embedding-based methodsrecently show state- of-the-art performance. These methods mainly rely on good feature representations and assume that abnormal features are usually far from the nor- mal clusters. Most superior methods [23,24,47 â€“49] utilize ImageNet pre-trained networks for feature extraction. PaDiM [23] extracts pre-trained fea- tures to model Multivariate Gaussian distribution and then utilizes Mahalanobis distance to mea- sure the anomaly scores. PatchCore [24] extends on this line by utilizing locally aggregated features and introducing a maximally representative mem- ory bank of normal features. However, industrial images generally have an obvious distribution shift from ImageNet. To better account for the distribu- tion shift, subsequent adaptations should be done. The normalizing flow based methods [25 â€“27,50] are proposed to transform the pre-trained feature dis- tribution into a latent Gaussian distribution, and thus can better learn the normal data distribution. Multi-class Anomaly Detection.Recently, some researchers have attempted to jump out from the one-for-one paradigm and studied how to design one multi-class AD model to accomplish anomaly detection for multiple classes simultane- ously. UniAD [34], OmniAL [35], and HVQ-Trans [51] are early proposed methods for this new direc- tion. UniAD is a transformer-based reconstructionmodel and mainly based on three improvements, layer-wise query decoder,neighbor masked atten- tion, andfeature jittering strategy, to address the â€œidentical shortcutâ€ issue to achieve multi-class AD. OmniAL is a unified CNN framework with anomaly synthesis, reconstruction, and localization improvements. To prevent identical reconstruction, OmniAL trains the model with panel-guided syn- thetic anomaly data rather than directly using normal data. HVQ-Trans follows UniAD, which mitigates the shortcut learning issue via a vector quantization mechanism. Unlike the above three reconstruction-based methods, HGAD [37] is a nor- malizing flow",
    "and localization improvements. To prevent identical reconstruction, OmniAL trains the model with panel-guided syn- thetic anomaly data rather than directly using normal data. HVQ-Trans follows UniAD, which mitigates the shortcut learning issue via a vector quantization mechanism. Unlike the above three reconstruction-based methods, HGAD [37] is a nor- malizing flow (NF) based AD model. The authors find that popular NF-based AD methods may fall into a â€œhomogeneous mappingâ€ issue when used for multi-class AD, and correspondingly propose a novel hierarchical Gaussian mixture normalizing flow modeling method to address this issue. DiAD [38], RLR [52], MambaAD [39], Dino- maly [53], and INP-Former [54] are more recent works. DiAD investigates a multi-class AD frame- work based on diffusion models, introducing a semantic-guided network to ensure the consistency of reconstructed image semantics. RLR utilizes learnable reference representations to compel the model to learn normal feature patterns explicitly, thereby preventing the model from succumbing to the shortcut issue. MambaAD is the first explo- ration work to employ state space models (SSM) to address multi-class AD, introducing a LSS module with hybrid state space (HSS) blocks and multi- kernel convolutions to effectively capture both long-range and local information. In Dinomaly, the authors follow the â€œless is moreâ€ philosophy to propose a pure Transformer-based AD framework without relying on complex designs or specialized tricks. To avoid the student overly mimicking the teacher, Dinomaly employs three simple compo- nents:noisy bottleneck,linear attention, andloose reconstructionto weaken the overfitting of the net- work during training. INP-Former proposes the intrinsic normal prototypes (INPs) concept, which are adaptively extracted from the test image itself. These INPs can provide more concise and well- aligned prototypes to the anomalies than those learned from training data. With the guidance of INPs, the decoder can more accurately reconstruct normal regions and effectively suppress the recon- struction of anomalous regions. Furthermore, the 5 improved method, INP-Former++ [55], utilizes our residual feature learning to amplify the discrim- inative boundary between normal and abnormal features for further performance improvement. Unified Anomaly Detection with Few- shot Learning.We think that few-shot AD methods are also valuable efforts to achieve class-agnostic anomaly detection. Few-shot AD methods can be regarded as indirectly achieving class-agnostic by only utilizing few-shot normal samples to construct AD models. Distance-based approaches such as SPADE [47], PaDiM [23] and PatchCore [24] can be adapted to address few-shot AD by only making use of few-shot samples to calculate distance-based anomaly scores without training. RegAD [56] proposes to train a feature registration network to align input images and follows [23] to model Multivariate Gaussian dis- tribution with few-shot normal samples. Recently, the CLIP-based AD methods, including WinCLIP [57] and AprilGAN [58], show better few-shot and even zero-shot AD performance. WinCLIP proposes multi-scale aggregation to construct small- scale and mid-scale feature memories. AprilGAN introduces extra linear layers to map image fea- tures to the text feature space. They both employ a text prompt ensemble strategy to obtain the language-guided anomaly map. AnoVL [59] fur- ther introduces to replace the QKV attention with V-V attention for enhancing the local visual seman- tics. Subsequently, many works",
    "extra linear layers to map image fea- tures to the text feature space. They both employ a text prompt ensemble strategy to obtain the language-guided anomaly map. AnoVL [59] fur- ther introduces to replace the QKV attention with V-V attention for enhancing the local visual seman- tics. Subsequently, many works actively attempt to accomplish zero-shot anomaly detection based on the CLIP model [60], such as AnomalyCLIP [61], CLIP-AD [62], ClipSAM [63], and FiLo [64]. Although different from the general definition of zero-shot AD, in MuSc [65], the authors also call their method as zero-shot AD, which detects anomalies by directly excavating normal informa- tion from unlabeled test images, without the need for normal training images. Different from class-agnostic AD, few-shot AD mainly focuses on how to effectively utilize few-shot normal samples to construct AD models. Some dedicated modules may be introduced to handle the few-shot normal samples. These methods still need to remodel or retrain in new classes based on few-shot normal samples,e.g., PatchCore needs to reconstruct the coreset, and RegAD needs to remodel the Multivariate Gaussian distribution for new classes. Therefore, few-shot AD methods are still class-reliant essentially. The CLIP-basedmethods can be seen as class-agnostic, as these methods can obtain anomaly maps by aligning vision features with text features without remod- eling in new classes. However, they rely on the visual-language comprehension ability of CLIP and handcrafted text prompts about anomalies (new classes may require new specific text prompts), making them difficult to generalize to anomalies in diverse classes. They may fail to work well when the text prompts cannot capture the desired anomaly semantics, especially for these CLIP-based zero- shot AD methods. Because these methods heavily rely on language descriptions of anomalies and donâ€™t effectively utilize normal samples. However, normal information is crucial for detecting anoma- lies, and few-shot normal samples are easy to acquire. More recently, the idea of in-context residuals in InCTRL [66] is very similar to ours. But our method has obvious differences with InCTRL in the definition and utilization of residuals. (1) The definition of residuals in InCTRL is based on fea- ture distances. We think that residual distances in InCTRL can limit the range of residual rep- resentation (as the cosine similarity is in [-1,1]). In contrast, our residual features donâ€™t limit the range of residual representation and can retain the feature properties. In high-dimensional feature space, we can establish better decision boundaries between normal and abnormal. (2) InCTRL is to train a binary classification network based on residual distance maps, which can only output an image-level anomaly score. Our method is to learn the distribution of residual features, an anomaly score can be estimated for each feature, thus can be used to locate anomalies. (3) Our residual fea- tures can be easily applied to other AD methods by simply replacing the original features, while the residuals in InCTRL are more dependent on the whole method and not easily applicable to other AD methods. (4) Our ResAD++ can remarkably surpass InCTRL on multiple datasets (see Tab.3). 3 Method Problem Statement.The objective of class- agnostic anomaly",
    "other AD methods by simply replacing the original features, while the residuals in InCTRL are more dependent on the whole method and not easily applicable to other AD methods. (4) Our ResAD++ can remarkably surpass InCTRL on multiple datasets (see Tab.3). 3 Method Problem Statement.The objective of class- agnostic anomaly detection is to train one class- agnostic AD model that can still work well for detecting anomalies on novel classes from diverse application domains without any retraining or fine-tuning on the target data. Thus, the classes 6 Reference Feature Pool ð‘ð‘“ð‘ ð»ð‘ŠÃ—ð¶ Few-shot Reference ImagesInput ImageInitial Features Residual Features Constrained Features Training /Testing Testing Only Feature ExtractorMatching ä¸€Residual Feature Generating Feature Extractor Codebook Embeddings ð¾Ã—ð¶ðœˆ1 ðœˆð¾âˆ’2ðœˆð¾âˆ’1 ðœˆ2ðœˆ3 ðœˆð¾Learn CodebookNearest Neighbor LookupFDM Distribution Estimatorð»ð‘ŠÃ—ð¶ ð»ð‘ŠÃ—ð¶ ð»ð‘ŠÃ—ð¶ â„’ð‘Žð‘–âˆ’ð‘œð‘ð‘ â„’ð‘šð‘™+â„’ð‘“ð‘œð‘ð‘Žð‘™ð‘1ð‘2ð‘3ð‘âˆ’3ð‘âˆ’2ð‘âˆ’1Feature Constraintor Subtracting Anomaly MapFig. 3:Framework overview. First, few-shot normal reference samples are fed into a pre-trainedFeature Extractorto obtain normal reference features. These features are stored in reference feature pools, we only show one pool. For each input image, we utilize the sameFeature Extractorto extract multi-layer feature maps, the figure only shows one layer. Each initial feature will match the nearest normal reference feature and subtract it to form the residual feature. The codebook is learned during training by vector quantization (VQ) technique to effectively represent the training residual feature distribution. TheFeature Distribution Matching (FDM)module is only utilized during testing for further making the testing data distribution more consistent with the training data distribution (see Sec.3.3). Then, aFeature Constraintoris utilized to transform the normal residual features into a constrained spatial hypersphere. Finally, we employ a normalizing flow model as theFeature Distribution Estimatorto learn and estimate the residual feature distribution. During testing, theFeature Distribution Estimatorcan be used to estimate log-likelihoods for input features, anomalies usually have smaller log-likelihoods, thereby being located. Note that training samples and testing samples belong to different classes. in the training set are assumed to be different from the classes in the test sets. Formally, let Itrain =Inâˆª Iabe an auxiliary training dataset with normal images and some anomalies (i.e., anomalies that exist in the training set should also be effectively utilized), where In={In i}N0 i=1and Ia={Ia j}M0 j=1indicate the collection of normal samples and abnormal samples. As for testing, the model evaluation is conducted on a collection of other AD datasets ( T={Itest 1,Itest 2, . . . ,Itest T}) except the training dataset. The classes in the test set are drawn from unknown classes Cuthat are different from the known classes Ckin the training set. Then the goal is to learn a class-agnostic model M:I â†’R that is trained with known classes Ck and can directly adapt to unknown classes Cuwith only few-shot (e.g., 4) normal samples as reference. Please note that the reference samples used dur- ing testing are not available in any way during the training of the class-agnostic AD model. Overview.Towards class-agnostic anomaly detection, we innovatively propose the residual feature learning approach and construct a simple framework, ResAD++. The framework is illus- trated in Fig.3. The detailed workflow descriptionof ResAD++ is in the caption of Fig.3. Our proposed residual",
    "in any way during the training of the class-agnostic AD model. Overview.Towards class-agnostic anomaly detection, we innovatively propose the residual feature learning approach and construct a simple framework, ResAD++. The framework is illus- trated in Fig.3. The detailed workflow descriptionof ResAD++ is in the caption of Fig.3. Our proposed residual feature learning approach con- sists of four parts:residual feature generating, feature hypersphere constraining,feature distribu- tion estimating, andVQ-based feature distribution matching. These modules will be described below in sequence. 3.1 Residual Feature Generating Residual feature learning is our core insight for solving class-agnostic anomaly detection. In this subsection, we describe how to generate residual features. For any input image IiâˆˆRH0Ã—W0Ã—3, we follow the common practice of previous AD methods to employ a pre-trained feature extrac- tion network Ï•to extract features from different layers. Formally, we define Las the total num- ber of layers for use. The feature map from layer lâˆˆ {1,2, . . . , L} is denoted as Ï•l(Ii)âˆˆRHlÃ—WlÃ—Cl, where Hl,Wl, and Clare the height, width, and channel dimension of the feature map. For a fea- ture vector xl h,w=Ï•l(Ii)h,wâˆˆRClat layer land location ( h, w), we will match it with the nearest normal reference feature from the corresponding 7 reference feature pool, and then convert it into the residual feature. The details are described in the following: Reference Feature Pools.The reference feature pools are utilized to store some normal features as reference. For new classes, we will provide few-shot normal samples (i.e., randomly selected and then fixed) as reference. The pre- trained network Ï•will also extract multi-layer features for these normal reference images, then the extracted features are sent into the feature pools as reference features. For lth layer, the lth refer- ence feature pool is composed of Pl={xl,i h,w|hâˆˆ {1, . . . , H l}, wâˆˆ { 1, . . . , W l}, lâˆˆ { 1, . . . , L}, iâˆˆ {1, . . . , N fs}}, where idenotes the ith normal ref- erence sample, the Nfsis the number of normal reference samples. Residual Features.For each initial feature xl h,w, we can search the nearest nominal reference feature xâˆ— n=argminxâˆˆPl||xâˆ’xl h,w||2from the lth reference feature pool Pl. Then, we define the residual representation of xl h,wto its closest normal reference feature as: xl,r h,w=xl h,wâˆ’xâˆ— n (1) Why are residual features more class- agnostic compared to initial features? We think that residual features can effectively realize feature decorrelation (they are generated by match- ing and then subtracting). From the principles of representation learning, we know that features of each class generated by well-trained neural net- works usually have many class-related attributes to the class for distinguishing from other classes [40]. The â€œclass-relatedâ€ means these attributes are typ- ical to the class and distinctive from other classes, representing the discriminative characteristics of the class (please see more explanations in the intro- duction). Thus, features from different classes are usually located in different feature domains [67]. As class-related attributes can also be embedded in normal reference features, the closer two fea- tures are, usually the more similar",
    "classes, representing the discriminative characteristics of the class (please see more explanations in the intro- duction). Thus, features from different classes are usually located in different feature domains [67]. As class-related attributes can also be embedded in normal reference features, the closer two fea- tures are, usually the more similar the embedded attributes will be. Then, the matching process at the patch level can be seen as matching the most similar class-related attributes to each query feature. Therefore, by subtracting, it is highly probabilistic that the class-related components in the initial features will be mutually eliminated. It can be imagined that residual features will bedistributed in a relatively fixed region centered around the origin. A prominent merit of residual features for class-agnostic anomaly detection is that they have fewer variations in unknown classes. Therefore, even in new classes, the distribution of normal residual features would not remarkably shift from the learned distribution. In Fig.4, the t-SNE visualization of initial feature and residual feature distributions also empirically validates our above explanations. In addition, residual features are also beneficial to highlight the discrepancy between normals and anomalies, as larger residu- als5are more likely to be anomalies than normal features. The normal features are usually closer to reference features compared to abnormal features, thus residuals of normal features will be closer to 0. To empirically validate this, we calculate the average absolute values of normal feature residu- als and abnormal feature residuals. The calculated average absolute values are shown in Tab.1. (a) Initial Feature Distribution (b) Residual Feature Distributionknown classe s newclasses different classesknown classe s newclasses different classes Fig. 4:Feature t-SNE visualization on the MVTecAD dataset. (a) In the initial feature space, the features from different classes are signif- icantly different. (b) In the residual feature space, even the residual feature distribution of unknown classes would not remarkably shift from the known distribution. Note that in (a) and (b), we only show normal features and use different colors to represent different classes. Finally, we attempt to numerically evaluate the effectiveness of residual operation for fea- ture decorrelation. Specifically, we calculate the Kurtosis of the multi-class feature distribution. For a multi-class distribution, Kurtosis can mea- sure the variations of the distribution. When the class feature correlation is strong, features across 5The word â€œresidualsâ€ is used to represent the specific residual values in residual features. For example, for a Cdimen- sional residual feature, it contains Cresidual values (a.k.a, C residuals). 8 Table 1:Statistics of the initial feature distribution and residual feature distribution. â€œAbsâ€ means the average absolute values of feature residuals. Anomalies are more likely to have larger residual values. Dataset MVTecAD VisA BTAD MVTec3D Statistics Kurtosisâ†‘Kurtosisâ†‘Kurtosisâ†‘Kurtosisâ†‘ Initial Features 0.261 0.757 0.190 0.047 Residual Features 1.679 1.313 0.518 0.552 Statistics Abs Abs Abs Abs Normal Residual Features 0.077 0.068 0.107 0.110 Abnormal Residual Features 0.392 0.365 0.335 0.409 classes will vary significantly, the multi-class fea- ture distribution will be multi-modal, resulting in a small Kurtosis. When the class feature correla- tion is weak, the multi-class feature distribution tends to be single-modal, thereby the Kurtosis is large.",
    "Features 0.077 0.068 0.107 0.110 Abnormal Residual Features 0.392 0.365 0.335 0.409 classes will vary significantly, the multi-class fea- ture distribution will be multi-modal, resulting in a small Kurtosis. When the class feature correla- tion is weak, the multi-class feature distribution tends to be single-modal, thereby the Kurtosis is large. The calculated Kurtosis values are shown in Tab.1. The results validate that feature correlation can be effectively decreased in residual features, namely, residual operation can effectively eliminate class-related attributes in features. 3.2 Feature Hypersphere Constraining Residual features can effectively realize feature decorrelation, but they still have one issue: class- related scale variation,a.k.a, scale correlation. Scale correlation refers that features of different classes may still have significant differences in scale, namely, the numerical value scales in the features of different classes may be remarkably different. This can lead to difficulty in obtaining a unified normal-abnormal decision boundary for different classes,i.e., the scales of decision boundaries in different classes may be significantly different, a good decision boundary in one class may be poor in another class. To empirically show the differ- ences in feature scales among different classes, we plot the boxplot of feature scales of those classes from the MVTecAD dataset in Fig.5. In order to further reduce variations in resid- ual features and also maintain the consistency in feature scales among different classes (scale decorrelation), we take the idea from one-class- classification (OCC) learning [18,20] and propose aFeature Constraintorto constrain the initial normal residual features to a spatial hypersphere. Fig. 5:Boxplot of feature scales of residual features on the MVTecAD dataset. We utilize L2 norm as the statistical value of feature scale. From the perspective of feature scales to distin- guish normals and anomalies, it can be seen that a good decision boundary in one class may be poor in another class. TheFeature Constraintor Ïˆ(Â·;W) projects the ini- tial residual feature xi, iâˆˆ { 1,2, . . . , N} , where N6is the number of normal residual features, to the constrained feature xâ€² iasxâ€² i=Ïˆ(xi;W). This is the typical Hypersphere Contraction optimiza- tion problem [41], where the goal is to seek a hypersphere that can enclose the transformed data around a given center. The general objective of this problem is formulated as follows: Dis(Ïˆ(x i;W),c)â‰¤R, i= 1, . . . , N(2) wherecis the center of the hypersphere, Ris the radius of the hypersphere, and Dis(Â·,Â·) represents any distance function such as the p-norm distance, 6For symbol simplicity, we use Nto represent the number of normal residual features in one feature layer. Please note that our method is based on multi-layer features, with aFeature Constraintoron each layer. 9 the cosine distance, and so on. Specifically, we employ pseudo-Huber loss [68] to measure the dis- tance. According to DeepSVDD [41], a good way to set the centercis to initialize it to the mean of transformed features,i.e,c=1 NPN i=1Ïˆ(xi;W). In our method, as the residual features are dis- tributed around the origin, we simply fix the center as 0, which is also easier to implement. With the center set to 0, the distance",
    "way to set the centercis to initialize it to the mean of transformed features,i.e,c=1 NPN i=1Ïˆ(xi;W). In our method, as the residual features are dis- tributed around the origin, we simply fix the center as 0, which is also easier to implement. With the center set to 0, the distance is Dis(Ïˆ(xi;W),0) =p ||Ïˆ(x i;W)||2 2+ 1âˆ’ 1, which is a more robust dis- tance measure that interpolates from quadratic to linear penalization [68]. However, the inequality in Eq.(2) is not a feasible optimization formula (i.e., no objectives to maximize or minimize). Then, we propose that the ideal optimization goal of Eq.(2) is to make all features contracted into the hyper- sphere, and for features that are already within the hypersphere, there is no need to further shrink towards the center to avoid mode collapse. Then, the ideal optimization objective can be formulated as: min W1 NNX i=1I(si) +Î» 2KX k=1||Wk||2 F (3) where Î»is the regularization parameter to reduce over-fitting ( Î»is set to 0 .001), andW kis the parameters of kth layer of theFeature Constraintor withW={W1, . . . ,WK}. And si=Diâˆ’Rmeans the distance to the hypersphere, where Direfers to Dis(Ïˆ(xi;W),0),I(si) is the indicator function of non-positive real numbers, which is defined as: I(s) =( 0, sâ‰¤0, âˆž, s >0.(4) I(s) tends to infinity when the transformed fea- tureÏˆ(xi;W) falls outside the hypersphere (c , R). Thus, minimizing Eq.(3) can enforce the network to transform all features to fall inside the hypersphere (c, R). However, Eq.(3) is difficult to directly opti- mize as the I(s) is not differentiable. To address this issue, we take insight from barrier methods [69] in convex optimization and employ the logarith- mic barrier function to smoothly approximate the indicator functionI (s), that is Ë†I(s) =âˆ’1 tlog(âˆ’s), s <0 (5)where tcontrols the approximation precision of the logarithmic barrier function Ë†I(s) to the indi- cator function I(s). Same as I(s),Ë†I(s) is also convex and non-decreasing function. Especially, it increases to âˆžsmoothly as sincreases to 0, and according to the convention, we can take âˆžwhen sâ‰¥ 0. However, different from I(s),Ë†I(s) is a differentiable closed function, moreover, the sam- ples closer to the boundary of the hypersphere will obtain higher penalties as well as larger gradient values. In this way, we can pay more attention to the samples around the boundary, while those sam- ples away from the boundary will not be further enforced to contract to the center, due to small losses and gradient values. With tincreasing, the approximation precision will gradually increase. However, in Eq.(5), when sâ‰¥0, the value of Ë†I(s) is not well defined. But s >0 indicates that the features are outside the boundary, we need to pay more attention to these features and contract them into the boundary. To this end, we further relax the limitation of Ë†I(s) and propose the log-barrier OCC loss, which can be expressed as: min Wâˆ’1 NtNX i=1logsig(âˆ’s i)Â·esi+Î» 2KX k=1||Wk||2 F(6) where logsig presents logarithmic sigmoid function. By employing the sigmoid function, we can ensure thatscan take values in the range ( âˆ’âˆž,âˆž ). Then, we denote",
    "relax the limitation of Ë†I(s) and propose the log-barrier OCC loss, which can be expressed as: min Wâˆ’1 NtNX i=1logsig(âˆ’s i)Â·esi+Î» 2KX k=1||Wk||2 F(6) where logsig presents logarithmic sigmoid function. By employing the sigmoid function, we can ensure thatscan take values in the range ( âˆ’âˆž,âˆž ). Then, we denote J(W) =âˆ’1 NtPN i=1logsig (âˆ’si)Â·esiand Ïƒi=sigmoid (âˆ’si), the gradient of J(W) with respect toWcan be derived as: âˆ‚J âˆ‚W=âˆ’1 NtNX i=1\u00121 Ïƒiâˆ‚Ïƒi âˆ‚siâˆ‚si âˆ‚Wesi+ logÏƒ iesiâˆ‚si âˆ‚W\u0013 =1 NtNX i=1\u0012esi 1 + esiâˆ‚Di âˆ‚Wâˆ’logÏƒ iesiâˆ‚Di âˆ‚W\u0013 =1 NtNX i=1\u0012 (1âˆ’Ïƒ i)âˆ‚Di âˆ‚Wâˆ’logÏƒ iesiâˆ‚Di âˆ‚W\u0013 (7) According to the gradient formula in Eq.(7), we can better understand our log-barrier OCC loss in Eq.(6), which can adaptively tune attention to different samples. The Ïƒican be explained as the probability that the feature belongs to the normal distribution. Thus, the features close to the center 10 have large probabilities (i.e., Ïƒiâ†’1) and thus small 1 âˆ’Ïƒiandâˆ’logÏƒ i, while the features falling on the boundary and outside the boundary have small Ïƒiâ†’0 and large âˆ’logÏƒ i(thus large gradient values). It is obvious that the objective in Eq.(6) will assign larger gradients to the margin samples (i.e., close or outside the boundary) for better contraction. Bidirectional Hypersphere Contraction. Although the goal of Optimization (6) is to pro- mote most normal features close to the origin and distribute compactly around the origin. However, empirical exploration has found that in a high- dimensional space, most samples are far away from the origin [70]. This phenomenon is calledsoap- bubble[70], which means the high-dimensional data may be more likely to be located in the interval region of two hyperspheres instead of a hypersphere. In Fig.6, we sample from Gaussian distribution N(0,Id) and show the histogram of distances to the center. It can be seen that the Gaussian is in the thin shell within a distance from the origin when the dimension is large. Moreover, the higher the dimensionality of the data, the more sampled instances are far from the center. Thesoap-bubble phenomenon is formally proven by the following proposition (cf.Lemma 1 of [71]). Fig. 6:Soap-bubblephenomenon in the high- dimensional data. All data are drawn from N(0,Id), and we show the empirical histogram of distances to the center 0 of 104samples. Proposition 1.Suppose z1, z2, . . . , z nare sam- pled from N(0,Id)independently. Then, for any ziand alltâ‰¥0, the following inequality hold. Ph ||zi|| â‰¥q dâˆ’2âˆš dti â‰¥1âˆ’eâˆ’t(8) The proposition shows that when the dimension is high, each ziis outside the hypersphere of radiusRâ€²:=p dâˆ’2âˆš dtwith a probability of at least 1 âˆ’ eâˆ’t. When Râ€²is closer to R, normal data are more likely to be away from the center. For example, when Ris 1 and dis 256, the proposition gives the probability that ziis outside the hypersphere of radiusRis almost 1. Therefore, to avoid the sparsity in the hyper- sphere of the normal data distribution and narrow the scope of the decision area, the optimization objective should obey the bi-hypersphere prop- erty of high-dimensional data (see Fig.6). Inspired by [72], we aim to compress normal data to an inter-",
    "almost 1. Therefore, to avoid the sparsity in the hyper- sphere of the normal data distribution and narrow the scope of the decision area, the optimization objective should obey the bi-hypersphere prop- erty of high-dimensional data (see Fig.6). Inspired by [72], we aim to compress normal data to an inter- val region between two co-centered hyperspheres. Combined with our log-barrier OCC loss in Eq.(6), we propose the following log-barrier bi-contraction OCC loss: Lbiâˆ’occ =âˆ’1 NtNX i=1\u0000 logsig(R maxâˆ’D i)Â·eDiâˆ’Rmax + logsig(D iâˆ’R min)Â·eRminâˆ’Di\u0001 (9) where we replace siwithDiâˆ’RmaxandRminâˆ’Di. From the previous analysis of Eq.(6), we can know that when Di> R maxandDi< R min, the loss in Eq.(9) will generate large gradients to compress the features outside the boundaries into the bi- hypersphere interval region. Anomaly-invariant Constraint Item.The log-barrier bi-contraction OCC loss in Eq.(9) can be used for constraining the normal residual fea- tures to a bi-hypersphere interval region. However, if we only constrain features to the interval region, the network may more easily overfit and simply map all features to the region. If we give the net- work another objective for anomalous features, this will urge the network to distinguish between nor- mal and abnormal, rather than forming a shortcut solution. Thus, we further introduce an anomaly- invariant constraint item by simply predicting the initial features to form an anomaly-invariant OCC loss to optimize ourFeature Constraintor. The loss is defined as: Laiâˆ’occ =Lbiâˆ’occ +1 MMX j=1||xâ€² jâˆ’xj||2(10) where Mis the number of abnormal residual features, xj, jâˆˆ { 1,2, . . . , M} represents each 11 Table 2:Statistics of initial residual features and constrained residual features.â€œStdâ€ means the standard deviation. Dataset MVTecAD VisA BTAD MVTec3D Statistics Stdâ†“Stdâ†“Stdâ†“Stdâ†“ Initial Residual Features 0.147 0.098 0.146 0.059 Constrained Residual Features 0.005 0.004 0.004 0.002 abnormal residual feature, and xâ€² j=Ïˆ(xj;W). â€œInvariantâ€ means the abnormal residual features remain relatively unchanged to themselves and will not be mapped into the hypersphere. In this way, our proposed anomaly-invariant OCC loss can not only make the distribution of normal residual features more compact but also keep abnor- mal residual features as invariant as possible. In addition, by constraining normal features into a bi-hypersphere interval region, the normal feature scales of different classes can also be more consis- tent. Therefore, after theFeature Constraintor, the normal and abnormal residual features are more distinguishable (see Fig.7), namely, we can obtain a better unified decision boundary. Finally, we also numerically evaluate the effec- tiveness of feature hypersphere constraining for scale decorrelation. Specifically, we utilize L2 norm as the statistical value of feature scale. For each class, we calculate an average feature scale value. Then, we calculate the standard deviation of fea- ture scale values of multiple classes. When the scale correlation is strong, scales of features across classes will vary significantly, resulting in a large standard deviation. When the scale correlation is weak, scales of features tend to be consis- tent, thereby the standard deviation is small. The numerical statistical values are shown in Tab.2. 3.3 VQ-based Feature Distribution Matching Residual features and feature hypersphere con- straining have been able to",
    "resulting in a large standard deviation. When the scale correlation is weak, scales of features tend to be consis- tent, thereby the standard deviation is small. The numerical statistical values are shown in Tab.2. 3.3 VQ-based Feature Distribution Matching Residual features and feature hypersphere con- straining have been able to effectively reduce the feature variations in new classes. In this subsec- tion, we aim to further enhance the consistency of testing and training data distributions. To this end, we propose a vector quantization (VQ) based feature distribution matching approach, which can contribute to improving the modelâ€™s class-adaptive ability on novel classes. Feature Dis- tribution Matching (FDM) is a group of techniquesthat aims to reduce the distribution mismatch or discrepancy of data from two different domains [73,74]. To ensure the generalization capability of our AD model in novel classes, we also want the testing residual features can obey the training fea- ture distribution as much as possible. This also conforms to the common sense in machine learning, where the more consistent the testing data distri- bution and the training data distribution, usually the better the modelâ€™s performance. Therefore, we think that FDM will be a promising and effective technique for class-agnostic anomaly detection. Specifically, following [75], we adopt the EFDM [74] to inject training distribution into the test- ing residual features, which is the SOTA of FDM. EFDM can precisely match empirical Cumulative Distribution Functions of image features, resulting in exact feature distribution alignment and accu- rate matching of statistical properties like mean, standard deviation, and high-order statistics. For- mally, for residual features Q âˆˆRNÃ—Cof a test sample, we need to select the same number of resid- ual features from the training distribution, denoted asP âˆˆRNÃ—C. Then, the EFDM is carried out as follows: EFDM(Q,P, Î±) :Q Ï„i=Î±Q Ï„i+ (1âˆ’Î±)P Ï‰i(11) where {QÏ„i}NC i=1and{PÏ‰i}NC i=1are sorted values ofQandPin ascending order. Here, NCrepre- sents the number of elements in QandP. From the perspective of style transfer [76, 77], Qcan be understood as playing the role of carrying the appearance information, and Pplays the role of car- rying the style information. In our method, the P plays the role in conveying distribution information pertaining to the training features. However, the training features are tremendous, itâ€™s not practical to represent the distribution by storing all training features, and this can also lead to feature selection being very computationally expensive. To this end, we employ the vector quantization [78] technique 12 during training to learn a codebook C âˆˆRKÃ—C that contains Kdiscrete codebook embeddings to represent training data. Based on PyTorch, we can simply usenn.Embedding(K, C)to construct the codebook. For a feature xipassed into the vector quantization module, the nearest element from the codebook Cwill be used as the quantized feature xqforxi. Then, the vector quantization loss is defined as follows: Lvq=1 NNX i=1||sg[x q]âˆ’xi||2 2+Î²||x qâˆ’sg[x i]||2 2(12) where sg[Â·] stands for the stop-gradient operator andÎ²is set to 0.25. The advantage of the code- book is that it not only significantly reduces the number of stored features but also can effectively represent the training feature distribution",
    "is defined as follows: Lvq=1 NNX i=1||sg[x q]âˆ’xi||2 2+Î²||x qâˆ’sg[x i]||2 2(12) where sg[Â·] stands for the stop-gradient operator andÎ²is set to 0.25. The advantage of the code- book is that it not only significantly reduces the number of stored features but also can effectively represent the training feature distribution [78,79]. Then, with the learned codebook C, we can look up the nearest neighbor in the codebook for each feature in Qand thus obtain P. Finally, we note that in implementation, we construct a specific codebook at each feature layer, and FDM is also performed at each layer. 3.4 Feature Distribution Estimating We employ the normalizing flow (NF) model [42] as ourFeature Distribution Estimatorto estimate the residual feature distribution. Note that our framework is not limited to normalizing flow, and other probabilistic models can also be used as the distribution estimator. Formally, we denote Ï†(Â·;Î¸) : X âˆˆRCâ†’ Z âˆˆRCas our normalizing flow. The input residual feature xâ€² iwill be transformed into a latent feature zi=Ï†(xâ€² i;Î¸) by the NF model. The estimated residual distribution pÎ¸(x) can be calculated according to the change of variables formula as follows [42,80]: logp Î¸(x) = logp Z(z) + log|detJ|(13) where the J=âˆ‡xzis the Jacobian matrix of the bijective transformation Ï†(Â·;Î¸). The model parameters Î¸can be optimized by maximizing the log-likelihoods and the latent variable Zis composed of ZnandZa.Znis the normal base dis- tribution for normal features and can be assumed to obey N(0,I). To distinguish normal and abnor- mal features, the latent base distribution for abnormal features needs to have a certain distancefrom the normal base distribution Zn, we denote it as Za=N(a,I), where ais a hyperparameter to control the distance between two base distri- butions. We set a= 1 in our model. Then, the log-likelihood of each feature xâ€² icorresponding to the normal and abnormal base distribution can be derived as follows, respectively logpZn Î¸(xâ€² i) =âˆ’C 2log(2Ï€) +1 2||zi||2 2âˆ’log|detJ i| and, logpZa Î¸(xâ€² i) =âˆ’C 2log(2Ï€)+1 2||ziâˆ’a||2 2âˆ’log|detJ i| (14) The maximum likelihood loss (minimize nega- tive log-likelihoods) for optimizing the NF model to fit the normal and abnormal residual features is derived as: Lml=âˆ’1 N+MN+MX i=1(1âˆ’y i)logpZn Î¸(xâ€² i) +y ilogpZa Î¸(xâ€² i) (15) where the binary indicator yiis set to be 1 when the corresponding position is anomalous and yi= 0 denotes that the corresponding location is normal. In practice, we can downsample the ground-truth mask to the size of each feature map, which can indicate normal and abnormal positions. Furthermore, we construct the anomaly clas- sification score s(xâ€² i) =pZa Î¸(xâ€² i) pZn Î¸(xâ€² i)+pZa Î¸(xâ€² i)(i.e., s(xâ€² i) measures the probability of xâ€² ibeing classified as abnormal). Accordingly, we can also construct the normal classification score: 1 âˆ’s(xâ€² i). Then, apart from the basic maximum likelihood loss, based on our constructed normal/anomaly classification scores, we can also use binary classification loss for optimization. Naturally, considering the imbal- ance between normal and abnormal, we employ the focal loss [81] as follows: Lfocal =1 N+MN+MX i=1FOCAL(s(xâ€² i), yi) (16) where FOCAL denotes the focal loss. The Eq.(15) and Eq.(16) both",
    "on our constructed normal/anomaly classification scores, we can also use binary classification loss for optimization. Naturally, considering the imbal- ance between normal and abnormal, we employ the focal loss [81] as follows: Lfocal =1 N+MN+MX i=1FOCAL(s(xâ€² i), yi) (16) where FOCAL denotes the focal loss. The Eq.(15) and Eq.(16) both allow the NF model to transform normal and abnormal features to two distinct base 13 distributions, thereby enhancing the discriminabil- ity. Then, the whole loss function for training the NF model is as follows: Lnf=Lml+Lfocal (17) 3.5 Inference and Anomaly Scoring For new classes, our method only requires few- shot normal samples to extract some features as reference, without any fine-tuning. We feed each test feature xl ifrom layer linto theFeature Con- straintor Ïˆ(Â·;W) and theFeature Distribution Estimator Ï†(Â·;Î¸) to get the latent feature zl i. In anomaly detection, anomalies are treated as out- liers. Thus, the anomaly score is calculated based on the normal base distributionZ n: s(xl i) = 1âˆ’exp\u0012 âˆ’Cl 2log(2Ï€)âˆ’1 2||zl i||2 2+log|detJl i|\u0013 (18) Then, we upsample all s(xl i) in the lth layer to the input image resolution ( H0Ã—W 0) using bilinear interpolation and combine all layers to obtain the anomaly score map. In addition, we also use the anomaly classification score s(xl i) = pZa Î¸(xâ€²,l i) pZn Î¸(xâ€²,l i)+pZa Î¸(xâ€²,l i)as anomaly score, and upsample all scores to the input image resolution. Then, the final anomaly score map is obtained by averaging the two above anomaly score maps. The maximum score of the anomaly score map is taken as the anomaly detection score of the image. 4 Experiments 4.1 Datasets Datasets.We conduct extensive experiments on 8 real-world datasets, covering various industrial inspection scenarios and medical imaging domains, to evaluate the performance of our ResAD++ sufficiently. In industrial inspection, we consider the popular benchmarks, including MVTecAD [1], VisA [3], BTAD [82], MVTec3D [2], and MPDD [83]. We also utilize the MVTecLOCO [84] dataset for logical anomaly detection. In medical imag- ing, we consider colon polyp detection dataset Kvasir-Seg [85] and brain tumor detection dataset BraTS [86].MVTecAD.The MVTecAD [1] dataset is widely used as a standard benchmark for evaluating unsupervised image anomaly detection methods. This dataset contains 5354 high-resolution images of 15 different product categories, in which 5 classes consist of textures and the other 10 classes contain objects. It comprises 3692 defect-free training sam- ples, as well as 1725 test images with and without defects. VisA.The VisA [3] dataset is another widely used unsupervised anomaly detection dataset. This dataset contains 10821 images with 9621 nor- mal and 1200 anomalous samples. In addition to images that only contain a single instance, the VisA dataset also has images that contain multi- ple instances. Moreover, some product categories of this dataset, such as Cashew, Chewing gum, Fryum, and Pipe fryum, have objects that are roughly aligned. These characteristics make the VisA dataset more challenging than the MVTecAD dataset. BTAD.The BeanTech Anomaly Detection dataset [82] contains 2830 images of 3 industrial products. The dataset is mainly used for texture anomaly detection, with all three products con- taining rich",
    "and Pipe fryum, have objects that are roughly aligned. These characteristics make the VisA dataset more challenging than the MVTecAD dataset. BTAD.The BeanTech Anomaly Detection dataset [82] contains 2830 images of 3 industrial products. The dataset is mainly used for texture anomaly detection, with all three products con- taining rich textures. Product 1, 2, and 3 of this dataset contain 400, 1000, and 399 training images, respectively. MVTec3D.The MVTec3D [2] dataset is pro- posed for 3D anomaly detection, which contains 4147 3D point cloud scans paired with 2D RGB images from 10 real-world categories. In this dataset, most anomalies can also be detected only through RGB images. Since we focus on image anomaly detection, we only use RGB images of the MVTec3D dataset. MPDD.MPDD [83] is focused specifically on defect detection during painted metal part fabri- cation, containing 6 classes of metal parts. This dataset poses more challenges than previous AD datasets,i.e., images are captured under complex conditions, including variable spatial orientations, positions, and distances of multiple objects con- cerning the camera at different light intensities and with a non-homogeneous background. MVTecLOCO.The MVTecLOCO [84] dataset is specifically proposed for logical anomaly detection. It contains five object categories from industrial inspection scenarios with a total of 1772 images for training and 1568 images for testing. The anomalies in MVTecLOCO mainly manifest 14 Table 3:Anomaly detection and localization results with AUROC metric (%) on eight real- world AD datasets under various few-shot AD settings. Â·/Â·means image-level and pixel-level AUROCs. In ResADâ€ ++, we utilize the ImageBind [88] as the feature extractor. RDAD and UniAD donâ€™t utilize the few-shot normal samples to fine-tune. AnomalyCLIP is a zero-shot AD model as a performance baseline in new classes, so the results under 2-shot, 4-shot, and 8-shot are the same. The best results are inbold, and the second best are underlined . Setting DatasetsBaselines Few-shot AD Methods (Non-CLIP-based)ResAD++ (W50)CLIP-based AD MethodsResADâ€  NeurIPS2024RDAD CVPR2022UniAD NeurIPS2022SPADE PaDiMPatchCore CVPR2022RegAD ECCV2022WinCLIP CVPR2023AnomalyCLIP ICLR2024InCTRL CVPR2024ResADâ€ ++ 2-shotMVTecAD 70.4/77.3 68.2/80.5 74.6/92.6 79.0/93.2 83.2/92.1 80.4/93.3 87.8/94.6 93.1/93.8 91.5/91.1 94.0 /- 94.4/95.6 96.0/95.9 VisA 61.3/80.9 49.9/79.7 71.7/88.7 62.8/90.5 77.4/91.3 69.4/93.3 87.3/96.5 81.9/94.9 82.1/95.5 85.8/- 84.5/95.1 90.8/97.6 BTAD 83.0/92.9 69.7/82.4 80.7/92.5 88.9/95.9 86.5/92.8 87.2/93.9 93.1/96.4 87.5/95.8 88.3/94.2 92.3/- 91.1/96.4 92.9/97.3 MVTec3D 55.8/93.5 52.2/89.1 62.5/93.6 56.5/95.4 62.9/94.5 59.5/95.4 69.4/97.1 72.3/96.8 65.8/94.8 68.9/- 78.5/97.5 82.9/97.4 MPDD 59.8/75.9 54.3/86.9 57.3/96.0 52.9/90.6 55.2/90.0 54.1/93.9 70.4/96.4 75.3/94.5 77.0 /96.5 71.5/- 77.3/97.3 78.2/97.5 MVTecLOCO 47.3/54.8 49.1/51.1 59.3/65.7 53.2/66.4 57.5/68.0 55.0/66.2 64.5/67.4 62.6/64.7 55.6/63.5 58.7/- 65.6/68.0 66.3/68.5 BraTS 49.8/66.7 59.5/88.5 58.0/92.8 49.4/90.2 58.2/93.5 54.6/81.4 66.1/91.4 55.9/91.5 63.4/90.874.6/- 67.9/94.3 69.0/94.7 Kvasir-Seg 54.4/54.2 50.3/49.9 85.7/67.8 86.8/65.4 79.0/69.0 70.1/51.0 86.5/69.2 76.2/73.0 75.9/78.9 63.3/- 77.5/79.4 80.5/80.3 Average 60.2/74.5 56.7/76.0 68.7/86.2 66.2/86.0 70.0/86.4 66.3/83.6 78.1/88.6 75.6/88.1 75.0/88.2 76.1/- 79.6/90.4 82.1/91.2 4-shotMVTecAD 70.4/77.3 68.2/80.5 75.5/94.3 82.0/94.4 87.4/94.2 84.8/94.5 90.8/95.8 94.6/94.2 91.5/91.1 94.5/- 94.2/96.9 96.4/96.3 VisA 61.3/80.9 49.9/79.7 75.0/93.7 70.3/93.4 82.6/94.0 78.0/93.5 89.3/96.8 84.1/95.2 82.1/95.5 87.7/- 90.8/97.5 92.1/98.0 BTAD 83.0/92.9 69.7/82.4 84.0/95.2 91.9 /96.9 91.1/95.5 90.8/94.9 94.1/97.3 89.5/95.8 88.3/94.2 91.7/- 91.5/96.8 91.0/97.2 MVTec3D 55.8/93.5 52.2/89.1 61.6/95.0 60.0/95.8 65.3/95.5 62.3/96.7 71.1/97.6 74.1/97.0 65.8/94.8 69.1/- 82.4/97.9 83.5/97.7 MPDD 59.8/75.9 54.3/86.9 62.4/97.0 52.9/92.8 67.9/95.5 66.2/94.1 75.6/97.4 79.4/95.1 77.0/96.5 71.7/- 86.0/98.0 86.5/98.1 MVTecLOCO 47.3/54.8 49.1/51.1 64.0/66.8 54.7/67.9",
    "84.1/95.2 82.1/95.5 87.7/- 90.8/97.5 92.1/98.0 BTAD 83.0/92.9 69.7/82.4 84.0/95.2 91.9 /96.9 91.1/95.5 90.8/94.9 94.1/97.3 89.5/95.8 88.3/94.2 91.7/- 91.5/96.8 91.0/97.2 MVTec3D 55.8/93.5 52.2/89.1 61.6/95.0 60.0/95.8 65.3/95.5 62.3/96.7 71.1/97.6 74.1/97.0 65.8/94.8 69.1/- 82.4/97.9 83.5/97.7 MPDD 59.8/75.9 54.3/86.9 62.4/97.0 52.9/92.8 67.9/95.5 66.2/94.1 75.6/97.4 79.4/95.1 77.0/96.5 71.7/- 86.0/98.0 86.5/98.1 MVTecLOCO 47.3/54.8 49.1/51.1 64.0/66.8 54.7/67.9 61.7/69.4 56.6/66.1 67.1/67.3 65.5/65.2 55.6/63.5 60.8/- 70.0/69.0 70.8/69.5 BraTS 49.8/66.7 59.5/88.5 66.3/94.8 60.6/94.5 71.2/95.9 60.0/87.3 74.9/94.2 67.3/93.2 63.4/90.8 76.9 /- 84.6/96.1 85.7/96.4 Kvasir-Seg 54.4/54.2 50.3/49.9 89.6/67.9 87.1/65.8 84.0/69.0 72.7/53.6 89.0/70.2 78.3/73.0 75.9/78.9 65.0/- 79.1/80.4 81.2/81.2 Average 60.2/74.5 56.7/76.0 72.3/88.1 69.9/87.7 76.4/88.7 71.4/85.1 81.5/89.6 79.1/88.6 75.0/88.2 77.2/- 84.8/91.6 85.9/91.8 8-shotMVTecAD 70.4/77.3 68.2/80.5 78.9/95.7 85.0/95.6 90.2/94.8 88.2/95.9 93.1/96.4 94.8/94.5 91.5/99.1 95.3 /- 97.7/96.7 98.6/96.7 VisA 61.3/80.9 49.9/79.7 73.7/94.2 77.9/95.0 85.4/93.9 80.0/95.3 90.6/97.2 85.4/95.4 82.1/95.5 88.7/- 92.3/97.8 93.4/98.2 BTAD 83.0/92.9 69.7/82.4 84.2/96.3 93.2/97.2 91.4/96.0 91.6/97.3 94.0/97.3 90.2/96.0 88.3/94.2 89.0/- 91.6/96.8 91.9/97.3 MVTec3D 55.8/93.5 52.2/89.1 63.8/95.6 63.5/96.3 68.3/94.2 67.4/96.9 74.5/98.0 75.8/97.1 65.8/94.8 71.4/- 83.0/97.9 85.2/97.8 MPDD 59.8/75.9 54.3/86.9 62.3/97.3 56.3/94.0 70.3/95.1 74.7/95.6 80.0/98.0 81.7/94.7 77.0/96.5 75.8/- 87.9/97.8 88.3/98.0 MVTecLOCO 47.3/54.8 49.1/51.1 64.3/67.3 57.7/69.1 65.7/69.7 62.2/69.6 67.8/67.6 68.0/65.5 55.6/63.5 62.9/- 69.5/69.3 69.8/69.9 BraTS 49.8/66.7 59.5/88.5 72.6/95.4 71.2/96.0 76.4/96.4 66.6/86.1 79.1/95.1 68.9/93.8 63.4/90.8 79.3 /- 85.9/96.3 87.9/96.8 Kvasir-Seg 54.4/54.2 50.3/49.9 90.1/68.791.2/67.1 84.5/68.4 76.7/51.6 90.5/68.8 83.1/73.1 75.9/78.9 75.0/- 81.5/81.2 83.4/82.1 Average 60.2/74.5 56.7/76.0 73.7/88.8 74.5/88.8 79.0/88.6 75.9/86.0 83.7/89.8 81.0/88.8 75.0/88.2 79.7/- 86.2/91.7 87.3/92.1 themselves in the violation of logical constraints, e.g., the screw bag contains two long screws and lacks a short one. BraTS.BraTS [86] is a multimodal magnetic resonance imaging (MRI) dataset for brain tumor segmentation, which is from the famous BraTS Challenge. We use the training set from the 2021 BraTS Challenge in this paper. The dataset con- tains 1251 brain MRIs acquired by 19 institutions employing different clinical protocols. Each brain MRI comprises multiple manually annotated MRI scans with the shape of 240 Ã—240Ã—155 (HÃ—WÃ—D ). To evaluate image AD models, we extract the 83rd MRI scan from each brain MRI and convert them to images. We can finally obtain 1097 images that contain brain tumors and 154 tumor-free images for testing. Kvasir-Seg.The Kvasir-Seg [85] dataset is based on the Kvasir [87] dataset, which is a multi- class dataset for gastrointestinal (GI) tract disease detection and classification. The original Kvasir dataset comprises 8000 GI tract images from 8 classes. However, the dataset is limited to classifi- cation tasks, due to only image-wise annotations. The Kvasir-Seg dataset is based on the polyp class of the Kavsir dataset, and the polyp images aremanually annotated by a medical doctor and then verified by an experienced gastroenterologist. 4.2 Evaluation Metrics For both image-level anomaly detection and pixel- level anomaly localization, the standard metric in anomaly detection, the area under the receiver operating characteristics curve (AUROC), will be used to evaluate the performance of AD meth- ods. Nonetheless, as abnormal areas in the image are usually smaller than the normal areas, this may cause overestimated pixel-level AUROC val- ues. This means that for some small anomalies, even if they are not correctly located, the pixel- level AUROC is still high. Thus, to more accurately measure the performance of anomaly localization, we",
    "in the image are usually smaller than the normal areas, this may cause overestimated pixel-level AUROC val- ues. This means that for some small anomalies, even if they are not correctly located, the pixel- level AUROC is still high. Thus, to more accurately measure the performance of anomaly localization, we also adopt the Per-Region-Overlap (PRO) curve metric proposed in [28]. The PRO score can take into account the overlap and recovery of connected anomaly components to better account for varying anomaly sizes, see [28] for details. Specifically, we report the PRO score with 0.3 FPR, which means that the normalized area under the PRO curve up to an average false positive rate per-pixel of 30%. 15 4.3 Implementation Details Implementation Details.All the training and test images are resized and cropped to 224 Ã—224 resolution. The parameters of the feature extractor are frozen during training. The layer numbers of the NF models are all 10. We use the Adam [89] optimizer with weight decay 5 eâˆ’4to train the model. The total training epochs are set to 100, and the batch size is 32 by default. The learn- ing rate is 1 eâˆ’5initially and dropped by 0.1 after [70,90] epochs. The boundary Rmaxis dynamic, Rmax =min(max({Dis (xj,0)}M j=1),0.4), where Dis(xj,0) is the distance from abnormal feature xj to origin. Rminis set as 0 .99âˆ—Rmax. The number of discrete quantization vectors in the VQ module is set to 1536, and the Î±in the FDM module is set to 0.4. We follow CFLOW [26] to implement the nor- malizing flow model. The normalizing flow model is mainly based on Real-NVP [42] architecture, which is composed of the so-called coupling layers. All coupling layers have the same architecture, where a learnable subnet is utilized to predict the affine parameters [42]. The convolutional subnet in Real- NVP is replaced with a two-layer MLP network. Each coupling layer is followed by a random and fixed soft permutation of channels [90] and a fixed scaling by a constant, similar to ActNorm layers introduced by [80]. Furthermore, we adopt the soft clamping of multiplication coefficients used by [42], the clamping coefficient is set to 1.9. We run all the experiments with the NVIDIA RTX 4090 GPU and random seed 42. Setup.Different from the conventional unsu- pervised anomaly detection, we train AD models on one dataset and then evaluate the modelâ€™s class-generalization ability on another dataset. Specifically, we train ResAD++ on the MVTecAD dataset and evaluate it on other datasets. As for MVTecAD, we train ResAD++ on the VisA dataset. During training, the anomalies that exist in the training dataset will also be effectively uti- lized. As these anomalies belong to known classes, they will not cause any anomaly leakage of the test set into training. Due to page limitation, we report dataset-level results, which are averaged values across all respective classes in the dataset, with the number of few-shot normal samples set toN fs= 2,4,8.Competing Methods.We select the represen- tative single-class AD method (RDAD [31]) and the multi-class AD method (UniAD [34]) as baselines. Our method is mainly compared with few-shot",
    "dataset-level results, which are averaged values across all respective classes in the dataset, with the number of few-shot normal samples set toN fs= 2,4,8.Competing Methods.We select the represen- tative single-class AD method (RDAD [31]) and the multi-class AD method (UniAD [34]) as baselines. Our method is mainly compared with few-shot AD methods. Following WinCLIP [57], we adapt three conventional full-shot AD methods, including SPADE [47], PaDiM [23], and PatchCore [24], to the few-shot setting by making use of few-shot nor- mal samples to calculate distance-based anomaly scores. We also compare with the few-shot AD method RegAD [56]. These methods are all based on WideResNet50 to extract features. For a fair comparison, we also utilize WideResNet50 [91] as the feature extractor. The corresponding model is denoted as ResAD++(W50). However, these meth- ods still need to remodel in new classes based on few-shot normal samples (see discussions in Sec.2), while our ResAD++ can be directly applied to new classes only requiring extracting features of few- shot normal samples as reference. Then, we also compare with the recent CLIP-based AD methods, including WinCLIP [57]7and InCTRL [66], and a CLIP-based zero-shot AD method AnomalyCLIP [61]. To guarantee the rationality of result compar- ison, we ensure all methods use the same few-shot normal samples, and all results are evaluated based on 224Ã—224 resolution. 4.4 Main Results Tab.3 shows the comparison results of our ResAD++ and other SOTA competing methods in image-level AUROC and pixel-level AUROC, respectively, on eight real-world AD datasets. The PRO scores are shown in Tab.4. Note that all the results are dataset-level average results across their respective data subsets. Compared to the results on trained classes (results in the original papers), our results (first two columns) demonstrate that conventional AD methods will fail when dealing with novel classes, whether it is the single-class (RDAD) or the multi-class (UniAD) AD method. By comparison, we can see that our ResAD++(W50) can significantly outperform all non-CLIP-based AD methods on all the 2-shot, 4-shot, and 8-shot settings. With more few-shot normal images, the performance of all meth- ods generally becomes better. On average, our 7No official implementation of WinCLIP is available. We use the public implementation at https://github.com/zqhang/ Accurate-WinCLIP-pytorch. 16 Table 4:Anomaly localization results with PRO metric (%) on eight real-world AD datasets under various few-shot AD settings. The InCTRL only provides image-level anomaly detection results, so the PRO scores of InCTRL are missing. Setting DatasetsBaselines Few-shot AD Methods (Non-CLIP-based)ResAD++ (W50)CLIP-based AD MethodsResADâ€  NeurIPS2024RDAD CVPR2022UniAD NeurIPS2022SPADE PaDiMPatchCore CVPR2022RegAD ECCV2022WinCLIP CVPR2023AnomalyCLIP ICLR2024InCTRL CVPR2024ResADâ€ ++ 2-shotMVTecAD 61.2 61.4 79.6 81.4 77.2 82.5 86.0 84.6 81.4 - 89.7 90.8 VisA 49.4 39.5 63.9 55.8 63.1 67.6 82.4 80.687.0- 84.9 86.6 BTAD 70.7 43.5 70.4 70.8 60.7 75.9 77.4 66.9 74.8 - 73.8 76.0 MVTec3D 79.0 69.0 80.6 84.5 81.1 85.3 90.0 88.8 86.4 - 89.9 90.7 MPDD 53.7 55.1 81.0 70.4 75.0 78.9 87.6 88.5 88.7 - 92.2 92.8 MVTecLOCO 44.6 41.9 61.0 53.0 51.7 57.5 61.2 57.5 54.3 - 60.2 60.8 BraTS 33.2 56.7 73.1 65.1 72.3 41.1 68.2 70.8 68.5 - 72.0 74.6 Kvasir-Seg 24.8 20.8 32.8 25.3 29.0 18.6",
    "86.4 - 89.9 90.7 MPDD 53.7 55.1 81.0 70.4 75.0 78.9 87.6 88.5 88.7 - 92.2 92.8 MVTecLOCO 44.6 41.9 61.0 53.0 51.7 57.5 61.2 57.5 54.3 - 60.2 60.8 BraTS 33.2 56.7 73.1 65.1 72.3 41.1 68.2 70.8 68.5 - 72.0 74.6 Kvasir-Seg 24.8 20.8 32.8 25.3 29.0 18.6 32.2 36.4 45.6 - 57.9 59.3 Average 52.1 48.5 67.8 63.4 63.8 63.4 73.1 71.8 73.3 - 77.6 79.0 4-shotMVTecAD 61.2 61.4 84.8 85.2 81.2 86.7 88.7 85.5 81.4 - 90.4 91.3 VisA 49.4 39.5 68.2 65.0 70.6 72.4 84.0 80.8 87.0 - 86.9 87.8 BTAD 70.7 43.5 73.3 74.4 67.2 77.0 80.4 66.9 74.8 - 73.6 75.7 MVTec3D 79.0 69.0 84.2 85.8 84.1 88.4 91.8 89.5 86.4 - 91.2 91.9 MPDD 53.7 55.1 86.5 76.9 85.4 80.4 90.4 89.5 88.7 - 93.8 94.2 MVTecLOCO 44.6 41.9 62.956.9 55.4 58.2 61.9 58.5 54.3 - 61.0 62.1 BraTS 33.2 56.7 77.3 73.9 77.9 52.1 75.8 71.6 68.5 - 75.5 77.4 Kvasir-Seg 24.8 20.8 34.0 25.4 32.6 23.6 34.0 37.9 45.6 - 59.1 60.0 Average 52.1 48.5 71.4 67.9 69.3 67.4 75.9 72.5 73.3 - 78.9 80.1 8-shotMVTecAD 61.2 61.4 86.8 87.9 83.2 89.1 90.1 86.1 81.4 - 91.4 92.2 VisA 49.4 39.5 72.3 69.7 68.4 75.2 86.7 79.9 87.0 - 88.3 88.7 BTAD 70.7 43.5 76.9 75.7 68.2 77.8 80.2 66.7 74.8 - 74.0 75.9 MVTec3D 79.0 69.0 85.6 87.5 81.7 89.5 93.0 89.8 86.4 - 91.6 92.3 MPDD 53.7 55.1 87.8 80.1 84.8 85.9 92.5 89.7 88.7 - 94.5 94.9 MVTecLOCO 44.6 41.9 64.162.5 56.7 63.5 63.0 58.8 54.3 - 61.7 63.1 BraTS 33.2 56.7 78.978.878.958.6 78.5 74.1 68.5 - 77.6 78.9 Kvasir-Seg 24.8 20.8 33.4 30.8 34.6 25.0 34.1 38.3 45.6 - 60.6 61.0 Average 52.1 48.5 73.2 71.6 69.6 70.6 77.3 72.9 73.3 - 80.0 80.9 Table 5:Per-class results on the MVTecAD dataset.Â·/Â·means image-level AUROC and PRO. Dataset ClassesBaselines Few-shot AD Methods (Non-CLIP-based)ResAD++ (W50)CLIP-based AD Methods RDAD CVPR2022UniAD NeurIPS2022SPADE PaDiMPatchCore CVPR2022RegAD ECCV2022WinCLIP CVPR2023AnomalyCLIP ICLR2024InCTRL CVPR2024ResADâ€ ++ MVTecADBottle 70.9/35.3 61.6/51.3 94.0/93.7 99.7 /94.8 99.6/91.5 99.3/91.9 99.9/95.5 99.5/85.3 89.3/80.5 99.2/- 99.5/94.4 Cable 64.4/56.6 42.6/34.0 77.8/76.1 76.3/73.9 93.8 /84.3 81.2/82.6 83.7/80.6 90.6/73.0 69.8/64.4 86.5/- 97.3/85.6 Capsule 46.6/76.6 45.1/67.2 68.6/88.7 63.8/87.9 67.6/82.5 65.9/87.9 66.9/91.2 79.7/83.689.9/87.2 84.0 /- 71.5/84.0 Carpet 99.5/90.2 98.0/96.3 89.4/95.3 99.8/97.5 98.3/92.4 98.2/94.7 98.9/95.4 99.9/95.5100/90.1 99.7/- 100/97.6 Grid 70.7/22.0 90.1/69.2 30.6/55.0 68.1/64.4 62.1/37.9 85.8/68.4 88.1/70.9 98.2/84.6 97.0/75.6 97.8/- 100/93.7 Hazelnut 85.5/88.4 82.6/75.0 75.6/91.7 96.6/91.3 99.0 /87.2 93.5/92.2 97.6/93.3 98.0/93.3 97.2/92.4 95.4/- 99.9/95.8 Leather 84.4/84.1 99.8 /97.7 94.7/97.4100/98.0 99.8 /92.8 97.2/96.3 100/98.2 100/98.0 99.8 /92.2100/- 100/98.3 Metal nut 64.7/65.8 48.2/28.9 56.1/84.1 60.5/66.4 90.2/85.8 88.0/88.3 94.3/90.2 96.8/77.3 93.6/71.0 94.8/- 100/89.9 Pill 63.7/77.8 47.2/63.9 61.6/92.7 61.7/92.3 80.3/85.2 63.9/88.7 91.6/96.6 91.7/90.6 81.8/88.2 89.0/- 98.7/96.3 Screw 62.6/79.4 51.9/71.5 44.3/81.1 49.7/77.5 51.3/67.7 58.1/87.1 58.0/86.1 81.5/86.3 81.1/88.0 80.0/- 85.5/91.9 Tile 83.0/53.0 88.7/69.7 94.2/81.0 96.8/81.8 99.5/85.4 95.0/78.4 99.8/88.1 99.4/78.2100/87.6 99.9/- 99.4/89.6 Toothbrush 46.3/66.2 58.5/53.1 66.4/86.0 89.2/87.9 80.0/70.6 86.0/84.7 99.7/93.1 95.0/89.4 84.7/88.5 97.5/- 100/93.1 Transistor 52.9/31.9 51.8/19.9 89.2/64.7 81.0/81.695.4 /75.4 76.2/81.3 88.7/65.8 90.9/68.1 92.8/52.8 89.8/- 96.5/72.6 Wood 90.9/70.6 95.0/85.4 97.3/91.4 99.3/91.7 98.8/89.1 99.4/89.8 99.7/93.6 99.8/87.5 96.8/91.2 99.7/- 99.6/93.2 Zipper 69.6/20.6 61.7/37.6 92.8/92.9 87.8/90.3 95.0/90.0 83.8/87.9 95.1/92.5 98.0/91.4 98.5 /65.3 96.5/- 98.6/93.1 ResAD++(W50)",
    "Toothbrush 46.3/66.2 58.5/53.1 66.4/86.0 89.2/87.9 80.0/70.6 86.0/84.7 99.7/93.1 95.0/89.4 84.7/88.5 97.5/- 100/93.1 Transistor 52.9/31.9 51.8/19.9 89.2/64.7 81.0/81.695.4 /75.4 76.2/81.3 88.7/65.8 90.9/68.1 92.8/52.8 89.8/- 96.5/72.6 Wood 90.9/70.6 95.0/85.4 97.3/91.4 99.3/91.7 98.8/89.1 99.4/89.8 99.7/93.6 99.8/87.5 96.8/91.2 99.7/- 99.6/93.2 Zipper 69.6/20.6 61.7/37.6 92.8/92.9 87.8/90.3 95.0/90.0 83.8/87.9 95.1/92.5 98.0/91.4 98.5 /65.3 96.5/- 98.6/93.1 ResAD++(W50) outperforms the best competing model, PatchCore, with up to 8.1%/2.2%/9.3%, 5.1%/0.9%/6.6%, and 4.7%/1.2%/7.7% improve- ments under the 2-shot, 4-shot, and 8-shot settings, respectively. Please note that when evaluating PatchCore, we utilize the few-shot normal sam- ples to remodel the coreset for each new class (see Sec.2), while our ResAD++(W50) is directly applied to each new class without any remodeling or fine-tuning. Even with remodeling, our method still has advantages over the conventional few-shot AD methods in cross-dataset generalization. Com- pared to the recent CLIP-based AD methods, our ResAD++(W50) by only using WideResNet50 can achieve comparable or even better results thanWinCLIP and InCTRL (with more powerful ViT- B/16+) and also AnomalyCLIP (with ViT-L/14), further demonstrating our superiority. We further implement a ResADâ€ ++ model by utilizing the powerful ImageBind [88] as the feature extractor. The outputs from the [8 ,16,24,32] layers of ImageBind are used as the pre-trained features. As shown in Tab.3, by employing a model with stronger representation capability, our method can achieve better cross-dataset performance, which significantly outperforms the SOTA CLIP-based AD methods, WinCLIP and InCTRL. This demon- strates that our framework can effectively combine the latest vision models to manifest a stronger class- adaptive ability. Moreover, these two CLIP-based 17 Table 6:Framework ablation studies. I-AUROC and P-AUROC mean image-level AUROC and pixel- level AUROC, respectively. â€œRFâ€ means residual features, â€œFHCâ€ means feature hypersphere constraining. â€œAI-OCC Lossâ€ adopts the abnormal-invariant OCC loss. â€œFDMâ€ represents feature distribution matching. â€œMACâ€ means the merged anomaly criterion (see descriptions in Sec.3.5). expID RF FHC AI-OCC Loss FDM MACMVTecAD VisA I-AUROC P-AUROC PRO I-AUROC P-AUROC PRO 3.1 70.6 78.5 62.2 59.0 83.6 53.0 3.2âœ“ 82.2 94.0 85.7 84.9 95.8 78.6 3.3âœ“ 66.6 76.6 60.0 55.2 83.0 50.1 3.4âœ“ âœ“ 87.2 94.6 86.2 85.3 96.0 81.4 3.5âœ“ âœ“ âœ“ 89.3 95.3 87.1 85.5 96.8 82.1 3.6âœ“ âœ“ 78.0 90.1 73.6 77.4 93.2 72.4 3.7âœ“ âœ“ âœ“ âœ“ 90.4 95.6 88.4 88.7 96.7 83.5 3.8âœ“ âœ“ âœ“ âœ“ âœ“ 90.8 95.8 88.7 89.3 96.8 84.0 methods also heavily rely on CLIP-based image encoders. When we employ WideResNet50 in these two methods, our method has more advantages than these two methods. The results under the 4- shot setting are in Tab.7. Compared to WinCLIP and InCTRL, our method is less reliant on the representation capability of the backbone network and is more widespreadly applicable for various backbones. Whatâ€™s more, when applied to medical AD datasets (generalization no longer at the class- level but at the domain-level), our method fur- ther demonstrates stronger cross-domain gen- eralization ability, despite it being trained on industrial data (MVTecAD). Specifically, under the 4-shot setting, our method surpasses Win- CLIP by 18.4%/3.2%/5.8% (on BraTS) and 2.9%/8.2%/22.1% (Kvasir-Seg) in image-level AUROC/pixel-level AUROC/PRO, respectively. Compared to the results of cross-class generaliza- tion in the industrial datasets, the",
    "ther demonstrates stronger cross-domain gen- eralization ability, despite it being trained on industrial data (MVTecAD). Specifically, under the 4-shot setting, our method surpasses Win- CLIP by 18.4%/3.2%/5.8% (on BraTS) and 2.9%/8.2%/22.1% (Kvasir-Seg) in image-level AUROC/pixel-level AUROC/PRO, respectively. Compared to the results of cross-class generaliza- tion in the industrial datasets, the cross-domain results also show that our method has more sig- nificant advantages in the more challenging task of cross-domain generalization. In Fig.8, we show the qualitative cross-domain generalization results on the BraTS dataset in the last row. An obvious observation is that our method can locate anoma- lies more accurately and can also effectively avoid false positives in normal regions. By comparison, the previous AD methods either canâ€™t correctly localize anomalies (the left example) or may still generate many normal misdetections (the right example). Sensitivity discussion. The extensive results on 8 real-world datasets have demonstrated thatour method has strong cross-class and even cross- domain generalization capabilities. We also utilize VisA for training and MVTecAD for testing. With only 8-shot normal samples as reference, our method can achieve 98.6% image-level AUROC and 96.7% pixel-level AUROC on MVTecAD. This also indicates that our method does not rely on MVTecAD for training. For each dataset, we con- duct experiments under the 2, 4, and 8 shot settings, and the results show that our method also performs well even with 2 reference samples. Thus, our method overall has good robustness to both the dataset and the number of reference samples. Per-class results. In Tab.5, we further provide per-class results on the most representative dataset, MVTecAD. The per-class results can provide more substantial evidence that our method outperforms other methods, as our method can achieve the best or second-best results in 13 classes out of 15 classes. 4.5 Comparison with Full-shot Trained Models Previous experiments have verified the class- generalization ability of our ResAD++ under three few-shot conditions. However, in practical appli- cations, itâ€™s usually not hard for us to obtain a sufficient number of normal samples for a new class. Therefore, we further wonder to know that how our ResAD++ performs compared to the AD mod- els trained on full-shot normal samples, and how our ResAD++ performs when it can access full- shot normal samples. Then, for a new dataset, we retrain UniAD and RDAD on full-shot normal sam- ples (for a class, full-shot means all normal samples provided by this classâ€™s training set) from all classes 18 Table 7:Anomaly detection and localization results (under 4-shot) with WideResNet50 as the feature extractor.Â·/Â·/Â·means image-level AUROC, pixel-level AUROC, and PRO, respectively. Dataset WinCLIP InCTRL ResAD++(W50) MVTecAD 86.6/91.6/81.7 86.9/-/- 90.8/95.8/88.7 VisA 80.7/92.5/76.2 82.3/-/- 89.3/96.8/84.0 BTAD 87.7/93.7/65.4 90.4/-/- 94.1/97.3/80.4 MVTec3D 63.1/91.7/83.6 63.2/-/- 71.1/97.6/91.8 Table 8:Performance comparison with full-shot trained AD models. RDAD and UniAD are trained with the full-shot normal samples (all normal samples in the training set) when used for new classes. Dataset RDAD UniADResAD++(W50) (8-shot)ResAD++(W50) (full-shot) MVTecAD 95.7/96.3/91.0 96.5/97.0/90.6 93.1/96.4/90.1 98.2/97.0/92.5 VisA 89.5/97.5/85.5 92.8/98.3/86.8 90.6/97.2/86.7 93.3/98.3/88.9 BTAD 93.9/97.5/74.8 94.2/97.2/76.8 94.0/97.3/80.2 94.4/97.5/81.3 MVTec3D 77.1/98.3/93.1 77.5/96.6/88.5 74.5/98.0/93.0 86.0/98.0/93.5 in this dataset (only one model is trained, namely under the multi-class",
    "samples (all normal samples in the training set) when used for new classes. Dataset RDAD UniADResAD++(W50) (8-shot)ResAD++(W50) (full-shot) MVTecAD 95.7/96.3/91.0 96.5/97.0/90.6 93.1/96.4/90.1 98.2/97.0/92.5 VisA 89.5/97.5/85.5 92.8/98.3/86.8 90.6/97.2/86.7 93.3/98.3/88.9 BTAD 93.9/97.5/74.8 94.2/97.2/76.8 94.0/97.3/80.2 94.4/97.5/81.3 MVTec3D 77.1/98.3/93.1 77.5/96.6/88.5 74.5/98.0/93.0 86.0/98.0/93.5 in this dataset (only one model is trained, namely under the multi-class AD paradigm). The results of full-shot trained UniAD and RDAD are shown in Tab.8, which can be regarded as the expected reference performance for each dataset. By compar- ison, our method can achieve comparable results with AD models retrained on new classes based on full-shot normal samples, even if only 8 nor- mal samples are used as reference. Furthermore, applying our method to the full-shot is also quite easy, we only need to extract features from full- shot normal samples as reference. However, the full-shot will bring too many normal reference fea- tures, leading to excessive computation costs in generating residual features. To this end, we follow the training-free global retrieval approach in [92] to match some spatially-aligned samples for each input sample, and then generate specific reference features for each input sample. Specifically, we matched 10 reference samples for each input sam- ple. The results in Tab.8 show that, with accessing the full-shot normal samples, our methodâ€™s per- formance on new classes can be further improved, surpassing the full-shot trained UniAD and RDAD. This further verifies the superiority of our method for class-agnostic anomaly detection and demon- strates its potential to achieve the same effect as trained classes on new classes.4.6 Ablation Studies In ablation studies, we conduct experiments under the â€œVisA to MVTecADâ€ and â€œMVTecAD to VisAâ€ cases and use WideResNet50 [91] as the feature extractor. All the experiments are under the 4-shot setting. Residual Features.As shown in Tab.6, with- out residual features, the cross-dataset perfor- mance drops dramatically from 82.2%/85.7% to 70.6%/62.2% on MVTecAD and 84.9%/78.6% to 59.0%/53.0% on VisA. This verifies our confirma- tion that residual features are of vital significance for class-agnostic anomaly detection. Analogously, any method that can realize feature decorrelation to reduce the variations of new class distribution relative to known class distributions is also promis- ing to achieve class-agnostic anomaly detection. Feature Hypersphere Constraining.The ablation study on the effectiveness of the feature hypersphere constraining is also in Tab.6. The effectiveness (expID 3.2 v.s. 3.5) indicates that by further reducing the variations in the feature distri- bution and making the distribution of new classes more consistent with the learned distribution, we can achieve better cross-class generalization results and ultimately arrive at the general anomaly detec- tion goal. In Fig.7, we also present a visualization figure to intuitively show the effect of the Feature Constraintor. Anomaly-invariant OCC Loss.The effec- tiveness of anomaly-invariant OCC loss is validated 19 Table 9:Ablation studies of hyperparameters in the VQ-based FDM module. To avoid excessive hyperparameter combinations, we fix the Kas 1024 when we change the Î±. The Î±is set as 0.4 when the Kis changing. Feature distribution matching coefficient (Î±, in Eq.(11)) Number of codebook embeddings (K) 0.3 0.4 0.5 0.6 0.7 512 1024 1536 2048 2560 MVTecAD 89.8/95.2/87.9 90.7/95.7/88.7",
    "module. To avoid excessive hyperparameter combinations, we fix the Kas 1024 when we change the Î±. The Î±is set as 0.4 when the Kis changing. Feature distribution matching coefficient (Î±, in Eq.(11)) Number of codebook embeddings (K) 0.3 0.4 0.5 0.6 0.7 512 1024 1536 2048 2560 MVTecAD 89.8/95.2/87.9 90.7/95.7/88.7 90.6/95.7/88.6 90.6/95.6/88.5 90.4/95.5/88.4 90.6/95.7/88.5 90.7/95.7/88.7 90.8/95.8/88.7 90.5/95.6/88.5 90.6/95.6/88.4 VisA 89.3/96.4/83.8 89.3/96.8/83.9 89.1/96.7/83.9 88.5/96.8/83.8 87.9/96.7/83.2 89.0/96.7/83.7 89.3/96.8/83.9 89.3/96.8/84.0 88.5/96.8/83.9 88.5/96.6/83.8 in Tab.6. With the anomaly-invariant OCC loss, image-level AUROC and PRO can be improved by 11.3%/13.5% on MVTecAD and 8.1%/9.7% on VisA (expID 3.5 v.s. 3.6). Moreover, we also find that without this loss, the results would rapidly decrease after certain epochs of training (i.e., over- fitting). This shows that keeping abnormal residual features as invariant as possible is beneficial to avoid the Feature Constriantor overfitting and thus achieve better results. Feature Distribution Matching.As shown in Tab.6, with feature distribution matching, the cross-dataset performance can be further improved by 1.1%/1.3% on MVtecAD and 2.2%/1.4% on VisA (expID 3.5 v.s. 3.7). The results demon- strate that FDM is effective in improving the class-generalization capability of our ResAD. How- ever, directly applying FDM to the initial features (expID 3.3), the results are not improved but decreased. This may be because when the discrep- ancy between testing and training distributions is too large, itâ€™s hard for FDM to reduce the distri- bution mismatch under such a condition. In initial features, the discrepancy between testing and train- ing distributions will be significantly greater than the discrepancy in residual features. Residual fea- tures are more conducive to FDMâ€™s effectiveness. We think that FDM will be a promising technique for class-agnostic anomaly detection, deserving more explorations in future work. Feature Constraintor Configuration.We further ablate the network architectures of the Feature Constraintor, the results are shown in Tab.10. The results indicate that the simple Conv+BN+ReLU network can yield the best per- formance. We observe a significant performance drop with a more complex feature constraintor (e.g., Bottleneck, MultiScaleFusion). One possible reason is that a complex network may lead to overfitting, reducing the generalization ability for various anomalies in the test. Hyperparameters of the VQ-based FDM module.We ablate the hyperparameters Î±andKin Tab.9. From Tab.9, we can draw the following main conclusions: (1) Î±has a more significant effect on performance compared to K. Small Î±may lead to excessive training distribution injecting, while large Î±may result in ineffective matching with the training distribution, both of which can cause performance degradation. Overall, setting Î±to 0.4 is a good choice. (2) The VQ-based FDM module is not very sensitive to K, setting Kto a moderate number (1536) can achieve the best results. 4.7 Generalization to Other AD Frameworks Furthermore, we think that our residual feature learning insight is not limited to the model pro- posed in this paper, but can be considered as an effective and general method for solving class- agnostic anomaly detection. The main reasons are: (1) The process of converting initial features to residual features can be easily applied to other AD models. (2) Residual features are less sensitive to new classes (see",
    "this paper, but can be considered as an effective and general method for solving class- agnostic anomaly detection. The main reasons are: (1) The process of converting initial features to residual features can be easily applied to other AD models. (2) Residual features are less sensitive to new classes (see explanations in Sec.3.1). In this subsection, we further extend our method to two popular AD frameworks: reconstruction-based and discrimination-based AD frameworks. Specifically, we employ UniAD [34] (reconstruction-based) and SimpleNet [93] (discrimination-based) as baselines and incorporate our method into them, the experi- mental results are shown in Tab.11. It can be found that the performance of UniAD and SimpleNet is quite poor when used for new classes, while con- verting to residual feature learning can significantly improve the modelsâ€™ class-generalization capability. The remarkable improvements (e.g., 24.7%/26.4% and 15.9%/23.5% on MVTecAD) validate the effec- tiveness and universality of residual features for designing class-agnostic AD models. 4.8 Visualization and Qualitative Results Visualization Results.In Fig.4, we have shown the t-SNE visualization of initial features and 20 Table 10:Comparison of different feature constraintors. â€œConvBnReluâ€ implements a simple Conv+BN+ReLU network. â€œBasicBlockâ€ adopts the BasicBlock in ResNet at each feature level. â€œBottle- Neckâ€œ replaces the BasicBlock with BottleNeck. â€œMultiScaleFusionâ€ is a FPN-like architecture to fuse multi-scale features. In â€œMultiScaleFusion+BasicBlock/BottleNeckâ€, we add BasicBlock/BottleNeck after the multi-scale fusion. Network Architecture MVtecAD VisA ConvBnRelu 90.8/95.8/88.7 89.3/96.8/84.0 BasicBlock 87.8/94.9/87.4 76.4/92.9/72.8 BottleNeck 85.7/94.6/86.8 75.8/92.0/70.6 MultiScaleFusion 86.6/94.5/86.1 75.6/93.2/69.0 MultiScaleFusion+BasicBlock 85.2/94.4/86.1 74.2/91.6/68.3 MultiScaleFusion+BottleNeck 85.7/94.2/85.7 73.8/90.6/66.8 Table 11:Anomaly detection and localization results when incorporating our method into UniAD and SimpleNet. â€œRFLâ€ represents residual feature learning. MVTecAD VisA BTAD MVTec3D MVtecAD VisA BTAD MVTec3D UniAD[34] 68.2/80.5/61.4 49.9/79.7/39.5 69.7/82.4/43.5 52.2/89.1/69.0 SimpleNet[31] 70.4/77.3/61.2 61.3/80.9/49.4 83.0/92.9/70.7 55.8/93.5/79.0 + RFL (Ours) 92.9/94.9/87.8 85.8/96.8/83.9 84.3/93.9/67.3 77.0/96.9/89.5 + RFL (Ours) 86.3/94.5/84.7 82.3/95.1/78.9 93.5/95.2/81.0 66.2/94.7/89.6 âˆ† +24.7/14.4/26.4 +35.9/17.1/44.4 +14.6/11.5/23.8 +24.8/7.8/20.5 âˆ† +15.9/17.2/23.5 +21/14.2/29.5 +10.5/2.3/10.3 +10.4/1.2/10.6 residual features. It can be found that in the initial feature space, the feature distribution of new classes is significantly different from the distribution of known classes, resulting in poor adaptability of AD models to new classes. How- ever, the variations among different classes can be significantly reduced by converting into residual feature space. In this way, the modelâ€™s generaliz- ability to new classes can be effectively improved. Fig.7 (a) and (b) further show the t-SNE visualiza- tion of initial residual features and residual features after feature hypersphere constraining. Results show that the proposed feature hypersphere con- straining approach can make the normal residual features more compact and more separated from the abnormal features. (a) Initial Residual Features (b) Constrained Residual Features Fig. 7:Feature t-SNE visualization. (a) The initial residual features. (b) The residual features after the Feature Constraintor.Qualitative Results.Fig.8 shows qualitative results across various classes from different datasets. From the first to the last row, the samples are from the MVTecAD, VisA, MVTecLOCO, and BraTS datasets, respectively. An obvious observation is that our method can locate anomalies more accu- rately and can also effectively avoid false positives in normal regions. However, the previous AD meth- ods either canâ€™t correctly localize anomalies (e.g., the second samples in the second and",
    "the MVTecAD, VisA, MVTecLOCO, and BraTS datasets, respectively. An obvious observation is that our method can locate anomalies more accu- rately and can also effectively avoid false positives in normal regions. However, the previous AD meth- ods either canâ€™t correctly localize anomalies (e.g., the second samples in the second and third rows), or may still generate many normal misdetections (e.g., almost all visualized samples). 4.9 Computational Complexity With the image size fixed as 224 Ã—224, we com- pare the number of parameters and per-image inference time with all competitors. The com- parison results are shown in Fig.9. Compared to the conventional few-shot AD methods (SPADE, PaDiM, and PatchCore), our ResAD++(W50) overall has fewer parameters with the same back- bone network (WideResNet50) as the feature extractor. Compared to WinCLIP and InCTRL, our ResAD++(W50) has fewer parameters and is significantly faster. Our ResADâ€ ++ also has the same magnitude of parameters, but its inference time is significantly faster than WinCLIP and InC- TRL. Overall, our method can achieve a better 21 UniAD Ground Truth PatchCore RegAD Ours WinCLIP UniAD Ground Truth PatchCore RegAD Ours WinCLIP Fig. 8:Qualitative results across various classes from different datasets. From the first to the last row, the samples are from the MVTecAD, VisA, MVTecLOCO, and BraTS datasets, respectively. latency, parameter/accuracy trade-off than these previous competing AD methods. Fig. 9:The latency, AUROC performance versus model parameters on the MVTecAD dataset. The AUROC is the image-level AUROC under the 4-shot setting. Our method can achieve a better latency, parameter/accuracy trade-off. The inference times are measured on a Nvidia RTX 4090 GPU with 24GB of VRAM. 5 Discussion and Conclusion Limitation.Even if our method manifests good cross-class and even cross-domain generalization AD performance on eight real-world AD datasets, including industrial anomalies and medical anoma- lies, there are still some limitations of our work.One limitation is that we only conducted experi- ments on data of image modality, itâ€™s very valuable to further extend our method to other applica- tion domains and data modalities, such as video data and time series, to more comprehensively validate our methodâ€™s generalizability. Another valuable future work is to incorporate our method into recent SOTA AD methods for achieving bet- ter class-agnostic AD performance. In Sec.4.7, we incorporate our method into UniAD and SimpleNet and gain remarkable improvements. However, these methods still have a large room for improvement. How to upgrade the other types of anomaly detec- tion methods to class-agnostic AD methods and how to find a general approach for class-agnostic (or even domain-agnostic) anomaly detection will be the future works. In a summary, we innovatively propose the residual feature learning approach and further con- struct a simple but effective framework: ResAD++, for achieving class-agnostic anomaly detection. ResAD++ consists of several simple neural net- work modules that are easy to train and apply in real-world scenarios. Despite the simplicity, ResAD++ achieves remarkable AD results in new classes. We conclude our findings for future research: residual features are really effective for designing class-agnostic AD models, and our fea- ture hypersphere constraining approach and the feature distribution matching techniques both have",
    "to train and apply in real-world scenarios. Despite the simplicity, ResAD++ achieves remarkable AD results in new classes. We conclude our findings for future research: residual features are really effective for designing class-agnostic AD models, and our fea- ture hypersphere constraining approach and the feature distribution matching techniques both have good reference values for future work. Data Availability.The related code and data for ResAD++ framework are available at https: //github.com/xcyao00/ResAD. 22 Acknowledgments This work was supported in part by the National Natural Science Fund of China (62371295), National Natural Science Fund of China (62201062), and the Science and Tech- nology Commission of Shanghai Municipality (22DZ2229005). References [1]P. Bergmann, K. Batzner, M. Fauser, D. Sat- tlegger, and C. Steger, â€œThe mvtec anomaly detection dataset: A comprehensive real-world dataset for unsupervised anomaly detection,â€ In IJCV, 2021. [2]P. Bergmann, X. Jin, D. Sattlegger, and C. Steger, â€œThe mvtec 3d-ad dataset for unsupervised 3d anomaly detection and local- ization,â€In Proceedings of the 17th Interna- tional Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications, 2021. [3]Y. Zou, J. Jeong, L. Pemula, D. Zhang, and O. Dabeer, â€œSpot-the-difference self- supervised pre-training for anomaly detection and segmentation,â€In ECCV, 2022. [4]A. Acsintoae, A. Florescu1, M.-I. Georgescu, T. Mare, P. Sumedrea1, R. T. Ionescu, F. S. Khan, and M. Shah, â€œUbnormal: New bench- mark for supervised open-set video anomaly detection,â€In CVPR, 2022. [5]W. Sultani, C. Chen, and M. Shah, â€œReal- world anomaly detection in surveillance videos,â€In CVPR, 2018. [6]Y. Cao, X. Xu, J. Zhang, Y. Cheng, X. Huang, G. Pang, and W. Shen, â€œA survey on visual anomaly detection: Chanllenge, approach, and prospect,â€arXiv preprint arXiv:2401.16402, 2024. [7]G. Pang, C. Shen, L. Cao, and A. V. D. Hen- gel, â€œDeep learning for anomaly detection: A review,â€ACM computing surveys (CSUR), 2021. [8]T. Vojir, T. Sipka, and R. Aljundi, â€œRoad anomaly detection by partial image recon- struction with segmentation coupling,â€In ICCV, 2021. [9]G. D. Biase, H. Blum, R. Siegwart, and C. Cadena, â€œPixel-wise anomaly detection in complex driving scenes,â€In CVPR, 2021.[10]P. Bergmann, S. Lowe, M. Fauser, D. Sattleg- ger, and C. Steger, â€œImproving unsupervised defect segmentation by applying structural similarity to autoencoders,â€In International Conference on Computational Vision Tech- nologies and Applications, 2019. [11]H. Park, J. Noh, and B. Ham, â€œLearning memory-guided normality for anomaly detec- tion,â€In CVPR, 2020. [12]J. Yang, Y. Shi, and Z. Qi, â€œDfr: Deep feature reconstruction for unsupervised anomaly seg- mentation,â€arXiv preprint arXiv: 2012.07122, 2020. [13]T. Schlegl, P. SeebÂ¨ock, S. M. Waldstein, U. Schmidt-Erfurth, and G. Langs, â€œUnsuper- vised anomaly detection with generative adver- sarial networks to guide marker discovery,â€ In International Conference on Information Processing in Medical Imaging, 2017. [14]S. Akcay, A. Atapour-Abarghouei, and T. P. Breckon, â€œGanomaly: Semi-supervised anomaly detection via adversarial training,â€ In ACCV, p. 622â€“637, 2018. [15]V. Zavrtanik, M. Kristan, and D. Sko- caj, â€œReconstrution by inpainting for visual anomaly detection,â€In Pattern Recognition, 2021. [16]R. T. I. Nicolae-Catalin Ristea, Neelu Madan, K. Nasrollahi, F. S. Khan, T. B. Moes- lund, and M. Shah, â€œSelf-supervised predictive convolutional attentive block for anomaly detection,â€In CVPR, 2022. [17]Z. Chen, X. Xie, L. Yang, and J.-H. Lai, â€œHard-normal",
    "caj, â€œReconstrution by inpainting for visual anomaly detection,â€In Pattern Recognition, 2021. [16]R. T. I. Nicolae-Catalin Ristea, Neelu Madan, K. Nasrollahi, F. S. Khan, T. B. Moes- lund, and M. Shah, â€œSelf-supervised predictive convolutional attentive block for anomaly detection,â€In CVPR, 2022. [17]Z. Chen, X. Xie, L. Yang, and J.-H. Lai, â€œHard-normal example-aware template mutual matching for industrial anomaly detection,â€ In IJCV, 2025. [18]L. Ruff, R. A. Vandermeulen, N. Gornitz, L. Deecke, and S. A. Siddiqui, â€œDeep one-class classification,â€In International Conference on Machine Learning, 2021. [19]L. Ruff, R. A. Vandermeulen, N. GÂ¨ ornitz, A. Binder, E. MÂ¨ uller, K.-R. MÂ¨ uller, and M. Kloft, â€œDeep semi-supervised anomaly detection,â€In International Conference on Learning Representations, 2021. [20]P. Liznerski, L. Ruff, R. A. Vandermeulen, B. J. Franks, M. Kloft, and K.-R. Muller, â€œExplainable deep one-class classification,â€In International Conference on Learning Repre- sentations, 2021. 23 [21]J. Yi and S. Yoon, â€œPatch svdd: Patch-level svdd for anomaly detection and segmentation,â€ In Asian Conference on Computer Vision, 2021. [22]C.-C. Tsai, T.-H. Wu, and S.-H. Lai, â€œMulti- scale patch-based representation learning for image anomaly detection and segmentation,â€ In WACV, 2022. [23]T. Defard, A. Setkov, A. Loesch, and R. Audigier, â€œPadim: a patch distribution modeling framework for anomaly detection and localization,â€In 1st International Work- shop on Industrial Machine Learning, 2021. [24]K. Roth, L. Pemula, J. Zepeda, B. Scholkopf, T. Brox, and P. Gehler, â€œTowards total recall in industrial anomaly detection,â€In CVPR, 2022. [25]M. Rudolph, B. Wandt, and B. Rosenhahn, â€œSame same but differnet: Semi-supervised defect detection with normalizing flows,â€In IEEE Winter Conference on Application of Computer Vision, 2021. [26]D. Gudovskiy, S. Ishizaka, and K. Kozuka, â€œCflow-ad: Real-time unsupervised anomaly detection with localization via conditional nor- malizing flows,â€In IEEE Winter Conference on Application of Computer Vision, 2022. [27]J. Yu, Y. Zheng, X. Wang, W. Li, Y. Wu, R. Zhao, and L. Wu, â€œFastflow: Unsuper- vised anomaly detection and localization via 2d normalizing flows,â€arXiv preprint arXiv:2111.07677, 2021. [28]P. Bergmann, M. Fauser, D. Sattlegger, and C. Steger, â€œUninformed students: Student- teacher anomaly detection with discriminative latent embeddings,â€In CVPR, 2020. [29]M. Salehi, N. Sadjadi, S. H. Rohban, and H. R.Rabiee, â€œMultiresolution knowledge dis- tillation for anomaly detection,â€In CVPR, 2021. [30]G. Wang, S. Han, E. Ding, and D. Huang, â€œStudent-teacher feature pyramid matching for unsupervised anomaly detection,â€In British Machine Vision Conference, 2021. [31]H. Deng and X. Li, â€œAnomaly detection via reverse distillation from one-class embedding,â€ In CVPR, 2022. [32]T. D. Tien, A. T. Nguyen, N. H. Tran, T. D. Huy, S. T. Duong, C. D. T. Nguyen, and S. Q. Truong, â€œRevisting reverse distillation for anomaly detection,â€In CVPR, 2023.[33]M. Rudolph, T. Wehrbein, B. Rosenhahn, and B. Wandt, â€œAsymmetric student-teacher net- works for industrial anomaly detection,â€In WACV, 2023. [34]Z. You, L. Cui, Y. Shen, K. Yang, X. Lu, Y. Zheng, and X. Le, â€œA unified model for multi-class anomaly detection,â€In NeurIPS, 2022. [35]Y. Zhao, â€œOminal: A unified cnn framework for unsupervised anomaly localization,â€In CVPR, 2023. [36]X. Yao, C. Zhang, R. Li, J. Sun, and Z. Liu, â€œOne-for-all: Proposal masked cross- class anomaly detection,â€In AAAI, 2023. [37]X. Yao, R. Li, Z. Qian, L. Wang, and C. Zhang, â€œHierarchical gaussian",
    "anomaly detection,â€In NeurIPS, 2022. [35]Y. Zhao, â€œOminal: A unified cnn framework for unsupervised anomaly localization,â€In CVPR, 2023. [36]X. Yao, C. Zhang, R. Li, J. Sun, and Z. Liu, â€œOne-for-all: Proposal masked cross- class anomaly detection,â€In AAAI, 2023. [37]X. Yao, R. Li, Z. Qian, L. Wang, and C. Zhang, â€œHierarchical gaussian mixture normalizing flow modeling for unified anomaly detection,â€ In ECCV, 2024. [38]H. He, J. Zhang, H. Chen, X. Chen, Z. Li, X. Chen, Y. Wang, C. Wang, and L. Xie, â€œDiad: A dffisuion-based framework for multi- class anomaly detection,â€In AAAI, 2024. [39]H. He, Y. Bai, J. Zhang, Q. He, H. Chen, Z. Gan, C. Wang, X. Li, G. Tian, and L. Xie, â€œMambaad: Exploring state space models for multi-class unsupervised anomaly detection,â€ arXiv preprint arXiv: 2404.06564, 2024. [40]K. T. Baghaei, A. Payandeh, P. Fayyazsanavi, S. Rahimi, Z. Chen, and S. B. Ramezani, â€œDeep representation learning: Fundamentals, perspectives, applications, and open chal- lenges,â€arXiv preprint arXiv:2211.14732, 2022. [41]D. M. Tax and R. P. Duin, â€œSupport vector data description,â€In Machine Learning, pp. 45â€“66, 2004. [42]L. Dinh, J. Sohl-Dickstein, and S. Bengio, â€œDensity estimation using real nvp,â€In Inter- national Conference on Learning Representa- tions, 2017. [43]X. Yao, Z. Chen, C. Gao, G. Zhai, and C. Zhang, â€œResad: A simple framework for class generalizable anomaly detection,â€In NeurIPS, 2024. [44]W. Liu, R. Li, M. Zheng, S. Karanam, Z. Wu, B. Bhanu, R. J. Radke, and O. Camps, â€œTowards visually explaining vari- ational autoencoders,â€In CVPR, 2020. [45]B. SchÂ¨olkopf, J. C. Plattz, J. Shawe-Taylory, A. J. Smolax, and R. C. Williamsonx, â€œEsti- mating the support of a high-dimensional 24 distribution,â€In Neural Computation, p. 1443â€“1471, 2001. [46]L. Ruff, R. A. Vandermeulen, B. J. Franks, K.- R. Muller, and M. Kloft, â€œRethinking assump- tions in deep anomaly detection,â€arXiv preprint arXiv:2006.00339, 2020. [47]N. Cohen and Y. Hoshen, â€œSub-image anomaly detection with deep pyramid correspondences,â€arXiv preprint arXiv: 2005.02357v3, 2020. [48]L. Bergman, N. Cohen, and Y. Hoshen, â€œDeep nearest neighbor anomaly detection,â€arXiv preprint arXiv: 2002.10445, 2020. [49]O. Rippel, P. Mertens, and D. Merhof, â€œMod- eling the distribution of normal data in pre-trained deep features for anomaly detec- tion,â€In 25th International Conference on Pattern Recognition, 2020. [50]M. Rudolph, T. Wehrbein, B. Rosenhahn, and B. Wandt, â€œFully convolutional cross-scale- flows for image-based defect detection,â€In IEEE Winter Conference on Application of Computer Vision, 2022. [51]R. Lu, Y. Wu, L. Tian, D. Wang, B. Chen, X. Liu, and R. Hu, â€œHierarchical vector quan- tized transformer for multi-class unsupervised anomaly detection,â€In NeurIPS, 2023. [52]L. He, Z. Jiang, J. Peng, W. Zhu, L. Liu, Q. Du, X. Hu, M. Chi, Y. Wang, and C. Wang, â€œLearning unified reference representation for unsupervised multi-class anomaly detection,â€ In ECCV, 2024. [53]J. Guo, S. Lu, W. Zhang, F. Chen, H. Li, and H. Liao, â€œDinomaly: The less is more phi- losophy in multi-class unsupervised anomaly detection,â€In CVPR, 2025. [54]W. Luo, Y. Cao, H. Yao, X. Zhang, J. Lou, Y. Cheng, W. Shen, and W. Yu, â€œExploring intrinsic normal prototypes within a single image for universal anomaly detection,â€In CVPR, 2025. [55]W. Luo, H. Yao, Y. Cao, Q. Chen, A. Gao, W. Shen, W. Zhang, and W.",
    "detection,â€In CVPR, 2025. [54]W. Luo, Y. Cao, H. Yao, X. Zhang, J. Lou, Y. Cheng, W. Shen, and W. Yu, â€œExploring intrinsic normal prototypes within a single image for universal anomaly detection,â€In CVPR, 2025. [55]W. Luo, H. Yao, Y. Cao, Q. Chen, A. Gao, W. Shen, W. Zhang, and W. Yu, â€œInp- former++: Advancing universal anomaly detection via intrinsic normal prototypes and residual learning,â€arXiv preprint arXiv: 2506.03660, 2025. [56]C. Huang, H. Guan, A. Jiang, Y. Zhang, M. Spratling, and Y. Wang, â€œRegistration based few-shot anomaly detection,â€In ECCV, 2022.[57]J. Jeong, Y. Zou, T. K. D. Zhang, A. Ravichan- dran, and O. Dabeer, â€œWinclip: Zero-/few- shot anomaly classification and segmentation,â€ In CVPR, 2023. [58]X. Chen, Y. han, and J. Zhang, â€œA zero-/few- shot anomaly classification and segmentation method for cvpr 2023 vnad workshop chal- lenge tracks 1&2: 1st place on zero-shot ad and 4th place on few-shot ad,â€In CVPR Workshop, 2023. [59]H. Deng, Z. Zhang, J. Bao, and X. Li, â€œAnovl: Adapting vision-language models for unified zero-shot anomaly localization,â€arXiv preprint arXiv: 2308.15939, 2023. [60]A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever, â€œLearning transferable visual models from natural language supervision,â€In ICML, 2021. [61]Q. Zhou, G. Pang, Y. Tian, S. He, and J. Chen, â€œAnomalyclip: Object-agnostic prompt learn- ing for zero-shot anomaly detection,â€In ICLR, 2024. [62]X. Chen, J. Zhang, G. Tian, H. He, W. Zhang, Y. Wang, C. Wang, and Y. Liu, â€œA language- guided staged dual-path model for zero-shot anomaly detection,â€arXiv preprint arXiv: 2311.00453, 2023. [63]S. Li, J. Cao, P. Ye, Y. Ding, C. Tu, and T. Chen, â€œClipsam: Clip and sam collabo- ration for zero-shot anomaly segmentation,â€ arXiv preprint arXiv: 2401.12665, 2024. [64]Z. Gu, B. Zhu, G. Zhu, Y. Chen, H. Li, M. Tang, and J. Wang, â€œFilo: Zero-shot anomaly detection by fine-grained description and high-quality localization,â€arXiv preprint arXiv: 2404.13671, 2024. [65]X. Li, Z. Huang, F. Xue, and Y. Zhou, â€œMusc: Zero-shot industrial anomaly classification and segmentation with mutual scoring of the unlabeled images,â€In ICLR, 2024. [66]J. Zhu and G. Pang, â€œToward generalist anomaly detection via in-context residual learning with few-shot sample prompts,â€In CVPR, 2024. [67]J. Wang, C. Lan, C. Liu, Y. Ouyang, T. Qin, W. Lu, Y. Chen, W. Zeng, and P. S. Yu, â€œGeneralizing to unseen domains: A survey on domain generalization,â€IEEE Trans. Knowl. Data Eng., 2021. 25 [68]P. J.Huber, â€œRobust estimation of a loca- tion parameter,â€Annals of Mathematical Statistics, 1964. [69]B. Stephen and V. Lieven, â€œConvex optimiza- tion,â€Cambridge University Press eBooks, 2004. [70]R. Vershynin, â€œHigh-dimensional probability: An introduction with applications in data sci- ence,â€Cambridge University Press eBooks, 2018. [71]B. Laurent and P. Massart, â€œAdaptive esti- mation of a quadratic functional by model selection,â€Annals of Statistics, 2000. [72]Y. Zhang, Y. S. J. Cai, and J. Fan, â€œDeep orthogonal hypersphere compression for anomaly detection,â€In ICLR, 2024. [73]K. Zhou, Y. Yang, Y. Qiao, and T. Xiang, â€œDomain generalization with mixstyle,â€In ICLR, 2021. [74]Y. Zhang, M. Li, R. Li, K. Jia, and L. Zhang, â€œExact feature distribution matching for arbi- trary style transfer and domain generaliza- tion,â€In",
    "J. Fan, â€œDeep orthogonal hypersphere compression for anomaly detection,â€In ICLR, 2024. [73]K. Zhou, Y. Yang, Y. Qiao, and T. Xiang, â€œDomain generalization with mixstyle,â€In ICLR, 2021. [74]Y. Zhang, M. Li, R. Li, K. Jia, and L. Zhang, â€œExact feature distribution matching for arbi- trary style transfer and domain generaliza- tion,â€In CVPR, 2022. [75]T. Cao, J. Zhu, and G. Pang, â€œAnomaly detection under distribution shift,â€In ICCV, 2023. [76]X. Huang and S. Belongie, â€œArbitrary style transfer in real-time with adaptive instance normalization,â€In ICCV, 2017. [77]M. Lu, H. Zhao, A. Yao, Y. Chen, F. XU, and L. Zhang, â€œA closed-form solution to universal style transfer,â€In ICCV, 2019. [78]A. van den Oord, O. Vinyals, and K. Kavukcuoglu, â€œNeural discrete representation learning,â€In NeurIPS, 2017. [79]J. Yu, X. Li, J. Y. Koh, H. Zhang, R. Pang, J. Qin, A. Ku, Y. Xu, J. Baldridge, and Y. Wu, â€œVector-quantized image modeling with improved vqgan,â€In ICLR, 2022. [80]D. P. Kingma and P. Dhariwal, â€œGlow: Gen- erative flow with invertible 1x1 convolutions,â€ In Conference and Workshop on Neural Infor- mation Processing Systems, 2019. [81]T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Dollar, â€œFocal loss for dense object detection,â€In ICCV, 2017. [82]P. Mishra, R. Verk, D. Fornasier, C. Piciarelli, and G. L. Foresti, â€œVt-adl: A vision trans- former network for image anomaly detection and localization,â€In 30th IEEE International Symposium on Industrial Electronics, 2021.[83]S. Jezek, M. Jonak, R. Burget, P. Dvorak, and M. Skotak, â€œDeep learning-based defect detection of metal parts: evaluating current methods in complex conditions,â€In 2021 13th International congress on ultra modern telecommunications and control systems and workshops (ICUMT), 2021. [84]P. Bergmann, K. Batzner, M. Fauser, D. Sat- tlegger, and C. Steger, â€œBeyond dents and scratches: Logical constraints in unsupervised anomaly detection and localization,â€In IJCV, 2022. [85]D. Jha, P. H. Smedsrud, M. A. Riegler, P. Halvorsen, T. de Lange, D. Johansen, and H. D. Johansen, â€œKvasir-seg: A seg- mented polyp dataset,â€In 26th International Conference on Multimedia Modeling, 2020. [86]B. H. Menze, A. Jakab, S. Bauer, J. Kalpathy- Cramer, K. Farahani, J. S. Kirby, Y. Burren, N. Porz, J. Slotboom, R. Wiest, L. Lanczi, E. R. Gerstner, M. Weber, T. Arbel, B. B. Avants, N. Ayache, P. Buendia, D. L. Collins, N. Cordier, J. J. Corso, A. Criminisi, T. Das, H. Delingette, C Â¸. Demiralp, C. R. Durst, M. Dojat, S. Doyle, J. Festa, F. Forbes, E. Geremia, B. Glocker, P. Golland, X. Guo, A. Hamamci, K. M. Iftekharuddin, R. Jena, N. M. John, E. Konukoglu, D. Lashkari, J. A. Mariz, R. Meier, S. Pereira, D. Precup, S. J. Price, T. R. Raviv, S. M. S. Reza, M. T. Ryan, D. Sarikaya, L. H. Schwartz, H. Shin, J. Shot- ton, C. A. Silva, N. J. Sousa, N. K. Subbanna, G. SzÂ´ ekely, T. J. Taylor, O. M. Thomas, N. J. Tustison, G. B. Â¨Unal, F. Vasseur, M. Winter- mark, D. H. Ye, L. Zhao, B. Zhao, D. Zikic, M. Prastawa, M. Reyes, and K. V. Leemput, â€œThe multimodal brain tumor image segmenta- tion benchmark (brats),â€IEEE Transactions on Medical Imaging, 2015. [87]K. Pogorelov, K. R. Randel, C. Griwodz, S. L. Eskeland,",
    "Tustison, G. B. Â¨Unal, F. Vasseur, M. Winter- mark, D. H. Ye, L. Zhao, B. Zhao, D. Zikic, M. Prastawa, M. Reyes, and K. V. Leemput, â€œThe multimodal brain tumor image segmenta- tion benchmark (brats),â€IEEE Transactions on Medical Imaging, 2015. [87]K. Pogorelov, K. R. Randel, C. Griwodz, S. L. Eskeland, T. de Lange, D. Johansen, C. Spampinato, D. Dang-Nguyen, M. Lux, P. T. Schmidt, M. Riegler, and P. Halvorsen, â€œKvasir: A multi-class image dataset for com- puter aided gastrointestinal disease detection,â€ In Proceedings of the 8th ACM on Multimedia Systems Conference (MMSys), 2017. [88]R. Girdhar, A. El-Nouby, Z. Liu, M. Singh, K. V. Alwala, A. Joulin, and I. Misra, â€œImage- bind: One embedding space to bind them all,â€ In CVPR, 2023. 26 [89]D. P.Kingma and J. L. Ba, â€œAdam: A method for stochastic optimization,â€In ICLR, 2015. [90]L. Ardizzone, C. LÂ¨ uth, J. Kruse, C. Rother, and U. KÂ¨ othe, â€œGuided image generation with conditional invertible neural networks,â€arXiv preprint arXiv: 1907.02392, 2019. [91]S. Zagoruyko and N. Komodakis, â€œWide residual networks,â€In BMVC, 2016. [92]J. H. Hanxi Li, B. Li, H. Chen, Y. Zheng, and C. Shen, â€œTarget before shooting: Accurate anomaly detection and localization under one millisecond via cascade patch retrieval,â€arXiv preprint arXiv: 2308.06748, 2023. [93]Z. Liu, Y. Zhou, Y. Xu, and Z. Wang, â€œSim- plenet: A simple network for image anomaly detection and localization,â€In CVPR, 2023. 27"
  ]
}