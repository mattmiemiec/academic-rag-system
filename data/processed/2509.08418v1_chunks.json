{
  "filename": "2509.08418v1.pdf",
  "total_chunks": 19,
  "text_length": 59813,
  "chunks": [
    "FACET:HIGHLY EFFICIENTE(3)-EQUIVARIANT NETWORKS FOR ACCELERATED TRAINING OF INTERATOMIC POTENTIALS Nicholas Miklaucic1, Lai Wei1, Rongzhi Dong1, Nihang Fu1, Sadman Sadeed Omee1, Qingyang Li1, Sourin Dey1, Victor Fung2Jianjun Hu1∗ 1Department of Computer Science and Engineering, University of South Carolina, Columbia, SC, USA 2Computational Science and Engineering, Georgia Institute of Technology, Atlanta, GA, USA ABSTRACT Computational materials discovery is severely constrained by the high expense of first-principles calculations. Machine learning (ML) potentials that accurately predict energies from crystal struc- tures offer a promising solution, but current approaches face significant computational bottlenecks. Steerable graph neural networks (GNNs) encoding geometric information with spherical harmonics respect the fundamental symmetries of atomic systems—permutation, rotation, and translation—for more physically realistic predictions. Maintaining equivariance throughout these networks, however, presents substantial design challenges: standard network components such as activation functions require modification, and each network layer must accommodate multiple data types corresponding to different harmonic orders. We introduce Facet, a novel GNN architecture for efficient ML potentials developed through systematic analysis of existing steerable GNN designs. Our key innovations include replacing compute-intensive multi-layer perceptrons (MLPs) for processing interatomic distances with efficient splines that achieve equivalent performance while dramatically reducing computational and memory requirements. We also propose a general-purpose equivariant layer that mixes node information via spherical grid projection followed by standard MLPs—an approach that outperforms iterative tensor products in speed and surpasses linear and gate layers in expres- siveness. Training on the MPTrj dataset, our model achieves performance comparable to leading approaches with significantly fewer parameters and less than 10% of the training computation. We demonstrate a 2× speedup compared to MACE models on a crystal relaxation task representative of crystal structure prediction workflows. We validate our design through systematic experiments with existing steerable GNNs, demonstrating that SevenNet-0’s parameter count can be reduced by over 25% without performance degradation. These techniques enable more than 10-fold acceleration in training large-scale foundation models for ML potentials, potentially transforming the landscape of computational materials discovery. Introduction First-principles-based atomic simulations such as Density Functional Theory (DFT) and ab initio Molecular Dynamics (MD) have been playing an increasingly important role in materials and chemical discovery [1–3], providing critical tools to predict and understand material structures and properties at the atomic and electronic levels. Their wide application [4–6] has been severely limited, however, by their extremely high computational complexity, which can render them infeasible for simulating large complex crystals or molecules [7, 8]. To accelerate these atomic simulations and consequently the materials discovery process, machine learning interatomic potentials (MLIPs) have been developed that predict energies, forces, and other properties without the huge computa- tional cost of first-principles calculations or the expense of experiments [9–17]. Among these machine learning (ML) potentials, deep neural network models have demonstrated superior performance [18–21] in modeling the complex underlying physics and predicting materials and chemical properties, providing a convenient mechanism to handle the symmetries of the problem via network design. An examination of the latest Matbench-Discovery leaderboard [22] ∗Corresponding author: jianjunh@cse.sc.eduarXiv:2509.08418v1 [cond-mat.mtrl-sci] 10 Sep 2025 Facet: highly efficientE(3)-equivariant networks for interatomic potentials shows that deep neural network-based models have made tremendous progress in",
    "and chemical properties, providing a convenient mechanism to handle the symmetries of the problem via network design. An examination of the latest Matbench-Discovery leaderboard [22] ∗Corresponding author: jianjunh@cse.sc.eduarXiv:2509.08418v1 [cond-mat.mtrl-sci] 10 Sep 2025 Facet: highly efficientE(3)-equivariant networks for interatomic potentials shows that deep neural network-based models have made tremendous progress in their performance for high-throughput materials discovery, specifically focusing on predicting the stability of inorganic crystals. Most performance metrics have been improved by 200% in the past 30 months: for example, energy prediction error RMSD has been improved from 0.112 meV/atom for M3GNet [19] to 0.061 meV/atom for eSEN-30M-OAM [23], MAEs reduced from 0.075 meV/atom to 0.018 meV/atom, with the combined performance score (CPS) increased from 0.428 (M3GNet) to 0.888 (eSEN). These advances are made possible through three main approaches of the foundational model strategy: data, model, and computing resources. The state-of-the-art model eSEN-30M-OAM is trained with 113M structures (from 66M stable structures) while M3GNet is trained with 118K structures (from 62.8K stable structures). For each performance metric, we found that the top four models were all trained with huge training sets containing more than 100M samples/structures, which also showed superior performance in downstream applications [24]. Training such models requires huge computational resources and time: for example, the MACE-MP-0 model (see Table 1) was trained on A100 for 310 days using 1.58M training samples while SevenNet-0 was trained for 90 days on A100. Such huge computing requirements make it challenging to quickly update models or iterate on models by exploring different training data, tuning model parameters, and training algorithms. While the ML potential community has been focusing on improving the predictive capabilities, generality, transferability, and scalability of such ML potentials [5, 7, 18], the importance of training large atomic foundation ML potential models makes it imperative to develop ML potential models that allow much more efficient training and inference, similar to the DeepSeek moment in large language models (LLMs) [25–27]. How can one train a high-performance model with less than 10% of the computing resources required to train current SOTA foundation models? To address this issue, several recent works have been proposed that focus on parallelization and distributed computing architectures for ML potential acceleration [28, 29]. In this work, we focus on model architecture development for building efficient ML potentials. One of the main strategies to improve model efficiency is increasing sample and parameter efficiency by implementing transformation (rotation and translation) invariance or equivariance. For example, graph neural network (GNN) architectures have been widely used to ensure invariance to permutation of nodes and edges as well as rotation and translation. The periodic symmetries of crystalline materials can additionally be incorporated by using periodic graphs [30], with nodes representing a set of symmetrically equivalent atoms and edges representing relative spatial positions between atoms. The use of relative spatial positions instead of absolute positions reflects the underlying translational invariance: any origin is fundamentally arbitrary. Many material properties also exhibit rotational symmetry, which is more challenging to ensure through neural network design. Early approaches relied onscalarization: restricting the model inputs to invariants under rotation, in the",
    "use of relative spatial positions instead of absolute positions reflects the underlying translational invariance: any origin is fundamentally arbitrary. Many material properties also exhibit rotational symmetry, which is more challenging to ensure through neural network design. Early approaches relied onscalarization: restricting the model inputs to invariants under rotation, in the way that the periodic graph construction ensures translation invariance by removing explicit coordinate information. CGCNN [31], for example, only considers the lengths of edges and discards directional information. This information loss necessarily limits the expressivity of the network, however, as shown through Weisfeiler-Leman isomorphism: there exist distinct graphs with different chemistry that element type and distance alone are insufficient to distinguish [32]. Considering the importance of angular information for material stability, strongly incorporating angular information should provide useful inductive biases for a network. To address these limitations, subsequent work [33–35] has introduced increasingly sophisticated scalarization techniques: incorporating angles between triplets of nodes [35] and even dihedral angles between quadruplets of nodes [34]. These techniques are effective, but they result in increasingly complex and expensive architectures that scale poorly with the size of the system being considered. Concurrently, asteerableGNN approach [12–14, 36] has avoided scalarization entirely. Using irreducible represen- tations (irreps) of the group of 3D proper rotations SO(3) based on spherical harmonics, the information of how an output varies under rotations to the inputs can be tracked. The Clebsch-Gordan tensor product combines these representations equivariantly, generalizing operations such as the dot product and cross product. By representing edges as spherical tensors, a simple graph convolutional network without complex scalarization schemes can still encode important geometric information. These models have proven successful in many problems with atomistic systems as input and include MACE-MP-0 [13], SevenNet [14], GNoME [36], and NequIP[12]. The simplicity of these network architectures, which do not need complex scalarization schemes or explicit many-order summations, masks the complexity of their underlying components. Most standard numerical operations do not preserve equivariance when applied to irreps. For example, a foundation of modern deep learning—the nonlinear activation function—has no direct equivalent for irreps, as applying element-wise operations to irreps depends on the arbitrary coordinate system. These limitations complicate the transfer of successful GNN architectures from other domains to materials property prediction. These constraints do simplify the development of new architectures: equivariance imposes strict restrictions on how information can flow and enables direct comparison of many existing MLIPs. In this paper, we make several contributions to the design of steerable GNNs for significant acceleration of high- performance machine learning potentials: 2 Facet: highly efficientE(3)-equivariant networks for interatomic potentials •We identify that the most computationally expensive part of many steerable GNNs is the message filter used in equivariant convolution, which can be greatly simplified without significant performance impacts. We demonstrate this by modifying a pre-trained SevenNet model, SevenNet-0, reducing the parameter count by over 25% with negligible change in accuracy. •We consider the trade-off between computational complexity and expressiveness in the part of the network that mixes information within an individual node. By applying multi-layer perceptrons (MLPs) to a regular spherical grid encoding directional information, we",
    "model, SevenNet-0, reducing the parameter count by over 25% with negligible change in accuracy. •We consider the trade-off between computational complexity and expressiveness in the part of the network that mixes information within an individual node. By applying multi-layer perceptrons (MLPs) to a regular spherical grid encoding directional information, we achieve a middle ground between the expensive tensor product operations used in MACE and the simpler nonlinear gate operations used in SevenNet and GNoMe. •Using the above insights, we propose and train a new deep learning potential named Facet, which achieves comparable high performance to much larger SOTA deep learning atomic potentials while using only a fraction of their parameter size and can be trained with less than 10% of their training cost, e.g., reducing the training time from SevenNet-0’s more than 90 days to 2 days when trained with the same number of training samples. Results Model architecture overview of Facet We begin by delineating our Facet potential model from prior work. While we base our approach on the successful SevenNet [14], we differ from the previous state of the art in several ways. To update node representations between message-passing steps, we use a general, expressive S2-MLP-Mixer layer explained below. This approach contrasts with the MACE model [13], which uses an expensive symmetric tensor product layer for the same purpose, and SevenNet/GNoMe/NequIP [12, 14, 36], which use nonlinear gate layers that do not mix non-scalars with each other. As in SevenNet, we apply the same node update to all nodes regardless of element type, which reduces the number of parameters required by two orders of magnitude and additionally mitigates overfitting. All of the aforementioned deep learning potential models share a message-passing convolution with essentially the same architecture. We find that the MLP computing weights from edge distance, the most expensive part of message passing in most such networks—and by extension one of the most expensive parts of the network—is unnecessary and can be replaced with a linear layer without any significant change to performance. These large changes, combined with many small optimizations designed with the aim of training efficiency without sacrificing expressivity, make our Facet architecture more effective at smaller sizes than previous networks trained on the same data, achieving both model parameter efficiency and training efficiency. The architecture of our model is shown in Figure 1. Figure 1: Architecture overview of Facet. Information is stored only for individual nodes. Geometric information is incorporated through message-passing blocks, updating each node with information from its local environment. Information from that environment is then mixed within each node through a self-interaction block. After N=3 interactions and self-interactions, each node’s embedding is a 128-dimensional vector processed to obtain energy. Facet encodes its input as a periodic orbit graph: each atom within the repeating unit cell is represented by a node. Edges are added to link each node with its 32 nearest neighbors: more than one edge can exist between two nodes, each referring to a different unit cell offset [30]. Each node’s atomic element is encoded with a 128-dimensional embedding trained from scratch. This",
    "cell is represented by a node. Edges are added to link each node with its 32 nearest neighbors: more than one edge can exist between two nodes, each referring to a different unit cell offset [30]. Each node’s atomic element is encoded with a 128-dimensional embedding trained from scratch. This initializes the node features as 128 scalars. Then, to incorporate geometric information, the interaction block combines this data with edge information. The interaction block updates the node features with incoming messages from neighbors to form node representations that contain non-scalar information in addition to scalars. Then this information is processed within each node by applying a node self-interaction layer to each node independently. The combination of message passing and self-interaction forms a single GNN block, which can be stacked to form a deeper network. For predicting energy or any other scalar, the last GNN block need only process 3 Facet: highly efficientE(3)-equivariant networks for interatomic potentials scalar values, producing a 128-dimensional scalar embedding for each node. These are processed individually by a readout layer to predict node energies and then summed across a structure to obtain the predicted energy. In the Facet model, we apply a normalization and a residual connection after each layer. Following EquiformerV2 [37], we use equivariant layer normalization with scalars treated separately. This normalizes the magnitudes of each kind of irrep to average 1 and then scales by a learned irrep-wise constant. This is then added to a residual connection, padding the layer input with zeros if needed to match the shape of the layer output. By initializing the layer scale to 0, each GNN block becomes a simple identity function at the start of training. This initialization simplifies training for deep networks by ensuring block outputs start with approximately unit scale and zero mean. To convert these approximately unit-scale outputs to energy, each node’s energy prediction is scaled by a species-wise constant with a species-wise shift, added together across the crystal, and then scaled and shifted by a global value. These are set initially with simple linear regression but set as trainable to correct for any biases introduced by the simplicity of the original fit, which assumes average atom energy is independent of other atoms in the composition. Interaction block: steerable graph convolution, streamlined Figure 2: The Facet interaction block. Edge vectors are decomposed into a directional component, embedded into spherical harmonics, and a distance component, encoded using a Bessel basis with a smooth distance cutoff so edges withr≤r max have no effect. The node features Hiand the spherical harmonics Yijare equivariantly combined using the Clebsch-Gordan tensor product. Each output is then weighted with a filter generated from a linear projection of the distance embedding. The resulting messages are then summed for each node and normalized by the estimated average degree. The average degree depends onr max, the cutoff distance for edges to be considered, and is estimated by interpolating between a set of known degrees for different values of rmax. The average degree is raised to the power p= 0.7 to better model the correlation between",
    "the estimated average degree. The average degree depends onr max, the cutoff distance for edges to be considered, and is estimated by interpolating between a set of known degrees for different values of rmax. The average degree is raised to the power p= 0.7 to better model the correlation between messages. Then the new node features are made by adding a linear combination of the old node features to the normalized messages. rmax and the frequencies of the individual Bessel basis functions are trained as any other parameter. The interaction block, depicted in Figure 2, elaborates on the basic convolution described in NequIP [12]. Node features and geometric information are combined using a Clebsch-Gordan tensor product, which fully characterizes all equivariant linear functions. Each output is weighted according to a filter generated from the distance associated with each edge, so the contribution of a specific component of the message varies smoothly with radius. Given the average number of neighbors k, we divide the sum of incoming messages by k0.7, a middle ground between values of√ kandk used by NequIP and MACE respectively. Instead of maintaining a single k, we compute this value for several choices ofrmax and then linearly interpolate between them to estimate the effective kfor any rmax. This lets us vary rmax during training without adverse effects on normalization of messages, increasing the model’s flexibility with only a single additional parameter. 4 Facet: highly efficientE(3)-equivariant networks for interatomic potentials Of particular note is the use of a simple linear layer instead of an MLP for the filter generation. We find that a simple linear layer has negligible loss of expressivity (99% of variance explained for SevenNet-0’s model checkpoint) and has dramatic performance benefits: the total compute, peak memory use, and parameter counts are all significantly improved. Further discussion of these design choices and their impact can be found in Methods. Node self-interaction: theS2-MLP-Mixer Figure 3: a) The Facet self-interaction block. Following EquiformerV2, scalars are treated separately. The initial linear layer mixes information across channels of the same type and maps so the non-scalar inputs have the same count. To reduce overfitting and speed up the model, the input to the S2-MLP-Mixer block is split into halves consisting of 16 groups of irreps each, processed separately, and then recombined. b) The Facet nonlinearity used in self-interaction, dubbed S2-MLP-Mixer. Different irreps are combined and projected on the sphere S2. This is then reinterpreted as a NA×NB×C image, with NA, NB= 18,17 controlling the fineness of the spherical grid. A channel-wise MLP in this space mixes inputs equivariantly, after which the outputs are projected back into the original basis. To process the information received during message passing and allow messages sent to the same node to mix, a self-interaction step is required: a node-wise update function that mixes information within a single environment. Standard GNNs [35] apply an MLP. This works when the representation is entirely scalar, and so Facet uses a standard 2-layer MLP for the last layer’s self-interaction, but standard MLPs are not equivariant and as such cannot process geometric information.",
    "node-wise update function that mixes information within a single environment. Standard GNNs [35] apply an MLP. This works when the representation is entirely scalar, and so Facet uses a standard 2-layer MLP for the last layer’s self-interaction, but standard MLPs are not equivariant and as such cannot process geometric information. Instead, Facet uses the correspondence between irreps and spherical harmonics, a basis of functions on the sphere S2(Figure 3 b). By projecting each input irrep to a function then discretized to a spherical grid, a collection of irreps can be interpreted as a spherical image with individual channels. Any function applied across the entire sphere corresponds to an equivariant function, treating all rotations equally. As such, a simple MLP can be applied channel-wise to mix the different irreps, after which the output is projected back into the space of the input. The core idea of projection to S2is not new: it was proposed in [38] and incorporated into a property prediction model in EquiformerV2 [37] as S2activation. To indicate the use of a full MLP instead of a single nonlinearity applied to the channels, and to connect it with MLP-Mixer architectures used in the computer vision literature, we call our block S2-MLP-Mixer. To our knowledge, it has yet to be used as the nonlinearity within a message-passing neural network, and we believe this choice offers several advantages over other common approaches. The MACE architecture uses a symmetric tensor-product layer, computing a linear function of Hi+ (H i⊗H i) + (H i⊗H i⊗H i) +. . . to a given max degree [39]. This operation is flexible, and can approximate any equivariant function through a Taylor approximation, but computing these tensor products is extremely expensive. NequIP and subsequent similar networks (GNoMe, SevenNet) use a nonlinear gate function, scaling each non-scalar by a weight derived from an MLP applied to the scalars. This operation is significantly cheaper, but only considers the norm of each non-scalar and does not mix non-scalars with one another. The S2-MLP-Mixer block is nonlinear and mixes all dimensions of the input without using a full tensor product. The computational difficulty in applying the S2-MLP-Mixer is the need to apply the MLP across each of the 18×17 spherical grid locations used in Facet. We find a simple way to reduce the expense of the computation: because individual channels can be easily mixed with simple linear layers, we can split the input into heads and only mix information within a single head. 5 Facet: highly efficientE(3)-equivariant networks for interatomic potentials Figure 3 a) shows how this nonlinearity is incorporated into the larger self-interaction block. Following EquiformerV2, the scalars are processed separately and recombined after mixing. Additional details of our Facet model are described in the Methods Section including representation, message generation, MLP convolution, message aggregation, node updating, element embeddings, and model readouts. Model Performance To fairly assess our design, we compare our models to other steerable GNNs trained on the MPTrj dataset containing 1,580,395 structures from the trajectories of 145,923 unique stable crystals. For objective evaluation, it should be noted that the",
    "convolution, message aggregation, node updating, element embeddings, and model readouts. Model Performance To fairly assess our design, we compare our models to other steerable GNNs trained on the MPTrj dataset containing 1,580,395 structures from the trajectories of 145,923 unique stable crystals. For objective evaluation, it should be noted that the two SevenNet models’ test set performance may be over-estimated due to possible data leakage. SevenNet-0’s performance is reported directly from the authors, specifically from the July 2024 checkpoint. They did not hold out any data, so it is difficult to estimate performance on new data. To more fairly compare SevenNet-0 to our work, we fine-tune starting from the SevenNet-0 checkpoint after making severalpost hocchanges. We fit a spline to the convolution filter MLPs used in SevenNet, collapse the two output linear layers into a single one, and re-normalize the weights to match the average number of neighbors of our dataset. We setrmax to the original value of 5 angstroms but, unlike SevenNet-0, let rmax vary. The resulting model, SevenNet- Streamlined, is fine-tuned for 12 epochs on the same dataset splits and objective as Facet (MAE of formation energy). Because the original model was trained without holding out data, it is unclear how much residual knowledge from the checkpoint remains in the fine-tuned and modified version. In addition, MACE-MP-0 refers to the \"small\" version MACE model trained for 250 epochs, of which the last 50 emphasize energy prediction. This makes it a good comparison for our model trained on energy alone. MACE-MP-0 was trained for 250 epochs on a cluster of 80 NVIDIA A100 GPUs. In contrast, Facet-Small is trained from scratch, on energy prediction alone, for 25 epochs. We train starting from 3 random seeds. We report both the average performance of the individual models (Facet-Small-Average) and the performance of an ensemble averaging the predictions of the three models (Facet-Small-Ensemble). For all models we train (including SevenNet-streamlined, Facet-Small-Average, and Facet-Small-Ensemble), we use EMA [40] with a decay of 0.99, applied every 32 steps, and a batch size of 32 structures. We use the Prodigy [41] optimizer, which adaptively modifies the learning rate. The dataset is split 10:1:1 training:validation:test, such that structures from the same trajectory are put into the same split. This prevents leakage between the three sets. The training loss curve for each of the three Facet-Small models, shown in Figure 4, indicates training is relatively stable. Figure 4: Training curves for Facet-Small models, each starting from a different random initialization with all else equal and trained for 25 epochs. (Loss starts at approximately 470 meV: epoch 0 is omitted so training progress is more clearly visible). With the use of residual layers, normalization, and element-wise scaling, Facet-Small training is reliable and consistent. 6 Facet: highly efficientE(3)-equivariant networks for interatomic potentials The final performance comparison results are shown in Table 1. The top half shows the performance and training costs for selected baseline algorithms for structure-based crystal energy prediction. All models are trained with the same training set. We can observe the impact of large foundation models on improving prediction performance:",
    "The final performance comparison results are shown in Table 1. The top half shows the performance and training costs for selected baseline algorithms for structure-based crystal energy prediction. All models are trained with the same training set. We can observe the impact of large foundation models on improving prediction performance: EquiformerV2-S has the largest number of model parameters ( 31M) compared to CHGNet’s 412K parameters, while the former achieves the best performance (6 meV training error and 12.4 meV validation error, which are more than two times better than CHGNet’s 26 meV and 29 meV , respectively). On the other hand, the MACE-MP-0 model has just about 12.3% of EquiformerV2-S’s model parameters while achieving similar validation error (13 meV compared to 12.4 meV), demonstrating its model efficiency. The SevenNet-0 model is even more efficient with 2.7% of EquiformerV2-S’s parameters while achieving 11 meV training error. While the top three models all achieved impressive prediction performance, their computational costs are significant, requiring from 90 days to 310 days of training on a single Nvidia A100 GPU. This makes it extremely challenging to optimize the hyperparameters for the models or to iterate model development. In addition, the large number of model parameters in EquiformerV2-S and MACE-MP-0 usually leads to slower inference, critical for long-range molecular dynamics simulations. Here, SevenNet-0 achieves the best balance between prediction accuracy and inference time, making it a good baseline for performance comparison with our Facet models. Table 1: Comparison of formation energy prediction performance of Facet against baselines on MPTrj (meV). MACE- MP-0 refers to the \"L0-energy\" checkpoint trained in January 2024: results are from the authors’ dataset splits. SevenNet-0 is reported directly from the authors (July 2024 checkpoint), who did not hold out test data. *SevenNet- Streamlined trains with the same objective as Facet-S on the same data split, but some data leakage from the original checkpoint likely persists. Three Facet-Small models were trained: the average MAE and the performance of the ensemble are both reported. All computing costs are mapped to a single GPU. Model Params Training Compute Train Valid Test EquiformerV2-S [42] 31,207,434 180-200 days w/A100 (estimated)6 12.4N/A MACE-MP-0 3,847,696 310 days w/ A10012.5 13.0N/A SevenNet-0 (July 2024) 842,748 90 days w/ A10011N/A N/A CHGNet v0.3 [43] 412,525 8.3 days w/ A10026 29 29 SevenNet-Streamlined 616,253 90 days w/ A100 + 1 day w/ RTX 3090 10.9 20.7* 20.1* Facet-Small (Average) 270,383 2 days w/ RTX 3090 12.8 27.5 28.6 Facet-Small (Ensemble) 811,149 6 days w/ RTX 3090 10.8 24.6 23.1 The second half of Table 1 shows the performance of our Facet models compared to the closest baseline model SevenNet with the same validation and test sets. We find that our Facet-Small (ensemble) model achieves similar training error (10.8 meV versus 10.9 meV) with a similar number of model parameters, but was trained using only 6 days of RTX 3090 running time compared to the 90 days of A100 plus 1 day of RTX 3090 for SevenNet-streamlined. Its test error of 23.1 meV is also close to SevenNet-Streamlined’s 20.7 meV (which may be overestimated due",
    "similar number of model parameters, but was trained using only 6 days of RTX 3090 running time compared to the 90 days of A100 plus 1 day of RTX 3090 for SevenNet-streamlined. Its test error of 23.1 meV is also close to SevenNet-Streamlined’s 20.7 meV (which may be overestimated due to data leakage in its pretraining stage). Our Facet-Small ensemble model achieves comparable formation energy prediction performance using only 1/15 of the training cost, a 93.3% reduction in training time. In addition, we find that the Facet-Small average model also achieves strong performance with a test error of only 28.6 meV , which can be trained in only 2 days on an RTX 3090, demonstrating the high efficiency of our Facet model in both the number of model parameters and associated training cost (a 97.8% reduction in training cost). This is similar to what DeepSeek-R1 has achieved in training a high-performance reasoning LLM. We believe the techniques we proposed in building the Facet model have great potential to improve other SOTA foundation models for atomic modeling. Facet as a Fast Screening Potential for Crystal Structure Prediction One application of energy prediction models is as an objective function for crystal structure prediction (CSP) algorithms such as MAGUS [44]. Facet’s accelerated training works well for procedures that use DFT as a reference and fine-tune a deep learning potential to screen candidate structures for DFT calculations. In procedures without DFT, the potential becomes the bottleneck for crystal structure prediction: a more comprehensive search for an energy minimum demands faster inference. To evaluate Facet’s performance in this scenario, we conducted a scaling test of various deep learning potentials for crystal structure prediction. We take three MACE-MP models written in PyTorch using the authors’ library, the SevenNet-0 model implemented in the same JAX codebase as Facet-Small, and Facet-Small itself. 7 Facet: highly efficientE(3)-equivariant networks for interatomic potentials We consider two chemical systems: Ca-O-P and Fe-K-O. We take all of the stable structures in these systems from the Materials Project and consider both the original structure and a 2×2×2 supercell. This produces a range of atomic configurations at different scales indicative of a typical crystal structure prediction workflow. We rattle the atomic coordinates randomly for each configuration and conduct relaxation for 100 steps for each structure, measuring the time each step requires. We plot the median time per step for each potential and input configuration (Figure 5.) The effect of JAX can be clearly seen for small models: its compilation reduces overhead, which is quite significant for small configurations. As atomic configurations increase in scale, there are notable outliers—persistent across different runs of the experiment—that we attribute to missed low-level compilation optimizations and differences in the sparsity of atomic configurations. Despite this noise, the scaling relationships are still easily visible: as the cost of convolutions comes to dominate the runtime of the model, the differences between the models decrease (each model uses broadly the same convolution operation), although Facet-Small still remains efficient. Across all shown configurations, Facet has a significant performance edge. The mean performance edge across each",
    "easily visible: as the cost of convolutions comes to dominate the runtime of the model, the differences between the models decrease (each model uses broadly the same convolution operation), although Facet-Small still remains efficient. Across all shown configurations, Facet has a significant performance edge. The mean performance edge across each configuration is a factor of 1.35 for SevenNet and 2.35/2.93/2.73 for MACE-MP Small/Medium/Large. While not as large as the orders-of-magnitude improvements in training time, the efficiency of Facet’s architecture also accelerates inference for crystal structure prediction. Figure 5: Speed of relaxation for various crystal structures at differing scales. Especially for smaller structures, Facet-Small offers very low overhead, scaling well as size increases. Facet Learns Informative Element Embeddings From Scratch Another way to understand the Facet model’s performance is analyzing its learned elemental embeddings. Facet trains 128-dimensional embeddings for each element, initialized with no prior information, and uses them as the first node values before message passing begins. We find that learning embeddings from scratch outperforms pretrained embeddings for final model performance. Specifically, we train a version of the Facet-Small model, substituting the learned embeddings for pre-trained crystal transformer universal atomic embeddings (ct-UAE) from [45] and leaving all else identical. Compared to our three Facet- Small models, which achieved an average 12.8/27.5/28.6 meV on train/valid/test data respectively, the version using ct-UAE has 14.6/30.5/31.9 meV , moderately worse across the board. In [45], the authors find that MACE-MP similarly does not improve with their embeddings, but other models such as ALIGNN and CHGNet do exhibit improvements. We hypothesize that a sufficiently powerful backbone network obviates the need for pre-trained embeddings: our model seems able to learn the character of each element even more effectively than ct-UAE’s model. A visualization using UMAP [46] of Facet’s learned embeddings in Supplementary File Figure S5 demonstrates Facet readily learns the structure of the periodic table without the need for explicit feature engineering. 8 Facet: highly efficientE(3)-equivariant networks for interatomic potentials MLP Convolutional Filters Are Simple Splines in Disguise The message filter, weighting each element by a scalar dependent on the distance between the sender and receiver, is the only mechanism by which distance information is included in the model network. It is also applied per edge, so achieving the optimal mix of expressivity and efficiency is paramount. Distance information is crucial for understanding the chemical environment, but redundant computation has a significant impact on performance within the filter. Prior work (NequIP, GNoME, SevenNet, MACE) parameterizes Was a multi-layer perceptron (MLP) mapping from a set of radial basis functions, with no biases and activation functions that map the origin to itself. This ensures that W(0) = ⃗0as required for the cutoff function to apply. We deviate from this multi-layer convention, which we show can significantly hamper network efficiency. No matter how complex our filter function W, at heart it maps a positive real to a vector of reals. Each output component can be considered independently. Consider a single channel of the output Wi. We expect Wito be broadly smooth: a small change to the input, the edge distance, should",
    "matter how complex our filter function W, at heart it maps a positive real to a vector of reals. Each output component can be considered independently. Consider a single channel of the output Wi. We expect Wito be broadly smooth: a small change to the input, the edge distance, should have a limited effect on the output. A natural choice is then to consider Was a spline. This corresponds to an MLP with no hidden layers or activation functions. For SevenNet-Streamlined, we use the fact that we can easily enumerate the input space of the filter to fit a spline to the existing filters learned by SevenNet-0 as a 3-layer MLP. We fit a spline with 8 Bessel basis functions using linear regression. We achieve R2>0.99 , a nearly perfect fit (Figure 6). This does introduce a noticeable error in the final outputs, but with fine-tuning the model can adjust to the differences. Our Facet architecture uses the same kind of filter: an 8-basis spline. We find that, not only does a spline achieve comparable results as shown in SevenNet-Streamlined, but the smoothness that it enforces provides a useful inductive bias during training, preventing overfitting on small differences in distance between atoms. Because filters can be refit quickly with orthogonal basis functions, future work could explore gradually increasing the resolution of the filter during training—tuning small differences in distance between atoms only as the network grows more accurate. In both models, we let the frequencies of the basis functions train. This lets the model adjust the filter’s fidelity in different areas of the input distribution with only eight additional parameters. Figure 6: Comparison of expensive MLP weight functions randomly selected from SevenNet-0 (left) and a simple linear combination of 8 Bessel basis functions fit to each curve (right), after applying SevenNet-0’s distance envelope. Even a model trained using an MLP essentially chooses to use the simpler linear subspace representable by a spline. Discussion As the importance of MLIPs for materials discovery has become clear, increasingly large datasets have been developed, with increasingly large models to match [36, 37, 47]. As dataset size becomes less of a bottleneck, efficient training of large potential models with limited computational resources becomes vital. Here we find that model architecture differences and optimizations can reduce the compute power needed to train and run inference on models by an order of 9 Facet: highly efficientE(3)-equivariant networks for interatomic potentials magnitude without sacrificing performance. For screening large amounts of hypothetical materials or fine-tuning on new data, this speed directly accelerates downstream materials discovery applications. There remain many avenues for future exploration. Scaling up the Facet architecture to larger models, datasets, and different training objectives should result in higher-accuracy potentials. The complex nonlinear behavior of the S2-MLP-Mixer block complicates model interpretability: future analysis could shed new light on what makes the block effective and lead to improvements. Facet uses a simple information flow, optimizing mainly at the level of individual blocks of a message-passing network. More carefully incorporating node species, hypergraphs, or long-range interactions could address known issues of message-passing",
    "interpretability: future analysis could shed new light on what makes the block effective and lead to improvements. Facet uses a simple information flow, optimizing mainly at the level of individual blocks of a message-passing network. More carefully incorporating node species, hypergraphs, or long-range interactions could address known issues of message-passing GNNs such as over-smoothing [48, 49] and further strengthen model performance. In conclusion, through careful pruning of redundant computation and the application of gridded MLPs as general equivariant nonlinearities, we have developed the Facet architecture for steerable GNNs widely used in SOTA ML potentials. Through training a small model that compares favorably with much larger potentials in the prediction of energy, we demonstrate that property prediction models optimized for training and inference speed could continue to accelerate materials discovery and make the fruits of recent deep learning efforts more accessible to practitioners. Methods Representations ofSO(3) Before examining the Facet model design, we introduce compact notation following e3nn and review the principles that govern allowed operations on representations ofSO(3). Different geometric quantities behave differently under rotation. Some quantities are invariant, such as distance: these are scalars, and they have degree 0. Other quantities behave as vectors, changing by a rotation matrix, and have degree 1. Higher-order degrees no longer correspond directly to conventional geometry: a nine-dimensional 3x3 tensor corresponding toe.g.,stress can be decomposed into a degree-0 scalar (the trace), a degree-1 vector (the symmetric component), and a degree-2 tensor (the antisymmetric component). We focus here on the irreducible representations, abbreviatedirreps. A degree-lirrep, or tensor, has2l+ 1dimensions, which we index by−l≤m≤l. Consider, for example, two vectors ⟨a1, b1, c1⟩and⟨a2, b2, c2⟩. Many potential products exist that linearly combine the two vectors into a scalar. However, most are not equivariant. The only equivariant combination is, up to a constant scaling factor, the standard dot product:a 1a2+b 1b2+c 1c2. TheClebsch-Gordan tensor productgeneralizes this operation, describing all possible equivariant products of spherical tensors: the typical scalar and scalar-vector products, the dot product, the cross product, and so on for higher-degree tensors. Not all combinations of tensors have equivariant combinations: this product is only nontrivial for input degrees l1, l2and output degree l3when|l1−l2| ≤l 3≤l 1+l2. These tensors are representations of SO(3) , the group of proper rotations. Additionally modeling inversions simply requires tracking the parity of each tensor, odd or even, and then only allowing tensor products that follow the normal rules of parity. In this work, we use only even tensors. A collection of tensors of different degrees is thus denoted using the number of tensors of a given degree, that degree, and efor even. A collection of 128 scalars, 64 vectors, and 32 degree-2 tensors is denoted128x0e + 64x1e + 32x2e. There is a correspondence between irreps and an orthogonal basis of functions on the sphere S2, the spherical harmonics. This correspondence allows us to project functions on the sphere onto irreps and vice versa. We use this to encode edge directions: the spherical harmonic representation of an edge is the embedding of a delta function with point mass aligned with the edge. The larger",
    "sphere S2, the spherical harmonics. This correspondence allows us to project functions on the sphere onto irreps and vice versa. We use this to encode edge directions: the spherical harmonic representation of an edge is the embedding of a delta function with point mass aligned with the edge. The larger the order of tensors we use in that embedding, the more fidelity the model has to distinguish directional inputs within the network. Message Generation In the ACE (Atomic Cluster Expansion) framework Facet uses, the network incorporates spatial information only through the representations of edge vectors and their effects on messages sent between nodes. Because the number of messages far exceeds the number of nodes, this part of the network needs to capture as much spatial information as possible efficiently. Models such as EquiformerV2 [37] use attention as an alternate means of aggregating messages, but here we limit ourselves to typical GNN convolutions. NequIP [12], GNoME [36], and SevenNet [14] use essentially identical messages, due originally to NequIP. For an edge with distance rand spherical harmonics Ybetween nodes with features zsandztof the source and target node, in principle the message can be any function M(r, Y, Z s, Zt).Ztis not considered: messages do not depend on the receiver. 10 Facet: highly efficientE(3)-equivariant networks for interatomic potentials To construct a message M(r, Y, Z s), the two sets of irreps are equivariantly combined via a tensor product ( ??). Each individual channel of the output is scaled by a weight controlling how information flows through the network. These weights are scalars computed from r. We denote this weighted tensor product M(r, Y, Z s) =Y⊗ W(r)Zs. W:R+→Rn, with nthe number of individual irreps in the output, is the only parameterized function, so we consider it in more detail. Note that, if W(r) = ⃗0, the message is also zero. This is how a differentiable distance cutoff is implemented. If we have a cutoff function c:R+→R such that c(0) = 1 andc(r) = 0 for all r≥r max, then we can use W(r) =f(rc(r)) for some functionfwithf(0) = ⃗0, and no edge beyond the cutoff will contribute to the output. Message Aggregation In our Facet model, the messages coming into a node are summed and then normalized by a constant, as is standard in the literature: this prevents any scale issues during training. In MACE [39], it was found that normalizing messages by the average number of neighbors kinstead of the square root√ k(as NequIP originally proposed) gives better performance. We shed light on this choice here. Consider kmessages: because summation and normalization do not mix dimensions, it suffices to analyze the 1- dimensional case. Let us assume that the messages are Gaussian with mean 0 and standard deviation σ. The standard deviation of the sum of these messages depends on the correlation between them. In particular, the standard deviation of the sum of kmessages distributed as N(0, σ2)such that each message has correlation ρwith each other message isp kσ2+k(k−1)ρσ2. Asρvaries from 0 (independence) to 1 (perfect correlation), the sum’s scale ranges from√ kσ tokσ.",
    "sum of these messages depends on the correlation between them. In particular, the standard deviation of the sum of kmessages distributed as N(0, σ2)such that each message has correlation ρwith each other message isp kσ2+k(k−1)ρσ2. Asρvaries from 0 (independence) to 1 (perfect correlation), the sum’s scale ranges from√ kσ tokσ. Although this analysis makes oversimplifying assumptions, we find its intuition useful: the choice between√ kandk represents a particular hypothesis about the correlation ρbetween messages to the same destination node, just as the choice of filter corresponds to a certain hypothesis about the maximum effect of small changes on distance between atoms. SevenNet-0, which does not modify rmax during training, lets us empirically estimate what the correlation between messages looks like for a trained model. Empirically, we find that in SevenNet-0 normalization by kpwith p≈0.7 is a better fit to the data compared to p= 0.5 orp= 1 , which both correspond to rather unrealistic expectations of message correlation: that messages to the same node are completely independent of each other, or that messages to the same node are perfectly correlated. Trainingr max, the maximum bond length We encode crystals as graphs using the 32 nearest neighbors for each atom. We then treat rmax, the cutoff radius, as a normal trainable parameter. This choice means our choice of normalization is not simply a difference in initialization but rather impacts the entire training run. In preprocessing, we compute the average number of neighbors for different values of rmax. We linearly interpolate between these values to estimate the average number of neighbors for any rmax in a differentiable manner. Denoting this estimated average degree as k(rmax), We divide the sum of incoming messages byk(r max)p, withp= 0.7as a hyperparameter we set based on our SevenNet-0 analysis. In SevenNet-Streamlined, we do the same, but to convert the original checkpoint we re-normalize the checkpoint with the normalization constant used in their data, starting at the samer max = 5as they do. We find that, when rmax is mutable, it tends to decrease sharply at the beginning of training, before the model has learned how to effectively incorporate edge information, and then slowly increases as training continues and the additional information can be smoothly incorporated. Future work could examine whether scheduling rmax can achieve better results, similarly to how diffusion models change the mix of time steps used over training to adjust the difficulty of the task. Node Update Between message-passing layers, we need to mix information within nodes by applying a node update N(Z, e) with Zthe hidden state of a node and ethe node’s element. NequIP and GNoME incorporate eby applying a different function for each element with separate parameters: N(Z, e) =N e(Z). This results in a near-hundred-fold increase in parameters and the inability to share parameters across elements in a way that makes learning more difficult. Following SevenNet, we remove this indexing and instead rely on the initial node embeddings to incorporate elemental information: N(Z, e) =N(Z). This makes the model smaller and more effective. 11 Facet: highly efficientE(3)-equivariant networks for interatomic potentials What",
    "share parameters across elements in a way that makes learning more difficult. Following SevenNet, we remove this indexing and instead rely on the initial node embeddings to incorporate elemental information: N(Z, e) =N(Z). This makes the model smaller and more effective. 11 Facet: highly efficientE(3)-equivariant networks for interatomic potentials What remains is determining the network parameterizing N. This function should be nonlinear to add expressivity to the network, as it surrounds linear operations. MACE computes a polynomial of the input up to a small fixed degree: computing tensor products Z, Z⊗Z, Z⊗Z⊗Z , etc., and then linearly combining into the desired size of the output. The connection between this product and many-order interactions, elucidated in [39], provides a strong theoretical foundation for the layer. The cost of computing these tensor products scales poorly with the size of the network, however. SevenNet instead uses a gate activation: the norms of non-scalar elements are modified by a set of gate scalars derived from the scalar elements. This is significantly cheaper to compute, but does not mix non-scalars between each other, limiting the flow of directional information. We use the S2-MLP-Mixer layer described above in an attempt to find a middle ground. We elucidate some details here. The spherical grid used is 18×17 with the quadrature described in [50]. EquiformerV2 uses 18×18 with a different quadrature. We find that, with this level of detail, the equivariance error (change in outputs from rotated versions of the inputs) is less than 1%, of which much is due to the underlying tensor product implementations and not the S2-MLP-Mixer layer. Model Readout Sequencing message passing and node update layers produces rich internal representations for each node. For energy prediction, we represent energy as a sum of node energies and thus need to predict a single scalar from the node state. Following SevenNet, we set the last message-passing layer to only produce scalars, effectively reducing the max degree already employed. A model head converts these scalars to a single output, summed across nodes. In SevenNet-0, the readout MLP consists of two linear layers. This introduces redundancy: two multiplications by matrices of size 128×64 and64×1 is equivalent to a single 128×1 matrix multiplication. Removing this redundancy further saves parameters and computation across the network with no change in model outputs. An alternative is to instead add a nonlinearity, producing a nonlinear MLP. We empirically find that the complexity of the head has a large impact on model training and overfitting. SevenNet-0 demonstrates that models can be expressive without a nonlinear head, and more tightly constraining the output mitigates overfitting and ensures more consistent training. Implementation details Facet and SevenNet-Streamlined are implemented in JAX [51], using the e3nn-jax library [52] for working with irreps and Flax [53] for neural network architecture. Data availability The MPTrj dataset used in this work is available at https://doi.org/10.6084/m9.figshare.23713842.v2. The code necessary to preprocess the data is available at https://github.com/nicholas-miklaucic/facet. Code availability All code necessary to replicate the results is available at https://github.com/nicholas-miklaucic/facet. References 1.Louie, S. G., Chan, Y . -H., da Jornada, F. H., Li, Z.",
    "availability The MPTrj dataset used in this work is available at https://doi.org/10.6084/m9.figshare.23713842.v2. The code necessary to preprocess the data is available at https://github.com/nicholas-miklaucic/facet. Code availability All code necessary to replicate the results is available at https://github.com/nicholas-miklaucic/facet. References 1.Louie, S. G., Chan, Y . -H., da Jornada, F. H., Li, Z. & Qiu, D. Y . Discovering and understanding materials through computation.Nature Materials20,728–735 (2021). 2.Oganov, A. R., Pickard, C. J., Zhu, Q. & Needs, R. J. Structure prediction drives materials discovery.Nature Reviews Materials4,331–348 (2019). 3.Neugebauer, J. & Hickel, T. Density functional theory in materials science.Wiley Interdisciplinary Reviews: Computational Molecular Science3,438–448 (2013). 4.Omee, S. S., Wei, L., Hu, M. & Hu, J. Crystal structure prediction using neural network potential and age-fitness Pareto genetic algorithm.Journal of Materials Informatics(2024). 5.Wang, G.et al.Machine learning interatomic potential: Bridge the gap between small-scale models and realistic device-scale simulations.Iscience(2024). 12 Facet: highly efficientE(3)-equivariant networks for interatomic potentials 6.Omee, S. S., Wei, L., Dey, S. & Hu, J. Polymorphism Crystal Structure Prediction with Adaptive Space Group Diversity Control.arXiv preprint arXiv:2506.11332(2025). 7.Deringer, V . L., Caro, M. A. & Csányi, G. Machine learning interatomic potentials as emerging tools for materials science.Advanced Materials31,1902765 (2019). 8.Ramprasad, R., Batra, R., Pilania, G., Mannodi-Kanakkithodi, A. & Kim, C. Machine learning in materials informatics: recent applications and prospects.npj Computational Materials3,54 (2017). 9.Friederich, P., Häse, F., Proppe, J. & Aspuru-Guzik, A. Machine-learned potentials for next-generation matter simulations.Nature Materials20,750–761 (2021). 10. Ruff, R., Reiser, P., Stühmer, J. & Friederich, P. Connectivity Optimized Nested Line Graph Networks for Crystal Structures.Digital Discovery3,594–601. (2024) (2024). 11. Liao, Y . -L. & Smidt, T. Equiformer: Equivariant Graph Attention Transformer for 3D Atomistic Graphs. (2024) (2022). 12. Batzner, S.et al.E(3)-Equivariant Graph Neural Networks for Data-Efficient and Accurate Interatomic Potentials. Nature Communications13,2453.ISSN: 2041-1723. (2024) (May 2022). 13. Batatia, I.et al. A Foundation Model for Atomistic Materials ChemistryMar. 2024. arXiv:2401.00096. (2024). 14. Park, Y ., Kim, J., Hwang, S. & Han, S. Scalable Parallel Algorithm for Graph Neural Network Interatomic Potentials in Molecular Dynamics Simulations.Journal of Chemical Theory and Computation20,4857–4868. ISSN: 1549-9618 (June 2024). 15. Belli, F. & Zurek, E. Efficient modelling of anharmonicity and quantum effects in PdCuH2 with machine learning potentials.npj Computational Materials11,87 (2025). 16. Roberts, J.et al.Machine learned interatomic potentials for ternary carbides trained on the AFLOW database.npj Computational Materials10,142 (2024). 17. Wang, Y .et al.Enhancing geometric representations for molecules with equivariant vector-scalar interactive message passing.Nature Communications15,313 (2024). 18. Fedik, N.et al.Extending machine learning beyond interatomic potentials for predicting molecular properties. Nature Reviews Chemistry6,653–672 (2022). 19. Chen, C. & Ong, S. P. A universal graph deep learning interatomic potential for the periodic table.Nature Computational Science2,718–728 (2022). 20. Wen, T., Zhang, L., Wang, H., Srolovitz, D. J.,et al.Deep potentials for materials science.Materials Futures1, 022601 (2022). 21. Yang, Z.et al.Efficient equivariant model for machine learning interatomic potentials.npj Computational Materials11,49 (2025). 22. Riebesell, J.et al.Matbench Discovery–A framework to evaluate machine learning crystal stability predictions. arXiv preprint arXiv:2308.14920(2023). 23. Fu, X.et al.Learning smooth and expressive interatomic potentials for physical property prediction.arXiv preprint arXiv:2502.12147(2025). 24. Han, B. & Cheng, Y . Benchmarking Universal Machine Learning Interatomic Potentials for",
    "interatomic potentials.npj Computational Materials11,49 (2025). 22. Riebesell, J.et al.Matbench Discovery–A framework to evaluate machine learning crystal stability predictions. arXiv preprint arXiv:2308.14920(2023). 23. Fu, X.et al.Learning smooth and expressive interatomic potentials for physical property prediction.arXiv preprint arXiv:2502.12147(2025). 24. Han, B. & Cheng, Y . Benchmarking Universal Machine Learning Interatomic Potentials for Real-Time Analysis of Inelastic Neutron Scattering Data.arXiv preprint arXiv:2506.01860(2025). 25. Guo, D.et al.Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.arXiv preprint arXiv:2501.12948(2025). 26. Wang, C. & Kantarcioglu, M. A review of DeepSeek models’ key innovative techniques.arXiv preprint arXiv:2503.11486(2025). 27. Deng, Z.et al.Exploring DeepSeek: A Survey on Advances, Applications, Challenges and Future Directions. IEEE/CAA Journal of Automatica Sinica12,872–893 (2025). 28. Fuchs, P., Chen, W., Thaler, S. & Zavadlav, J. chemtrain-deploy: A parallel and scalable framework for machine learning potentials in million-atom MD simulations.arXiv preprint arXiv:2506.04055(2025). 29. Han, K., Deng, B., Farimani, A. B. & Ceder, G. DistMLIP: A Distributed Inference Platform for Machine Learning Interatomic Potentials.arXiv preprint arXiv:2506.02023(2025). 30. Yan, K., Liu, Y ., Lin, Y . & Ji, S. Periodic Graph Transformers for Crystal Material Property Prediction.Advances in Neural Information Processing Systems35,15066–15080. (2024) (Dec. 2022). 31. Xie, T. & Grossman, J. C. Crystal Graph Convolutional Neural Networks for an Accurate and Interpretable Prediction of Material Properties.Physical Review Letters120,145301.ISSN: 0031-9007, 1079-7114. (2025) (Apr. 2018). 32. Xu, K., Hu, W., Leskovec, J. & Jegelka, S.How Powerful are Graph Neural Networks?inInternational Conference on Learning Representations(2019).https://openreview.net/forum?id=ryGs6iA5Km. 13 Facet: highly efficientE(3)-equivariant networks for interatomic potentials 33. Gasteiger, J., Giri, S., Margraf, J. T. & Günnemann, S.Fast and Uncertainty-Aware Directional Message Passing for Non-Equilibrium MoleculesinMachine Learning for Molecules Workshop, NeurIPS(2020). 34. Gasteiger, J., Becker, F. & Günnemann, S.GemNet: Universal Directional Graph Neural Networks for Molecules inAdvances in Neural Information Processing Systems34(Curran Associates, Inc., 2021), 6790–6802. (2025). 35. Choudhary, K. & DeCost, B. Atomistic Line Graph Neural Network for Improved Materials Property Predictions. npj Computational Materials7,185.ISSN: 2057-3960. (2025) (Nov. 2021). 36. Merchant, A.et al.Scaling Deep Learning for Materials Discovery.Nature624,80–85.ISSN: 1476-4687. (2025) (Dec. 2023). 37. Liao, Y . -L., Wood, B., Das, A. & Smidt, T. EquiformerV2: Improved Equivariant Transformer for Scaling to Higher-Degree Representations. (2024) (2023). 38. Cohen, T. S., Geiger, M., Köhler, J. & Welling, M.Spherical CNNsinInternational Conference on Learning Representations(Feb. 2018). (2025). 39. Batatia, I.et al. The Design Space of E(3)-Equivariant Atom-Centered Interatomic PotentialsNov. 2022. arXiv: 2205.06643. (2024). 40. Polyak, B. & Juditsky, A. B. Acceleration of stochastic approximation by averaging.Siam Journal on Control and Optimization30,838–855.https://api.semanticscholar.org/CorpusID:3548228(1992). 41. Mishchenko, K. & Defazio, A.Prodigy: An Expeditiously Adaptive Parameter-Free LearnerinForty-First International Conference on Machine Learning(2024). 42. Barroso-Luque, L.et al. Open Materials 2024 (OMat24) Inorganic Materials Dataset and ModelsOct. 2024. arXiv:2410.12771. (2025). 43. Zhou, Y .et al. FastCHGNet: Training One Universal Interatomic Potential to 1.5 Hours with 32 GPUsDec. 2024. arXiv:2412.20796. (2025). 44. Wang, J.et al.MAGUS: machine learning and graph theory assisted universal structure searcher.National Science Review10,nwad128.ISSN: 2095-5138. eprint: https : / / academic . oup . com / nsr / article - pdf/10/7/nwad128/50709989/nwad128.pdf.https://doi.org/10.1093/nsr/nwad128(May 2023). 45. Jin, L.et al.Transformer-Generated Atomic Embeddings to Enhance Prediction Accuracy of Crystal Properties with Machine Learning.Nature Communications16,1210.ISSN: 2041-1723. (2025) (Jan. 2025). 46. McInnes, L., Healy,",
    "theory assisted universal structure searcher.National Science Review10,nwad128.ISSN: 2095-5138. eprint: https : / / academic . oup . com / nsr / article - pdf/10/7/nwad128/50709989/nwad128.pdf.https://doi.org/10.1093/nsr/nwad128(May 2023). 45. Jin, L.et al.Transformer-Generated Atomic Embeddings to Enhance Prediction Accuracy of Crystal Properties with Machine Learning.Nature Communications16,1210.ISSN: 2041-1723. (2025) (Jan. 2025). 46. McInnes, L., Healy, J. & Melville, J.UMAP: Uniform Manifold Approximation and Projection for Dimension ReductionSept. 2020. arXiv:1802.03426. (2025). 47. Qu, E. & Krishnapriyan, A. S.The Importance of Being Scalable: Improving the Speed and Accuracy of Neural Network Interatomic Potentials Across Chemical Domainsin (Oct. 2024). (2024). 48. Omee, S. S.et al.Scalable deeper graph neural networks for high-performance materials property prediction. Patterns3(2022). 49. Cai, C. & Wang, Y . A note on over-smoothing for graph neural networks.arXiv preprint arXiv:2006.13318(2020). 50. Kostelec, P. J. & Rockmore, D. N. FFTs on the Rotation Group.Journal of Fourier Analysis and Applications14, 145–179.ISSN: 1531-5851. (2025) (Apr. 2008). 51. Bradbury, J.et al. JAX: Composable Transformations of Python+NumPy Programshttp://github.com/jax-ml/jax. 2018. 52. Geiger, M.et al. Euclidean Neural Networks: E3nnhttps://doi.org/10.5281/zenodo.6459381. Apr. 2022. 53. Heek, J.et al. Flax: A Neural Network Library and Ecosystem for JAXhttp://github.com/google/flax. 2024. Acknowledgement The research reported in this work was supported in part by National Science Foundation under the grant and 2110033, 2311202, 2320292, and OAC-2311203. The views, perspectives, and content do not necessarily represent the official views of the NSF. Author contributions J.H. initiated the project, N.M.conceived the Facet model, implemented the software and conducted all software experiments under the guidance of J.H. L.W., R.D., N.F., S.S.O., Q.L., S.D., and V .F. participated in the discussion of the Facet model and evaluation of the models. N.F. and R.D. helped to polish the table results. All authors contributed to writing and revising the manuscript. 14 Facet: highly efficientE(3)-equivariant networks for interatomic potentials Competing interests The authors declare no competing interests. 15"
  ]
}