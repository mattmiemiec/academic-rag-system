{
  "filename": "2509.03378v3.pdf",
  "total_chunks": 23,
  "text_length": 78463,
  "chunks": [
    "Understanding and Improving Shampoo and SOAP via Kullbackâ€“Leibler Minimization Wu Lin1, Scott C. Lowe1, Felix Dangel1, Runa Eschenhagen2, Zikun Xu3, and Roger B. Grosse1,4 1Vector Institute, Toronto, Canada 2University of Cambridge, Cambridge, United Kingdom 3Microsoft, United States 4University of Toronto, Toronto, Canada yorker.lin@gmail.com {scott.lowe,fdangel}@vectorinstitute.ai re393@cam.ac.uk zikunxu@microsoft.com rgrosse@cs.toronto.edu Abstract Shampoo and its efficient variant, SOAP, employ structured second-moment estimations and have shown strong performance for training neural networks (NNs). In practice, however, Shampoo typically requires step-size grafting with Adam to be competitive, and SOAP mitigates this by applying Adam in Shampooâ€™s eigenbasisâ€”at the cost of additional memory overhead from Adam in both methods. Prior analyses have largely relied on the Frobenius norm to motivate these estimation schemes. We instead recast their estimation procedures as covariance estimation under Kullback-Leibler (KL) divergence minimization, revealing a previously overlooked theoretical limitation and motivating principled redesigns. Building on this perspective, we develop KL-ShampooandKL-SOAP, practical schemes that match or exceed the performance of Shampoo and SOAP in NN pre-training while achieving SOAP-level per-iteration runtime. Notably, KL-Shampoo does not rely on Adam to attain competitive performance, eliminating the memory overhead introduced by Adam. Across our experiments, KL-Shampoo consistently outperforms SOAP, Shampoo, and even KL-SOAP, establishing the KL-based approach as a compelling foundation for designing structured methods in NN optimization. 1 Introduction Optimizer Shampoo (Gupta et al., 2018) has recently dethroned Adam (Kingma & Ba, 2015) as the winner of several competitions in training a wide range of neural network (NN) models (Dahl et al., 2023; Kasimbeg et al., 2025). Consequently, Shampoo and its variant, SOAP (Vyas et al., 2025a), have drawn increasing attention. In practice, Shampoo requires step-size grafting with Adam to achieve competitive performance (Agarwal et al., 2020; Shi et al., 2023). SOAP addresses this by applying Adam in Shampooâ€™s eigenbasis and further reducing per-iteration runtime. However, reliance on Adam introduces additional memory overhead in both methods. Prior work (Morwani et al., 2025; Eschenhagen et al., 2025; An et al., 2025; Xie et al., 2025) has investigated their structural preconditioner schemesâ€”which approximate the flattened gradient 2ndmoment (Duchi et al., 2011)â€”through the Frobenius norm. However, few studies have examined these schemes from the perspective of Kullbackâ€“Leibler (KL) divergence. Compared to the Frobenius norm, the KL divergence between zero-mean Gaussian covariance matrices is more appropriate for interpreting Shampooâ€™s and SOAPâ€™s preconditioners as Gaussian covariance (Amari, 2016; Minh & Murino, 2017), since the second moment they approximate can be viewed as the covariance matrix of a zero-mean Gaussian. A similar KL perspective has provided a unified framework to interpret (Fletcher, 1991; Waldrip & Niven, 2016) and extend (Kanamori & Ohara, 2013a,b) structural preconditioner estimation in quasi-Newton methods such as BFGS and DFPâ€”something the Frobenius norm does not. Moreover, the KL divergence intrinsically respects the symmetric positive-definite constraint (Amari, 2016; Minh & Murino, 2017) that preconditioners in Shampoo and SOAP must satisfy as adaptive (preconditioned) methods (Nesterov et al., 2018)â€”a property the Frobenius norm lacks. This constraint implies that the entries of the preconditioning matrix do not play equivalent roles and therefore should not be treated equally (Pennec et al., 2006; Bhatia, 2007)â€”a",
    "2017) that preconditioners in Shampoo and SOAP must satisfy as adaptive (preconditioned) methods (Nesterov et al., 2018)â€”a property the Frobenius norm lacks. This constraint implies that the entries of the preconditioning matrix do not play equivalent roles and therefore should not be treated equally (Pennec et al., 2006; Bhatia, 2007)â€”a point the Frobenius norm ignores. In this work, we introduce a KL perspective that interprets the estimation schemes of Shampoo and SOAP as solutions to KL-minimization problems for covariance estimation. Our approach naturally extends to tensor-valued settings, where some existing theoretical interpretations may not apply. This perspective reveals a key limitation (illustrated in Fig. 5): the Kronecker-structured estimators used by Shampoo and SOAP do not adequately solve the corresponding KL-minimization problem. This limitation, in turn, opens new opportunities for improvement. 1arXiv:2509.03378v3 [stat.ML] 6 Oct 2025 KL-Shampoo and KL-SOAP Wu Lin, et al. (2025) 0 50 100 150 Minutes3.2503.2753.3003.3253.3503.3753.4003.4253.450Test LossNanoGPT-123M-FineWeb1B 0 50 100 150 Minutes3.2503.2753.3003.3253.3503.3753.4003.4253.450NanoRWKV7-162M-FineWeb1B 0 50 100 150 Minutes3.203.253.303.353.403.45Llama-134M-C4-2B 0 100 200 300 Minutes3.15 3.123.203.253.303.35NanoMoE-227M-OpenWeb2.5B KL-Shampoo KL-SOAP SOAP Shampoo (p=1 2, grafting) Shampoo (p=1 4, grafting) Figure 1: Empirical results (random search using 150 runs for each method) on language models demonstrate the advantages of KL-based methods over Shampoo and SOAP while matching SOAPâ€™s pre-iteration runtime. All methods take the same number of iterations in these experiments. Surprisingly, KL-Shampoo also outperforms KL-SOAP. We include the best Shampoo run based on a state-of-the-art implementation from Meta (Shi et al., 2023) in the plots. Leveraging this insight, we refine the estimation rules of Shampoo and SOAP and develop practical KL-based schemesâ€”KL-Shampoo and KL-SOAPâ€”that meet or exceed the performance of Shampoo and SOAP for NN (pre-)training while maintaining SOAP-level per-iteration runtime. Notably, KL-Shampoo does not rely on Adam to achieve competitive performance, thereby avoiding Adamâ€™s additional memory overhead (Table 1). In addition, our practical techniques developed for KL-Shampoo (Sec. 4) can be adapted to strengthen existing Shampoo variants and make the trace-scaling variant competitive without step-size grafting (Fig. 9, Sec. H), while also achieving SOAP-level per-iteration runtime (Fig. 10, Sec. H). Empirically, we show that KL-based methods are competitive for training a range of NNs and remain as flexible as Shampoo and SOAP for tensor-valued weights. Surprisingly, KL-Shampoo consistently outperforms the other methods in our experiments (Figs. 1 and 5). Overall, our KL-based approach provides a principal way for designing structured methods in NN optimization. 2 Background NotationFor presentation simplicity, we focus on matrix-valued weights and the optimization update for a single parameter matrix ğš¯âˆˆRğ‘‘ğ‘Ã—ğ‘‘ğ‘, rather than a set of weight matrices for NN training. We use Mat(Â·) to unflatten its input vector into a matrix and vec(Â·) to flatten its input matrix into a vector. For example, Î¸:=vec(ğš¯) is the flattened weight vector and ğš¯â‰¡Mat(Î¸) is the unflattened weight matrix. Vector gis a (flattened) gradient vector for the weight matrix. We denote ğ›¾,ğ›½2andSto be a step size, a weight for moving average, and a preconditioning matrix for an adaptive method, respectively. Diag(Â·) returns a diagonal matrix whose diagonal entries are given by its input vector, whilstdiag(Â·)extracts the diagonal entries of its input matrix as",
    "gradient vector for the weight matrix. We denote ğ›¾,ğ›½2andSto be a step size, a weight for moving average, and a preconditioning matrix for an adaptive method, respectively. Diag(Â·) returns a diagonal matrix whose diagonal entries are given by its input vector, whilstdiag(Â·)extracts the diagonal entries of its input matrix as a vector. ShampooGiven a matrix gradient Gand the flattened gradient g=vec(G) , the original Shampoo method (Gupta et al., 2018) considers aKronecker-factoredapproximation, (Sğ‘)2ğ‘âŠ—(Sğ‘)2ğ‘, of the flattened gradient second moment, Eg[ggâŠ¤], whereğ‘denotes a matrix power, Sğ‘:=Eg[GGâŠ¤],Sğ‘:=Eg[GâŠ¤G], andâŠ—denotes a Kronecker product. In practice, we often approximate the expectation, Eg[ggâŠ¤], with an exponentially moving average (EMA) on the outer product (Morwani et al., 2025). The original Shampoo method uses the 1/4power (i.e., ğ‘= 1/4) and other works (Anil et al., 2020; Shi et al., 2023; Morwani et al., 2025) suggest using the 1/2power (i.e., ğ‘= 1/2). At each iteration, Shampoo follows this update rule with EMA onS ğ‘andSğ‘: Sğ‘â†(1âˆ’ğ›½ 2)Sğ‘+ğ›½2GGâŠ¤,Sğ‘â†(1âˆ’ğ›½ 2)Sğ‘+ğ›½2GâŠ¤G(Kronecker 2ndmoment est.), Î¸â†Î¸âˆ’ğ›¾Sâˆ’1/2gâ‡â‡’ğš¯â†ğš¯âˆ’ğ›¾Sâˆ’ğ‘ ğ‘GSâˆ’ğ‘ ğ‘(preconditioning),(1) where S:=S2ğ‘ ğ‘âŠ—S2ğ‘ ğ‘is Shampooâ€™s preconditioning matrix, and we leverage the Kronecker structure of Sto move from the left expression to the right expression in the second line. Shampooâ€™s implementation employs eigendecomposition.Shampoo is typically implemented by using the eigendecomposition of Sğ‘˜, such as Qğ‘˜Diag(Î»ğ‘˜)QâŠ¤ ğ‘˜=eigen(S ğ‘˜), forğ‘˜âˆˆ{ğ‘,ğ‘} , every few steps and storing Qğ‘˜andÎ»ğ‘˜(Anil et al., 2020; Shi et al., 2023). Therefore, the power of Sğ‘˜is computed using an 2 KL-Shampoo and KL-SOAP Wu Lin, et al. (2025) elementwise power in Î»ğ‘˜such as Sâˆ’ğ‘ ğ‘˜=Qğ‘˜Diag\u0000Î»âŠ™âˆ’ğ‘ ğ‘˜\u0001QâŠ¤ ğ‘˜, whereÂ·âŠ™ğ‘denotes elementwise ğ‘-th power. This computation becomes an approximation if the decomposition is not performed at every step. Using Adam for Shampooâ€™s stabilization increases memory usage.If the eigendecomposition is applied infrequently to reduce iteration cost, Shampoo has to apply step-size grafting with Adam to maintain performance (Agarwal et al., 2020; Shi et al., 2023) as empirically shown in Fig. 2. Unfortunately, this increases its memory usage introduced by Adam (see Table 1). SOAPSOAP improves Shampoo with the ğ‘= 1/2power by running Adam in the eigenbasis of Shampooâ€™s preconditioner(Sğ‘)2ğ‘âŠ—(Sğ‘)2ğ‘=Sğ‘âŠ—Sğ‘. Notably, SOAP reuses Shampooâ€™s Kronecker estimation rule for computing the eigenbasis Q:=Qğ‘âŠ—Qğ‘and incorporates Adamâ€™s 2ndmoment, denoted by d, for preconditioning, where Qğ‘˜is Shampooâ€™s Kronecker eigenbasis Sğ‘˜forğ‘˜âˆˆ{ğ‘,ğ‘} defined above. As a result, SOAP effectively employs anaugmentedpreconditioner, S:=QDiag(d)QâŠ¤, which cannot be expressed as a Kronecker product of any two matrices with the same shape as Sğ‘andSğ‘. Because we omit momentum (i.e. let Adamâ€™s ğ›½1=0), SOAP takes the following step with the Adam update becoming an RMSProp update (Tieleman & Hinton, 2012): Sğ‘â†(1âˆ’ğ›½ 2)Sğ‘+ğ›½2GGâŠ¤,Sğ‘â†(1âˆ’ğ›½ 2)Sğ‘+ğ›½2GâŠ¤G(Shampooâ€™s 2ndmoment est.), dâ†(1âˆ’ğ›½ 2)d+ğ›½ 2Ë†gâŠ™2(RMSPropâ€™s diagonal 2ndmoment est. in the eigenbasis), Î¸â†Î¸âˆ’ğ›¾Sâˆ’1 2gâ‡â‡’ğš¯â†ğš¯âˆ’ğ›¾QâŠ¤ ğ‘Mat\u0012Ë†gâˆš d\u0013 Qğ‘(preconditioning),(2) where Ë†g:=QâŠ¤g=vec(QâŠ¤ ğ‘GQğ‘)is a â€œprojected\" gradient vector in eigenbasis Qand recall that S:= QDiag(d)QâŠ¤is SOAPâ€™s preconditioner. Here, we leverage the Kronecker structure and orthogonality of the eigenbasis to move from the left to the right in the last line of Eq. (2). Note that this EMA weight ğ›½2is defined as 1âˆ’ğ›½(Adam) 2, whereğ›½(Adam) 2is Adamâ€™s (RMSPropâ€™s) ğ›½2. We use this definition rather than Adamâ€™s because we want to further interpret this moving-average scheme through the lens",
    "eigenbasis to move from the left to the right in the last line of Eq. (2). Note that this EMA weight ğ›½2is defined as 1âˆ’ğ›½(Adam) 2, whereğ›½(Adam) 2is Adamâ€™s (RMSPropâ€™s) ğ›½2. We use this definition rather than Adamâ€™s because we want to further interpret this moving-average scheme through the lens of our KL perspective. SOAPâ€™s implementation utilizes QR decomposition.SOAP requires only the eigenbasis, which can be approximated via a QR decomposition, whereas Shampoo typically requires an eigendecomposition to compute both the eigenbasis and the eigenvalues. Vyas et al. (2025a) therefore suggest replacing the slower eigendecomposition with the faster QR decomposition, such as Qğ‘˜â†qr(Sğ‘˜Qğ‘˜)forğ‘˜âˆˆ{ğ‘,ğ‘} . This makes SOAP more computationally efficient than Shampoo. Runing Adam in the eigenbasis increases memory usage.Introducing Adamâ€™s (RMSPropâ€™s) 2ndmoment estimation increases SOAPâ€™s memory consumption (see Table 1). This is because this estimation, dâˆˆRğ‘‘ğ‘ğ‘‘ğ‘Ã—1, uses extra memory and cannot be expressed as a Kronecker product of any two vectors, such as dâ‰ rğ‘âŠ—rğ‘, whererğ‘âˆˆRğ‘‘ğ‘Ã—1andrğ‘âˆˆRğ‘‘ğ‘Ã—1. The original Shampooâ€™s Kronecker estimation rule ( ğ‘= 1/4) (Gupta et al., 2018; Duvvuri et al., 2024) is proposed based on a matrix Loewner bound (LÃ¶wner, 1934), while recent estimation rules ( ğ‘= 1/2) (Morwani et al., 2025; Eschenhagen et al., 2025) focus on bounds induced by the Frobenius norm. SOAP reuses Shampooâ€™s Kronecker estimation rule and additionally introduces Adamâ€™s (RMSPropâ€™s) 2nd-moment estimation rule in the eigenbasis (Vyas et al., 2025a). None of these works interpret or motivate their estimation rules as covariance estimation, thereby missing the opportunity to introduce the KL perspective. Shampoo SOAP KL-Shampoo KL-SOAP Kronecker factors (S ğ‘,Sğ‘)ğ‘‘2 ğ‘+ğ‘‘2 ğ‘ğ‘‘2 ğ‘+ğ‘‘2 ğ‘ğ‘‘2 ğ‘+ğ‘‘2 ğ‘ğ‘‘2 ğ‘+ğ‘‘2 ğ‘ Kronecker factorsâ€™ eigenbasis (Q ğ‘,Qğ‘)ğ‘‘2 ğ‘+ğ‘‘2 ğ‘ğ‘‘2 ğ‘+ğ‘‘2 ğ‘ğ‘‘2 ğ‘+ğ‘‘2 ğ‘ğ‘‘2 ğ‘+ğ‘‘2 ğ‘ Kronecker factorsâ€™ eigenvalues (Î» ğ‘,Î»ğ‘)ğ‘‘ğ‘+ğ‘‘ğ‘ N/Ağ‘‘ ğ‘+ğ‘‘ğ‘ğ‘‘ğ‘+ğ‘‘ğ‘ Adamâ€™s 2ndmoment in the eigenbasis (d) (interpreted as augmented eigenvalues, Sec. 5)N/Ağ‘‘ ğ‘ğ‘‘ğ‘ N/Ağ‘‘ ğ‘ğ‘‘ğ‘ Momentumğ‘‘ ğ‘ğ‘‘ğ‘ğ‘‘ğ‘ğ‘‘ğ‘ğ‘‘ğ‘ğ‘‘ğ‘ğ‘‘ğ‘ğ‘‘ğ‘ Step-size grafting with Adamğ‘‘ ğ‘ğ‘‘ğ‘ N/A N/A N/A Table 1: Memory usage of each method considered in this work. We store and update1all of them in half precision (bfloat16). The memory overhead introduced by Adam is highlighted in red. Note that SOAPâ€™s and KL-SOAPâ€™s preconditioners, QDiag(d)QâŠ¤, can not be expressed as a Kronecker product due to the augmented eigenvalues d, while Shampooâ€™s and KL-Shampooâ€™s preconditioners,QDiag(Î» ğ‘âŠ—Î»ğ‘)QâŠ¤, can, whereQ:=Q ğ‘âŠ—Qğ‘. 3 KL-Shampoo and KL-SOAP Wu Lin, et al. (2025) 3Second Moment Estimation via Kullbackâ€“Leibler Minimization We first focus on Shampoo with ğ‘= 1/2and show that its second-moment estimation can be viewed as a structured covariance estimation problem solved via Kullbackâ€“Leibler (KL) minimization. This perspective reflects the natural connection between the flattened gradient second moment (Duchi et al., 2011) that Shampoo approximates and a covariance matrix. From the KL perspective, we reveal a previously unrecognized limitation of Shampooâ€™s estimation rule: the Kronecker-structured estimators used by Shampoo and SOAP do not adequately solve the corresponding KL-minimization problem. This limitation, in turn, opens new opportunities for improvement. Building on this insight, we propose a KL-based estimation scheme for Shampoo, which we term the idealized KL-Shampoo. KL MinimizationFor simplicity, we begin by introducing a KL perspective in a matrix-valued case and drop subscripts when referring",
    "the corresponding KL-minimization problem. This limitation, in turn, opens new opportunities for improvement. Building on this insight, we propose a KL-based estimation scheme for Shampoo, which we term the idealized KL-Shampoo. KL MinimizationFor simplicity, we begin by introducing a KL perspective in a matrix-valued case and drop subscripts when referring to the flattened gradient 2ndmoment, like E[ggâŠ¤]:=Eg[ggâŠ¤], where g=vec(G) is a flattened gradient vector of a matrix-valued gradient GâˆˆRğ‘‘ğ‘Ã—ğ‘‘ğ‘. The goal is to estimate a Kronecker-structured preconditioning matrix, S=Sğ‘âŠ—Sğ‘, that closely approximates the 2ndmoment, where Sğ‘âˆˆRğ‘‘ğ‘Ã—ğ‘‘ğ‘and Sğ‘âˆˆRğ‘‘ğ‘Ã—ğ‘‘ğ‘are both symmetric positive-definite (SPD). Motivated by the natural connection between the second moment and a covariance matrix, we treat these as covariance matrices of zero-mean Gaussian distributions and achieve this goal by minimizing the KL divergence between the two distributions, KL Perspective for Covariance Estimation KL(E[ggâŠ¤],S):=ğ· KL(N(0,E[ggâŠ¤]+ğœ…I)âˆ¥N(0,S)) =1 2\u0000log det(S)+Tr((E[ggâŠ¤]+ğœ…I)Sâˆ’1)\u0001+const,(3) where E[ggâŠ¤]andSare considered as Gaussianâ€™s covariance, det(Â·) denotes the matrix determinant of its input, andğœ…â‰¥0is a damping weight to ensure the positive-definiteness of E[ggâŠ¤]+ğœ…I if necessary. Mathematically, this KL divergence coincides (up to a factor of 1/2) with the log-determinant divergence widely used in matrix optimization (Dhillon & Tropp, 2008; Kulis et al., 2009; Sra, 2016), which is defined for any pair of SPD matrices and does not require a zero-mean assumption. This additional zero-mean Gaussian viewpoint provides a probabilistic interpretation of this SPD-aware â€œdistanceâ€, even when the target matrix is not itself a second moment, such as the curvature matrix used in quasi-Newton methods (Fletcher, 1991; Waldrip & Niven, 2016). Moreover, the KL divergence naturally extends to tensor-valued cases, such as a 3D tensor gradient, GâˆˆRğ‘‘ğ‘Ã—ğ‘‘ğ‘Ã—ğ‘‘ğ‘, by considering a structured preconditioner S=Sğ‘âŠ—Sğ‘âŠ—Sğ‘to approximate the flattened gradient second moment, E[ggâŠ¤], where matrixS ğ‘˜âˆˆRğ‘‘ğ‘˜Ã—ğ‘‘ğ‘˜is SPD forğ‘˜âˆˆ{ğ‘,ğ‘,ğ‘}. Justification of using the KL divergenceMany works (Morwani et al., 2025; Eschenhagen et al., 2025; An et al., 2025; Xie et al., 2025) primarily focus on matrix-valued weights and interpret Shampooâ€™s and SOAPâ€™s estimation rules from the Frobenius-norm perspective. However, this norm does not account for the SPD constraint implicitly imposed on Shampooâ€™s and SOAPâ€™s preconditioners, which ensures that the preconditioned gradient direction is a descent direction (Nesterov et al., 2018). As emphasized in the literature (Pennec et al., 2006; Bhatia, 2007), it is more appropriate to consider a â€œdistanceâ€ that respects this constraint. We adopt the KL divergence because it naturally incorporates the SPD constraint, is widely used for covariance estimation (Amari, 2016; Minh & Murino, 2017), and provides a unified framework for reinterpreting and improving Shampooâ€™s estimationâ€”even in tensor-valued settings where some existing interpretations based on singular value decomposition (Van Loan & Pitsianis, 1993) may not apply. In numerical optimization, the KL divergence, known as a merit function (Byrd & Nocedal, 1989), offers a unifying interpretation (Fletcher, 1991; Waldrip & Niven, 2016) and extension (Kanamori & Ohara, 2013a,b) of the low-rank estimation schemes of BFGS and DFP. In contrast, the standard Frobenius norm cannot recover these updates without additional weighting (see Sec. 6.1 of Nocedal & Wright (2006)). In statistical estimation and inference, this KL divergence is also preferred over the Frobenius norm (James et al., 1961; Kivinen",
    "of the low-rank estimation schemes of BFGS and DFP. In contrast, the standard Frobenius norm cannot recover these updates without additional weighting (see Sec. 6.1 of Nocedal & Wright (2006)). In statistical estimation and inference, this KL divergence is also preferred over the Frobenius norm (James et al., 1961; Kivinen & Warmuth, 1999; Davis & Dhillon, 2006; Khan & Lin, 2017; Lin et al., 2019; Kunstner et al., 2021). 3.1 Interpreting Shampooâ€™s estimation as covariance estimation Similar to existing works (Morwani et al., 2025; Eschenhagen et al., 2025; Vyas et al., 2025a), we first disable the moving average (i.e., let ğ›½2=1) for our descriptions and focus on Shampoo with power ğ‘= 1/2, presenting a KL 1Since the current QR/eigen implementation in PyTorch does not support half-precision (bfloat16), we perform QR/eigen to compute Qğ‘˜forğ‘˜âˆˆ{ğ‘,ğ‘} in single precision (float32) and then cast and store them in half precision (bfloat16). Other matrices and vectors can be computed and updated in half precision. 4 KL-Shampoo and KL-SOAP Wu Lin, et al. (2025) 4567NanoGPT-123M-FineWeb1B 46810NanoRWKV7-162M-FineWeb1B 456Llama-134M-C4-2B 3.54.04.55.0NanoMoE-227M-OpenWeb2.5B 10 5 1 Iterations (thousands)3.253.303.35aa 4 2 1 Iterations (thousands)3.33.43.5 10 5 1 Iterations (thousands)3.23.33.43.5 10 5 1 Iterations (thousands)3.153.203.25Test Loss KL-Shampoo Shampoo (p=1 4) Shampoo (p=1 4, grafting) Shampoo (p=1 2) Shampoo (p=1 2, grafting) Figure 2: Empirical results (random search using 150 runs for each method) on language models demonstrate that KL-Shampoo does not rely on step-size grafting with Adam to perform well. Shampoo without grafting does not perform well, even when using the state-of-the-art implementation (Shi et al., 2023). In particular, Shampoo with powerğ‘= 1/2fails to train the RWKV7 model in all 150 runs when grafting is disabled. minimization perspective and interpreting its estimation rule from this perspective. We will show that Shampooâ€™s estimation rule can be obtained by solving a KL minimization problem. Claim 1 (Shampooâ€™s Kronecker-based covariance estimation)The optimal solution of KL minimization minSğ‘KL\u0000E[ggâŠ¤],S\u0001with a one-sided preconditioner S=( 1/ğ‘‘ğ‘Sğ‘)âŠ—Iğ‘isSâˆ— ğ‘=E[GGâŠ¤], whereğ‘‘ğ‘˜is the dimension of matrixS ğ‘˜âˆˆRğ‘‘ğ‘˜Ã—ğ‘‘ğ‘˜forğ‘˜âˆˆ{ğ‘,ğ‘}andG=Mat(g). Likewise, we can obtain the estimation rule forS ğ‘by consideringS=I ğ‘âŠ—(1/ğ‘‘ğ‘Sğ‘). Shampooâ€™s estimation rule as Kronecker-based covariance estimationAccording to Claim 1 (proof in Sec. A), Shampooâ€™s estimation rule with power ğ‘= 1/2in Eq. (1) can be viewed as the optimal solution of a KL minimization problem (up to a constant scalar) when one Kronecker factor is updated independently and the other is fixed as the identity, which is known as a one-sided preconditioner (An et al., 2025; Xie et al., 2025). In practice, Shampoo further approximates the required expectations using the EMA scheme in Eq. (1). 3.2 Improving Shampooâ€™s estimation: Idealized KL-Shampoo Our KL perspective reveals a keylimitationâ€”empirically demonstrated in Fig. 5â€”of Shampooâ€™s Kronecker estimation with ğ‘= 1/2as a one-sided approach: it does not adequately solve the KL-minimization problem when both factors are learned jointly. Motivated by this, we design an improved estimation rule that updates the two factors simultaneously. We term this scheme asidealized KL-Shampoo, which is a two-sided approach. Claim 2 (Idealized KL-Shampooâ€™s covariance estimation for Sğ‘andSğ‘)The optimal solution of KL minimiza- tionmin Sğ‘,Sğ‘KL\u0000E[ggâŠ¤],S\u0001with a two-sided precontionerS=S ğ‘âŠ—Sğ‘should satisfy the following condition. Sâˆ— ğ‘=1 ğ‘‘ğ‘E[G\u0000Sâˆ— ğ‘\u0001âˆ’1GâŠ¤],Sâˆ— ğ‘=1",
    "design an improved estimation rule that updates the two factors simultaneously. We term this scheme asidealized KL-Shampoo, which is a two-sided approach. Claim 2 (Idealized KL-Shampooâ€™s covariance estimation for Sğ‘andSğ‘)The optimal solution of KL minimiza- tionmin Sğ‘,Sğ‘KL\u0000E[ggâŠ¤],S\u0001with a two-sided precontionerS=S ğ‘âŠ—Sğ‘should satisfy the following condition. Sâˆ— ğ‘=1 ğ‘‘ğ‘E[G\u0000Sâˆ— ğ‘\u0001âˆ’1GâŠ¤],Sâˆ— ğ‘=1 ğ‘‘ğ‘E[GâŠ¤\u0000Sâˆ— ğ‘\u0001âˆ’1G].(4) Idealized KL-Shampooâ€™s estimationClaim 2 (proof in Sec. B) establishes a closed-form condition (see Eq. (4)) when simultaneously learning both Kronecker factors to minimize the KL problem. Statistically, this condition corresponds to the maximum-likelihood covariance estimator of a zero-mean matrix Gaussian (Dutilleul, 1999). This condition implies that Gis generated from a zero-mean matrix Gaussian with row-wise covariance Sâˆ— ğ‘ and column-wise covariance Sâˆ— ğ‘. Under this condition, Shampoo-style preconditioning naturally induces matrix- Gaussian (row and column) whitening. Our KL-based approach further extends this to tensor-Gaussian whitening for tensor-valued gradientsâ€”without the prohibitive cost typically associated with SVD-based methods. Notably, this kind of whitening arises from minimizing KL divergence, not the Frobenius norm. In machine learning, Lin et al. (2019, 2024) treated this condition as a theoretical example of a multilinear exponential-family (see Sec 5 of Lin et al. (2019)) for Kronecker-based optimization via natural gradient descent on an exponential-family manifold, while more recently, Vyas et al. (2025b) considered a similar condition motivated heuristically by gradient whitening. However, we cannot directly use this condition due to thecorrelatedupdate between Sğ‘and 5 KL-Shampoo and KL-SOAP Wu Lin, et al. (2025) Sğ‘. For example, solving Sâˆ— ğ‘requires knowing Sâˆ— ğ‘in Eq. (4) or vice versa. In practice, this optimal condition is often unachievable because the expectations in Eq. (4) have to be approximated. Thus, we consider an estimated Sğ‘˜to approximate Sâˆ— ğ‘˜forğ‘˜âˆˆ{ğ‘,ğ‘} and propose an exponential moving average (EMA) scheme in practice: Sğ‘â†(1âˆ’ğ›½ 2)Sğ‘+ğ›½2 ğ‘‘ğ‘GSâˆ’1 ğ‘GâŠ¤,Sğ‘â†(1âˆ’ğ›½ 2)Sğ‘+ğ›½2 ğ‘‘ğ‘GâŠ¤Sâˆ’1 ğ‘G.(5) Our KL perspective allows us to further justify this EMA scheme as a stochastic proximal-gradient step (see Claim 3 and a proof in Sec. C) and establish a formal connection to the theoretical example of Lin et al. (2019, 2024). Notably, our approach uses Sâˆ’1/2for preconditioning (Eq. (1)), following Shampoo, whereas Lin et al. (2019, 2024) propose usingSâˆ’1. A straightforward implementation of our scheme is computationally expensive, since it requires expensive matrix inversions (highlighted in red in Eq. (5)) and the slow eigendecomposition for Shampoo-type preconditioning (e.g., Sâˆ’1/2). However, these issues can be alleviatedâ€”in Sec. 4 we propose an efficient implementation. Claim 3 (KL-Shampooâ€™s moving average scheme)The moving average scheme for Sğ‘˜(Eq.(5)) in idealized KL-Shampoo is a stochastic proximal-gradient step with step-size ğ›½2to solve the KL minimization problem in Eq.(3), forğ‘˜âˆˆ{ğ‘,ğ‘} . Recall that ğ›½2in Eq.(5)is closely related to Adamâ€™s ğ›½2asğ›½2=1âˆ’ğ›½(Adam) 2, whereğ›½(Adam) 2is Adamâ€™sğ›½2. 3.3 Comparison with Kronecker Schemes Using Alternative Divergences Frobenius normMorwani et al. (2025) consider a two-sided Shampoo variant based on the Frobenius norm and derive the optimal solution via rank-1 singular value decomposition (SVD) of the outer product E[ggâŠ¤](Van Loan & Pitsianis, 1993). However, this optimal solution is often unattainable in practice and is computationally expensive for two reasons: (1) the expectationE[ggâŠ¤]must be approximated; and (2) performing the SVD itself is costlyâ€”",
    "norm and derive the optimal solution via rank-1 singular value decomposition (SVD) of the outer product E[ggâŠ¤](Van Loan & Pitsianis, 1993). However, this optimal solution is often unattainable in practice and is computationally expensive for two reasons: (1) the expectationE[ggâŠ¤]must be approximated; and (2) performing the SVD itself is costlyâ€” yielding complexity ( ğ‘‚(ğ‘‘2 ğ‘ğ‘‘2 ğ‘)) in general even for rank-1 SVDâ€”which is higher than the eigendecompositions with complexity ( ğ‘‚(ğ‘‘3 ğ‘˜)) forğ‘˜âˆˆ{ğ‘,ğ‘} that we explicitly aim to avoid. Instead, we analyze the stationarity conditions (Claim 6, Sec. F) and derive a new variant, idealized F-Shampoo (Fig. 6, Sec. F), that is structurally similar to KL-Shampoo. While a straightforward implementation of F-Shampoo performs poorly in practice, the techniques we develop for KL-Shampoo in Sec. 4 can be adapted to improve its performance (see Fig. 7, Sec. H). von Neumann divergenceAnother Shampoo variant, often discussed in the literature (Morwani et al., 2025; Vyas et al., 2025a; Eschenhagen et al., 2025), is Shampoo with trace scaling. Vyas et al. (2025a) established that Shampoo with trace scaling is equivalent to running Adafactor (Shazeer & Stern, 2018) in Shampooâ€™s eigenbasis. In contrast, KL-Shampoo is not equivalent to running Adafactor in its eigenbasis. To clarify this distinction, we make the theoretical connection between Shampoo and Adafactor more explicit: Shampoo with trace scaling is exactly a matrix generalization of Adafactor obtained by minimizing the von Neumann (VN) divergence (Tsuda et al., 2005; Dhillon & Tropp, 2008; Nock et al., 2012) and recovers Adafactor when its Kronecker factors are restricted to be diagonal, as we establish in Claim 7 (Sec. G). By contrast, KL-Shampoo minimizes the KL divergence instead of the VN divergence. A straightforward implementation of Shampoo with trace scalingâ€”referred to as idealized VN-Shampooâ€”performs poorly in practice. Howeve, the techniques we develop for KL-Shampoo in Sec. 4 can be adapted (Fig. 8, Sec. G) to substantially improve its performance, as shown in Fig. 9 (Sec. H). A natural question then arises: which â€œdistanceâ€ is more suitable? Theoretically, the KL divergence is broadly applicable to arbitrary SPD matrices (Bhatia, 2007; Boumal et al., 2014) and is widely used for covariance matrices (Minh & Murino, 2017). In contrast, the Frobenius norm does not respect the SPD constraint, and the VN divergence is primarily motivated, studied, and applied for unit-trace SPD matrices (Tsuda et al., 2005; Nielsen & Chuang, 2010). Empirically, adopting the KL divergence yields larger improvements than both the Frobenius norm and the VN divergence for designing Shampooâ€™s schemes (see Fig. 5) and in other applications (Kulis et al., 2009). 4Efficient Implementation: KL-Shampoo with QR Decomposition We develop techniques that enable KL-Shampoo to match SOAP-level per-iteration runtime and to achieve competitive performance without step-size grafting, all without relying on eigendecomposition. Vyas et al. (2025a) demonstrated that the eigendecomposition used in Shampooâ€™s implementation (Shi et al., 2023) is more computationally expensive than QR decomposition. Motivated by this result, we aim to improve KL-Shampooâ€™s computational efficiency by replacing the eigendecomposition with QR decomposition. However, incorporating QR decomposition into KL-Shampoo is non-trivial because the eigenvalues of the Kronecker factors are required, and",
    "in Shampooâ€™s implementation (Shi et al., 2023) is more computationally expensive than QR decomposition. Motivated by this result, we aim to improve KL-Shampooâ€™s computational efficiency by replacing the eigendecomposition with QR decomposition. However, incorporating QR decomposition into KL-Shampoo is non-trivial because the eigenvalues of the Kronecker factors are required, and QR does not directly provide them without a significant overhead. Specifically, the eigenvalues are essential for a reduction in the computational cost of KL-Shampoo in two reasons: (1) they remove the need to compute the matrix 6 KL-Shampoo and KL-SOAP Wu Lin, et al. (2025) Shampoo with powerğ‘= 1/2versus Our idealized KL-Shampoo 1:Gradient Computationg:=âˆ‡â„“(Î¸) G:=Mat(g)âˆˆRğ‘‘ğ‘Ã—ğ‘‘ğ‘ 2:Covariance Estimation (each iter)\u0012Sğ‘ Sğ‘\u0013 â†(1âˆ’ğ›½ 2)\u0012Sğ‘ Sğ‘\u0013 +ğ›½2\u0012Î”ğ‘ Î”ğ‘\u0013 Î”ğ‘:=( GGâŠ¤ GQğ‘Diag(Î»âŠ™âˆ’1 ğ‘)QâŠ¤ ğ‘GâŠ¤/ğ‘‘ğ‘ Î”ğ‘:=( GâŠ¤G GâŠ¤Qğ‘Diag(Î»âŠ™âˆ’1 ğ‘)QâŠ¤ ğ‘G/ğ‘‘ğ‘ 3:Eigendecomposition (everyğ‘‡â‰¥1iters) Î»ğ‘˜,Qğ‘˜â†eig(Sğ‘˜)forğ‘˜âˆˆ{ğ‘,ğ‘} 4:Preconditioning usingQ:=Q ğ‘âŠ—Qğ‘ Î¸â†Î¸âˆ’ğ›¾(QDiag(Î» ğ‘âŠ—Î»ğ‘)âˆ’1/2QâŠ¤)g Figure 3:Left:Simplified Shampoo-based schemes without momentum, damping, and weight decay. See Fig. 5 for an empirical comparison.Top Right: For computational efficiency, we replace the eigen step with our exponential moving average (EMA) scheme to estimate eigenvalues and infrequent eigenbasis estimation using QR, where we esti- mate eigenvalues Î»ğ‘˜using an outdated eigenbasis Qğ‘˜forğ‘˜âˆˆ{ğ‘,ğ‘} , and use the QR procedure to es- timate Qğ‘˜.Bottom Right: Simplified SOAP-based schemes without momentum. Notably, KL-SOAP needs estimation for Î»ğ‘˜in Step 3a to compute the eigenbasisQ, whereas SOAP does not.Replacing the slow eigendecomposition with more efficient QR updates(replace Step 3) 3a: Eigenvalue Estimation with EMA (each iter) (Kronecker-factored Diagonal VectorÎ» ğ‘âŠ—Î»ğ‘)\u0012Î»ğ‘ Î»ğ‘\u0013 â†(1âˆ’ğ›½ 2)\u0012Î»ğ‘ Î»ğ‘\u0013 +ğ›½2\u0012diag(QâŠ¤ ğ‘Î”ğ‘Qğ‘) diag(QâŠ¤ ğ‘Î”ğ‘Qğ‘)\u0013 3b: Infrequent Eigenbasis Estimation using QR (everyğ‘‡â‰¥1iters) Qğ‘˜â†qr(Sğ‘˜Qğ‘˜)forğ‘˜âˆˆ{ğ‘,ğ‘} SOAP (ues Shampooâ€™s eigenbasis) versus Our KL-SOAP (uses KL-Shampooâ€™s eigenbasis): Using augmented preconditioners and QR updates (replace Step 4) 4a:AugmentedEigenvalue Estimation with EMA (each iter) (Full Diagonal Vectordas RMSPropâ€™s 2ndmoment) dâ†(1âˆ’ğ›½ 2)d+ğ›½ 2Ë†gâŠ™2 Ë†g:=QâŠ¤g=vec\u0000QâŠ¤ ğ‘GQğ‘\u0001 4b: Preconditioning using Augmented Eigenvalues with EigenbasisQ:=Q ğ‘âŠ—Qğ‘ Î¸â†Î¸âˆ’ğ›¾\u0000QDiag(d)âˆ’1/2QâŠ¤\u0001g Equivalent to running RMSProp in Eigenbasis: Mat(Î¸)â†Mat(Î¸)âˆ’ğ›¾Q ğ‘Mat\u0000Ë†gâˆš d|{z} RMSProp\u0001QâŠ¤ ğ‘ âˆ’1/2power, Sâˆ’1/2=(Qğ‘Diag(Î»âŠ™âˆ’1/2 ğ‘)QâŠ¤ ğ‘)âŠ—(Qğ‘Diag(Î»âŠ™âˆ’1/2 ğ‘)QâŠ¤ ğ‘), used for KL-Shampooâ€™s preconditioning; (2) they eliminate expensive matrix inversions in its Kronecker estimation rule (Eq. (5)), such as Sâˆ’1 ğ‘=Pğ‘:= Qğ‘Diag(Î»âŠ™âˆ’1 ğ‘)QâŠ¤ ğ‘in the update forS ğ‘: Sğ‘â†(1âˆ’ğ›½ 2)Sğ‘+ğ›½2 ğ‘‘ğ‘GSâˆ’1 ğ‘GâŠ¤=(1âˆ’ğ›½ 2)Sğ‘+ğ›½2 ğ‘‘ğ‘GPğ‘GâŠ¤,(6) whereQğ‘˜andÎ»ğ‘˜are eigenbasis and eigenvalues ofS ğ‘˜forğ‘˜âˆˆ{ğ‘,ğ‘}, respectively. KL-based estimation rule for the eigenvalues Î»ğ‘andÎ»ğ‘using an outdated eigenbasisWe aim to estimate the eigenvalues using an outdated eigenbasis and replace the slow eigendecomposition with a fast QR decomposition in KL-Shampoo. Eschenhagen et al. (2025) propose estimating the eigenvalues from a Frobenius- norm perspective, using an instantaneous scheme: Î»(inst) ğ‘˜:=diag(QâŠ¤ ğ‘˜Sğ‘˜Qğ‘˜)forğ‘˜âˆˆ{ğ‘,ğ‘} . However, our empirical results (Fig. 4) indicate that this approach becomes suboptimal when an outdated eigenbasis Qğ‘˜is reused to reduce the frequency and cost of QR decompositions. In contrast, our KL perspective (see Claim 4 and its proof in Sec. D) provides a principled alternative, allowing us to use an outdated eigenbasis. Building on this claim, we introduce an exponential moving average (EMA) scheme (Step 3a of Fig. 3) for eigenvalue estimation, which can be justified as a stochastic proximal-gradient step under our KL perspective, similar to Claim 3. This scheme updates the eigenvaluesat every iterationwhile updating the eigenbasis less frequently through an efficient",
    "this claim, we introduce an exponential moving average (EMA) scheme (Step 3a of Fig. 3) for eigenvalue estimation, which can be justified as a stochastic proximal-gradient step under our KL perspective, similar to Claim 3. This scheme updates the eigenvaluesat every iterationwhile updating the eigenbasis less frequently through an efficient QR-based procedure, similar to SOAP. We can view this estimation as an eigenvalue correction for using an outdated eigenbasis, as will be discussed in Sec. 5. Since it naturally scales the eigenvalues by the dimensions of the Kronecker factors, step-size grafting should not be necessary for KL-Shampoo, as argued by Eschenhagen et al. (2025) and confirmed by our empirical results (Fig. 2). Furthermore, applying this scheme enables other variants of Shampoo to be competitive and even outperform SOAP, as empirically demonstrated in Figs. 7, 9 and 10 of Sec. H. These empirical results underscore the importance of our EMA scheme on eigenvalues. Claim 4(Covariance estimation for eigenvaluesÎ» ğ‘andÎ»ğ‘)The optimal solution of KL minimization 7 KL-Shampoo and KL-SOAP Wu Lin, et al. (2025) minÎ»ğ‘,Î»ğ‘KL\u0000E[ggâŠ¤],S\u0001with preconditionerS=(Q ğ‘Diag(Î»ğ‘)QâŠ¤ ğ‘)âŠ—(Qğ‘Diag(Î»ğ‘)QâŠ¤ ğ‘)satisfies this condition: Î»âˆ— ğ‘=1 ğ‘‘ğ‘diag\u0000QâŠ¤ ğ‘E[GPâˆ— ğ‘GâŠ¤]Qğ‘\u0001,Î»âˆ— ğ‘=1 ğ‘‘ğ‘diag\u0000QâŠ¤ ğ‘E[GâŠ¤Pâˆ— ğ‘G]Qğ‘\u0001,(7) where Pâˆ— ğ‘˜:=Qğ‘˜Diag\u0010 (Î»âˆ— ğ‘˜)âŠ™âˆ’1\u0011 QâŠ¤ ğ‘˜is also defined in Eq.(6)and considered as an approximation of Sâˆ’1 ğ‘˜for ğ‘˜âˆˆ{ğ‘,ğ‘}when using an outdated eigenbasisQ=Q ğ‘âŠ—Qğ‘precomputed by QR. 5 Interpreting and Improving SOAP via KL Minimization We extend the KL perspective to better understand and improve the estimation scheme used in SOAP. Interpreting SOAPâ€™s estimation as covariance estimationRecall that SOAP (Eq. (2)) applies Shampooâ€™s scheme to estimate its Kronecker factors and then performs RMSProp updates in the eigenbasis of these factors. Consequently, the interpretation of SOAPâ€™s Kronecker factor estimation is identical to that of Shampoo. RMSPropâ€™s second-moment estimation in the eigenbasis can itself be interpreted as the optimal solution to a separate KL divergence minimization problem, as established in Claim 5 (see Sec. E for a proof). The KL perspectiveâ€” distinct from the Frobenius-norm viewpoint (George et al., 2018; Eschenhagen et al., 2025)â€”provides a new lens for understanding RMSPropâ€™s estimation in the eigenbasis as the estimation of augmented eigenvalues of a covariance matrix under KL divergence. When an outdated eigenbasis is used, RMSPropâ€™s scheme (Step 4a of Fig. 3) for eigenvalue estimation can be viewed as a correction in an augmented (full-diagonal) space, QDiag(d)QâŠ¤, analogous in spirit to the Frobenius-norm interpretation but derived under the KL framework. This perspective also highlights a close similarity to KL-Shampooâ€™s estimation scheme: recall that we introduced a comparable correction (Step 3a of Fig. 3) for KL-Shampoo, but in the original Kronecker-factored diagonal space, QDiag(Î» ğ‘âŠ—Î»ğ‘)QâŠ¤. Claim 5 (SOAP and KL-SOAPâ€™s covariance estimation for augmented eigenvalues d)The optimal solution of KL minimization: mindKL\u0000E[ggâŠ¤],S\u0001with preconditioner S=QDiag(d)QâŠ¤isdâˆ—=Eh\u0000vec(QâŠ¤ ğ‘GQğ‘)\u0001âŠ™2i = E\u0002Ë†gâŠ™2\u0003 , where dâˆˆRğ‘‘ğ‘ğ‘‘ğ‘Ã—1is viewed as an augmented eigenvalue vector, Ë†g=QâŠ¤gis defined at the update of (KL-)SOAP (see Eq.(2)), andQ=Q ğ‘âŠ—Qğ‘can be an outdated eigenbasis of (KL-)Shampooâ€™s preconditioner. Improving SOAPâ€™s estimationSimilar to SOAP, we propose KL-SOAP, which utilizes KL-Shampooâ€™s estimation to update Kronecker factors and additionally employs Adam (RMSProp) in KL-Shampooâ€™s eigenbasis. Our unified KL perspective enables us to reuse Claim 5 to justify",
    "update of (KL-)SOAP (see Eq.(2)), andQ=Q ğ‘âŠ—Qğ‘can be an outdated eigenbasis of (KL-)Shampooâ€™s preconditioner. Improving SOAPâ€™s estimationSimilar to SOAP, we propose KL-SOAP, which utilizes KL-Shampooâ€™s estimation to update Kronecker factors and additionally employs Adam (RMSProp) in KL-Shampooâ€™s eigenbasis. Our unified KL perspective enables us to reuse Claim 5 to justify the use of Adamâ€™s (RMSPropâ€™s) 2ndmoment estimation as augmented eigenvalue estimation in KL-SOAP. 6 Experimental Setup and Empirical Evaluations We consider four sets of experiments to demonstrate the benefits of using the KL divergence and the effectiveness of KL-based methods. See Sec. H for additional experiments. Experimental SetupIn all the experiments, we consider training four language models based on existing implementations: NanoGPT (Jordan, 2024) (123 M), NanoRWKV7 (Bo, 2024) (162 M), Llama (Glentis, 2025) (134 M), and NanoMoE (Wolfe, 2025) (227 M). We consider NanoMoE, as it contains 3D weight tensors. This model provides a natural testbed for evaluating a tensor extension of KL-Shampoo and KL-SOAP, derived directly from our KL perspective. In doing so, we demonstrate that our methods retain the same flexibility as Shampoo and SOAP in handling tensor-valued weights without reshaping them into matrices. We train NanoGPT and NanoRWKV7 using a subset of FineWeb (1 B tokens), Llama using a subset of C4 (2 B tokens), and NanoMoE using a subset of OpenWebText (2.5 B tokens). All models except NanoMoE are trained using mini-batches with a batch size of 0.5 M. We use a batch size of 0.25 M to train NanoMoE to reduce the run time. We use the default step-size schedulers from the source implementations; NanoGPT and NanoRWKV7: linear warmup + constant step-size + linear cooldown; Llama and NanoMoE: linear warmup + cosine step-size. We tune all available hyperparameters for each methodâ€”including step-size, moving average, weight decay, damping, and momentumâ€”using random search with 150 runs. Our hyperparameter search follows a two-stage strategy, with 75 runs in each stage. In the first stage, we search over a wider range of hyperparameters. In the second stage, we refine the search space based on the results from the first stage and focus on a narrower range. In our experiments, Shampoo by default performs eigendecomposition every 10 steps, while SOAP, KL-Shampoo, and KL-SOAP perform QR decomposition every 10 steps, as suggested by Vyas et al. (2025a). 8 KL-Shampoo and KL-SOAP Wu Lin, et al. (2025) 10 8 6 4 2 Iterations (thousands)5.0 4.5 4.0 3.8 3.5 3.2Test LossNanoGPT-FineWeb1B 10 8 6 4 2 Iterations (thousands)6.5 6.0 5.5 5.0 4.5 4.0 3.5 3.1Llama-134M-C4-2B KL-Shampoo KL-Shampoo (instantaneous) Figure 4: Empirical results (random search using 150 runs for each method) demonstrate that our EMA scheme for the eigenvalue estimation makes KL- Shampoo competitive when using an outdated eigen- basis. Without this scheme, KL-Shampoo performs poorly under an outdated eigenbasis Qğ‘˜even when employing the instantaneous eigenvalue estimation Î»(inst) ğ‘˜=diag(QâŠ¤ ğ‘˜Sğ‘˜Qğ‘˜)at every iteration, as sug- gested by Eschenhagen et al. (2025) for ğ‘˜âˆˆ {ğ‘,ğ‘} . Adapting the EMA scheme also makes other variants of Shampoo competitive (Figs. 7 and 9, Sec. H) and allows the trace-scaling variant to outperform SOAP (Fig. 10, Sec. H). 3.54.04.55.0NanoGPT-123M-FineWeb1B 4.04.55.0Llama-134M-C4-2B",
    "eigenvalue estimation Î»(inst) ğ‘˜=diag(QâŠ¤ ğ‘˜Sğ‘˜Qğ‘˜)at every iteration, as sug- gested by Eschenhagen et al. (2025) for ğ‘˜âˆˆ {ğ‘,ğ‘} . Adapting the EMA scheme also makes other variants of Shampoo competitive (Figs. 7 and 9, Sec. H) and allows the trace-scaling variant to outperform SOAP (Fig. 10, Sec. H). 3.54.04.55.0NanoGPT-123M-FineWeb1B 4.04.55.0Llama-134M-C4-2B 10 5 1 Iterations (thousands)3.31 3.28 3.26 3.24aa 10 5 1 Iterations (thousands)3.25 3.20 3.18Test Loss KL-Shampoo Shampoo F-Shampoo VN-ShampooFigure 5: Empirical resultsâ€”based on random search with 150 runs per methodâ€”demonstrate the advan- tages of KL-Shampooâ€™s (two-sided) estimation over other Shampoo variants under comparable settings for NN training, including Shampoo with ğ‘= 1/2(one- sided, no grafting), F-Shampoo (two-sided, Frobenius- normâ€“based), and VN-Shampoo (trace scaling, two- sided von-Neumann-divergence-based). We make these variants practical by incorporating a QR step and an EMA scheme for eigenvalue estimation (Fig. 3). To ensure a fair comparison and minimize implemen- tation bias, we implement Shampoo, F-Shampoo, and VN-Shampoo ourselves, aligning them closely with KL- Shampoo. See Fig. 10 (Sec. H) for a detailed comparison between KL-Shampoo and VN-Shampoo. In the first set of experiments, we demonstrate that our KL-based perspective enables a principled redesign of Shampoo, resulting in KL-Shampoo, and achieves superior performance without step-size grafting. We evaluate Shampoo with matrix powers ğ‘= 1/2andğ‘= 1/4, using a state-of-the-art implementation (Shi et al., 2023). As shown in Fig. 2, Shampoo requires step-size grafting to perform well, whereas KL-Shampoo performs robustly without it. Moreover, KL-Shampoo outperforms Shampoo with graftingâ€”even in terms of step-wise progressâ€”even when Shampoo is equipped with eigendecomposition and step-size grafting via Adam. In the second set of experiments, we demonstrate that our QR-based scheme enables KL-Shampoo and KL-SOAP to achieve the same pre-iteration runtime as SOAP. We use the official SOAP implementation for comparison. As shown in Fig. 1, KL-Shampoo and KL-SOAP outperform SOAP. Remarkably, KL-Shampoo also consistently surpasses KL-SOAP while using less memory. In the third set of experiments, we underscore the importance of using our EMA scheme for the eigenvalue estimation when working with an outdated eigenbasis. As shown in Fig. 4, the EMA scheme enables KL-Shampoo to perform well in practice, even under stale eigenbases. Moreover, this scheme can be adapted to strengthen the trace scaling variant of Shampoo (Fig. 9, Sec. H), enabling it to outperform SOAP (Fig. 10, Sec. H). In the last set of experiments, we evaluate the benefits of using the two-sided estimation scheme under our KL perspective. Specifically, we compare the two-sided approach (KL-Shampoo) against the one-sided approach (Shampoo) in a comparable setting. To ensure fairness and eliminate implementation bias, we use our own implementation of Shampoo aligned closely with that of KL-Shampoo. For this comparison, we extend Shampoo with a QR-based step and our EMA scheme for eigenvalue estimation, as described in Fig. 3. As shown in Fig. 5, KL-Shampoo consistently and significantly outperforms Shampoo, even when Shampoo employs a similar QR-based estimation rule. 7 Conclusion We introduced a KL perspective for interpreting Shampooâ€™s and SOAPâ€™s structured second-moment estimation schemes. This perspective uncovers a previously unrecognized limitation of Shampoo, motivates an alternative estimation strategy to overcome it, enables",
    "5, KL-Shampoo consistently and significantly outperforms Shampoo, even when Shampoo employs a similar QR-based estimation rule. 7 Conclusion We introduced a KL perspective for interpreting Shampooâ€™s and SOAPâ€™s structured second-moment estimation schemes. This perspective uncovers a previously unrecognized limitation of Shampoo, motivates an alternative estimation strategy to overcome it, enables a practical implementation of our approach, and extends naturally to tensor-valued estimation. Our empirical results demonstrate the effectiveness of our approach for improving 9 KL-Shampoo and KL-SOAP Wu Lin, et al. (2025) Shampooâ€™s and SOAPâ€™s estimation schemes. References Naman Agarwal, Rohan Anil, Elad Hazan, Tomer Koren, and Cyril Zhang. Disentangling adaptive gradient methods from learning rates.arXiv preprint arXiv:2002.11803, 2020. doi:10.48550/arxiv.2002.11803. Shun-ichi Amari.Information geometry and its applications, volume 194. Springer, 2016. ISBN 9784431559788. doi:10.1007/978-4-431-55978-8. Kang An, Yuxing Liu, Rui Pan, Yi Ren, Shiqian Ma, Donald Goldfarb, and Tong Zhang. ASGO: Adaptive structured gradient optimization.arXiv preprint arXiv:2503.20762, 2025. doi:10.48550/arxiv.2503.20762. Rohan Anil, Vineet Gupta, Tomer Koren, Kevin Regan, and Yoram Singer. Scalable second order optimization for deep learning.arXiv preprint arXiv:2002.09018, 2020. doi:10.48550/arxiv.2002.09018. Rajendra Bhatia.Positive definite matrices. Princeton University Press, 2007. ISBN 9780691129181. URL http: //www.jstor.org/stable/j.ctt7rxv2. Peng Bo. RWKV-7: Surpassing GPT. https://github.com/BlinkDL/modded-nanogpt-rwkv , 2024. Accessed: 2025-06. Nicolas Boumal, Bamdev Mishra, P.-A. Absil, and Rodolphe Sepulchre. Manopt, a matlab toolbox for optimization on manifolds.Journal of Machine Learning Research, 15(42):1455â€“1459, 2014. URL http://jmlr.org/papers/ v15/boumal14a.html. Lev M Bregman. The relaxation method of finding the common point of convex sets and its application to the solution of problems in convex programming.USSR computational mathematics and mathematical physics, 7(3): 200â€“217, 1967. ISSN 0041-5553. doi:10.1016/0041-5553(67)90040-7. Richard H Byrd and Jorge Nocedal. A tool for the analysis of quasi-Newton methods with application to unconstrained minimization.SIAM Journal on Numerical Analysis, 26(3):727â€“739, 1989. doi:10.1137/0726042. George E Dahl, Frank Schneider, Zachary Nado, Naman Agarwal, Chandramouli Shama Sastry, Philipp Hennig, Sourabh Medapati, Runa Eschenhagen, Priya Kasimbeg, Daniel Suo, et al. Benchmarking neural network training algorithms.arXiv preprint arXiv:2306.07179, 2023. doi:10.48550/arxiv.2306.07179. Jason Davis and Inderjit Dhillon. Differential entropic clustering of multivariate gaussians.Advances in Neural Information Processing Systems, 19, 2006. Jan de Boer, Victor Godet, Jani Kastikainen, and Esko Keski-Vakkuri. Quantum information geometry of driven CFTs.Journal of High Energy Physics, 2023(9):1â€“89, 2023. doi:10.1007/JHEP09(2023)087. Inderjit S Dhillon and Joel A Tropp. Matrix nearness problems with Bregman divergences.SIAM Journal on Matrix Analysis and Applications, 29(4):1120â€“1146, 2008. doi:10.1137/060649021. John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization.Journal of Machine Learning Research, 12(61):2121â€“2159, 2011. URL http://jmlr.org/papers/ v12/duchi11a.html. Pierre Dutilleul. The MLE algorithm for the matrix normal distribution.Journal of Statistical Computation and Simulation, 64(2):105â€“123, 1999. doi:10.1080/00949659908811970. Sai Surya Duvvuri, Fnu Devvrit, Rohan Anil, Cho-Jui Hsieh, and Inderjit S Dhillon. Combining axes preconditioners through Kronecker approximation for deep learning. InThe Twelfth International Conference on Learning Representations, 2024. URLhttps://openreview.net/forum?id=8j9hz8DVi8. Runa Eschenhagen, Aaron Defazio, Tsung-Hsien Lee, Richard E Turner, and Hao-Jun Michael Shi. Purifying Shampoo: Investigating Shampooâ€™s heuristics by decomposing its preconditioner.arXiv preprint arXiv:2506.03595, 2025. doi:10.48550/arxiv.2506.03595. Roger Fletcher. A new variational result for quasi-Newton formulae.SIAM Journal on Optimization, 1(1):18â€“21, 1991. doi:10.1137/0801002. 10 KL-Shampoo and KL-SOAP Wu Lin, et al. (2025) Thomas George, CÃ©sar Laurent, Xavier Bouthillier, Nicolas Ballas, and Pascal Vincent. Fast",
    "Shi. Purifying Shampoo: Investigating Shampooâ€™s heuristics by decomposing its preconditioner.arXiv preprint arXiv:2506.03595, 2025. doi:10.48550/arxiv.2506.03595. Roger Fletcher. A new variational result for quasi-Newton formulae.SIAM Journal on Optimization, 1(1):18â€“21, 1991. doi:10.1137/0801002. 10 KL-Shampoo and KL-SOAP Wu Lin, et al. (2025) Thomas George, CÃ©sar Laurent, Xavier Bouthillier, Nicolas Ballas, and Pascal Vincent. Fast approximate natural gradient descent in a kronecker factored eigenbasis. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.),Advances in Neural Information Processing Systems, volume 31. Cur- ran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper_files/paper/2018/file/ 48000647b315f6f00f913caa757a70b3-Paper.pdf. Athanasios Glentis. A minimalist optimizer design for LLM pretraining. https://github.com/OptimAI-Lab/ Minimalist_LLM_Pretraining, 2025. Accessed: 2025-06. Vineet Gupta, Tomer Koren, and Yoram Singer. Shampoo: Preconditioned stochastic tensor optimization. In Jennifer Dy and Andreas Krause (eds.),Proceedings of the 35th International Conference on Machine Learning, volume 80 ofProceedings of Machine Learning Research, pp. 1842â€“1850. PMLR, 10â€“15 Jul 2018. URL https: //proceedings.mlr.press/v80/gupta18a.html. William James, Charles Stein, et al. Estimation with quadratic loss. InProceedings of the fourth Berkeley symposium on mathematical statistics and probability, volume 1, pp. 361â€“379. University of California Press, 1961. Keller Jordan. NanoGPT (124M) in 3 minutes.https://github.com/KellerJordan/modded-nanogpt, 2024. Accessed: 2025-06. Takafumi Kanamori and Atsumi Ohara. A Bregman extension of quasi-Newton updates I: an information geomet- rical framework.Optimization Methods and Software, 28(1):96â€“123, 2013a. doi:10.1080/10556788.2011.613073. Takafumi Kanamori and Atsumi Ohara. A Bregman extension of quasi-Newton updates II: Analysis of robustness properties.Journal of computational and applied mathematics, 253:104â€“122, 2013b. doi:10.1016/j.cam.2013.04.005. Priya Kasimbeg, Frank Schneider, Runa Eschenhagen, Juhan Bae, Chandramouli Shama Sastry, Mark Saroufim, Boyuan Fend, Less Wright, Edward Z Yang, Zachary Nado, et al. Accelerating neural network training: An analysis of the AlgoPerf competition. InThe Thirteenth International Conference on Learning Representations, 2025. URLhttps://openreview.net/forum?id=CtM5xjRSfm. Mohammad Khan and Wu Lin. Conjugate-computation variational inference: Converting variational inference in non-conjugate models to inferences in conjugate models. In Aarti Singh and Jerry Zhu (eds.),Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, volume 54 ofProceedings of Machine Learning Research, pp. 878â€“887. PMLR, 20â€“22 Apr 2017. URL https://proceedings.mlr.press/v54/khan17a.html . Mohammad Emtiyaz Khan, Reza Babanezhad, Wu Lin, Mark Schmidt, and Masashi Sugiyama. Faster stochastic variational inference using proximal-gradient methods with general divergence functions. InProceedings of the Thirty-Second Conference on Uncertainty in Artificial Intelligence, pp. 319â€“328. AUAI Press, 2016. URL https://www.auai.org/uai2016/proceedings/papers/218.pdf. Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. InInternational Conference on Learning Representations, 2015. doi:10.48550/arxiv.1412.6980. Jyrki Kivinen and Manfred K. Warmuth. Boosting as entropy projection. InProceedings of the Twelfth Annual Conference on Computational Learning Theory, pp. 134â€“144, New York, NY, USA, 1999. Association for Computing Machinery. ISBN 1581131674. doi:10.1145/307400.307424. Brian Kulis, MÃ¡tyÃ¡s A Sustik, and Inderjit S Dhillon. Low-rank kernel learning with bregman matrix divergences. Journal of Machine Learning Research, 10(2), 2009. Frederik Kunstner, Raunak Kumar, and Mark Schmidt. Homeomorphic-invariance of EM: Non-asymptotic convergence in KL divergence for exponential families via mirror descent. In Arindam Banerjee and Kenji Fukumizu (eds.),Proceedings of The 24th International Conference on Artificial Intelligence and Statistics, volume 130 ofProceedings of Machine Learning Research, pp. 3295â€“3303. PMLR, 13â€“15 Apr 2021. URL https:// proceedings.mlr.press/v130/kunstner21a.html. Wu Lin, Mohammad Emtiyaz Khan, and Mark Schmidt. Fast and simple natural-gradient variational inference",
    "via mirror descent. In Arindam Banerjee and Kenji Fukumizu (eds.),Proceedings of The 24th International Conference on Artificial Intelligence and Statistics, volume 130 ofProceedings of Machine Learning Research, pp. 3295â€“3303. PMLR, 13â€“15 Apr 2021. URL https:// proceedings.mlr.press/v130/kunstner21a.html. Wu Lin, Mohammad Emtiyaz Khan, and Mark Schmidt. Fast and simple natural-gradient variational inference with mixture of exponential-family approximations. InInternational Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 3992â€“4002. PMLR, 09â€“15 Jun 2019. URL https://proceedings. mlr.press/v97/lin19b.html. 11 KL-Shampoo and KL-SOAP Wu Lin, et al. (2025) Wu Lin, Valentin Duruisseaux, Melvin Leok, Frank Nielsen, Mohammad Emtiyaz Khan, and Mark Schmidt. Simplifying momentum-based positive-definite submanifold optimization with applications to deep learning. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.),Proceedings of the 40th International Conference on Machine Learning, volume 202 ofProceedings of Machine Learning Research, pp. 21026â€“21050. PMLR, 23â€“29 Jul 2023. URL https://proceedings.mlr.press/v202/ lin23c.html. Wu Lin, Felix Dangel, Runa Eschenhagen, Juhan Bae, Richard E Turner, and Alireza Makhzani. Can we remove the square-root in adaptive gradient methods? A second-order perspective. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp (eds.),Proceedings of the 41st International Conference on Machine Learning, volume 235 ofProceedings of Machine Learning Research, pp. 29949â€“29973. PMLR, 21â€“27 Jul 2024. URLhttps://proceedings.mlr.press/v235/lin24e.html. Karl LÃ¶wner. Ãœber monotone matrixfunktionen.Mathematische Zeitschrift, 38(1):177â€“216, 1934. doi:10.1007/BF01170633. HÃ  Quang Minh and Vittorio Murino. Covariances in computer vision and machine learning.Synthesis Lectures on Computer Vision, 7(4):1â€“170, 2017. ISSN 2153-1056. doi:10.1007/978-3-031-01820-6. Depen Morwani, Itai Shapira, Nikhil Vyas, Eran Malach, Sham M Kakade, and Lucas Janson. A new perspective on Shampooâ€™s preconditioner. InThe Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=c6zI3Cp8c6. Yurii Nesterov et al.Lectures on convex optimization, volume 137. Springer Cham, 2018. ISBN 9783319915784. doi:10.1007/978-3-319-91578-4. Michael A Nielsen and Isaac L Chuang.Quantum computation and quantum information. Cambridge university press, 2010. Jorge Nocedal and Stephen J Wright.Numerical optimization. Springer, 2006. ISBN 978-0-387-40065-5. doi:10.1007/978-0-387-40065-5. Richard Nock, Brice Magdalou, Eric Briys, and Frank Nielsen. Mining matrix data with bregman matrix divergences for portfolio selection. InMatrix Information Geometry, pp. 373â€“402. Springer, 2012. Neal Parikh and Stephen Boyd. Proximal algorithms.Foundations and trends in Optimization, 1(3):127â€“239, 2014. doi:10.1561/2400000003. Xavier Pennec, Pierre Fillard, and Nicholas Ayache. A Riemannian framework for tensor computing.International Journal of Computer Vision, 66(1):41â€“66, Jan 2006. ISSN 1573-1405. doi:10.1007/s11263-005-3222-z. Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost. InProceedings of the 35th International Conference on Machine Learning, pp. 4596â€“4604. PMLR, 2018. URL https://proceedings. mlr.press/v80/shazeer18a.html. Hao-Jun Michael Shi, Tsung-Hsien Lee, Shintaro Iwasaki, Jose Gallego-Posada, Zhijing Li, Kaushik Rangadu- rai, Dheevatsa Mudigere, and Michael Rabbat. A distributed data-parallel PyTorch implementation of the distributed Shampoo optimizer for training neural networks at-scale.arXiv preprint arXiv:2309.06497, 2023. doi:10.48550/arxiv.2309.06497. Suvrit Sra. Positive definite matrices and the s-divergence.Proceedings of the American Mathematical Society, 144 (7):2787â€“2797, 2016. Tijmen Tieleman and Geoffrey Hinton. RMSProp: Divide the gradient by a running average of its recent magnitude. Coursera, 2012. Koji Tsuda, Gunnar RÃ¤tsch, and Manfred K Warmuth. Matrix exponentiated gradient updates for on-line learning and Bregman projection.Journal of Machine Learning Research, 6(34):995â€“1018, 2005. URL https://jmlr. org/papers/v6/tsuda05a.html. C. F. Van",
    "2016. Tijmen Tieleman and Geoffrey Hinton. RMSProp: Divide the gradient by a running average of its recent magnitude. Coursera, 2012. Koji Tsuda, Gunnar RÃ¤tsch, and Manfred K Warmuth. Matrix exponentiated gradient updates for on-line learning and Bregman projection.Journal of Machine Learning Research, 6(34):995â€“1018, 2005. URL https://jmlr. org/papers/v6/tsuda05a.html. C. F. Van Loan and N. Pitsianis.Approximation with Kronecker Products, pp. 293â€“314. Springer Netherlands, Dordrecht, 1993. ISBN 978-94-015-8196-7. doi:10.1007/978-94-015-8196-7_17. 12 KL-Shampoo and KL-SOAP Wu Lin, et al. (2025) Nikhil Vyas, Depen Morwani, Rosie Zhao, Itai Shapira, David Brandfonbrener, Lucas Janson, and Sham M Kakade. SOAP: Improving and stabilizing Shampoo using Adam for language modeling. InThe Thirteenth International Conference on Learning Representations, 2025a. URLhttps://openreview.net/forum?id=IDxZhXrpNf. Nikhil Vyas, Rosie Zhao, Depen Morwani, Mujin Kwun, and Sham Kakade. Improving SOAP using iterative whitening and Muon.https://nikhilvyas.github.io/SOAP_Muon.pdf, 2025b. Steven H Waldrip and Robert K Niven. Maximum entropy derivation of quasi-Newton methods.SIAM Journal on Optimization, 26(4):2495â€“2511, 2016. doi:10.1137/15M1027668. Cameron R. Wolfe. An extension of the NanoGPT repository for training small MOE models. https://github. com/wolfecameron/nanoMoE, 2025. Accessed: 2025-06. Shuo Xie, Tianhao Wang, Sashank J Reddi, Sanjiv Kumar, and Zhiyuan Li. Structured preconditioners in adaptive optimization: A unified analysis. InForty-second International Conference on Machine Learning, 2025. URL https://openreview.net/forum?id=GzS6b5Xvvu. 13 KL-Shampoo and KL-SOAP Wu Lin, et al. (2025) Appendices A Proof of Claim 1 We will show that the optimal solution of KL minimization minSğ‘KL\u0000E[ggâŠ¤],S\u0001with a one-sided preconditioner S=( 1/ğ‘‘ğ‘Sğ‘)âŠ—Iğ‘isSâˆ— ğ‘=E[GGâŠ¤]. By definition in Eq. (3) and substitutingS=( 1/ğ‘‘ğ‘Sğ‘)âŠ—Iğ‘, we can simplify the objective function as KL\u0000E[ggâŠ¤],S\u0001 =1 2\u0000log det(S)+Tr(Sâˆ’1E[ggâŠ¤])\u0001+const. =1 2\u0000ğ‘‘ğ‘log det(1 ğ‘‘ğ‘Sğ‘)+Tr(Sâˆ’1E[ggâŠ¤])\u0001+const.(Kronecker identity for matrix det.) =1 2\u0000ğ‘‘ğ‘log det(S ğ‘)+Tr(Sâˆ’1E[ggâŠ¤])\u0001+const.(identity for a log-determinant) =1 2\u0000ğ‘‘ğ‘log det(S ğ‘)+E[Tr(Sâˆ’1ggâŠ¤)]\u0001+const.(linearity of the expectation) =1 2\u0000ğ‘‘ğ‘log det(S ğ‘)+E[Tr(ğ‘‘ ğ‘Sâˆ’1 ğ‘GIğ‘GâŠ¤)]\u0001+const.(identity for a Kronecker vector product) =ğ‘‘ğ‘ 2\u0000log det(S ğ‘)+E[Tr(Sâˆ’1 ğ‘GGâŠ¤)]\u0001+const. =ğ‘‘ğ‘ 2\u0000âˆ’log det(P ğ‘)+E[Tr(P ğ‘GGâŠ¤)]\u0001+const.,(8) whereG=Mat(g)andP ğ‘:=Sâˆ’1 ğ‘. If we achieve the optimal solution, the gradient stationary condition must be satisfied regardless of the gradient with respect toS ğ‘orSâˆ’1 ğ‘â‰¡Pğ‘, such as 0=ğœ•Sâˆ’1ğ‘KL\u0000E[ggâŠ¤],S\u0001 =ğœ•Pğ‘KL\u0000E[ggâŠ¤],S\u0001 =ğ‘‘ğ‘ 2\u0000âˆ’Pâˆ’1 ğ‘+E[GGâŠ¤]\u0001 (use Eq. (8) and matrix calculus identities) =ğ‘‘ğ‘ 2\u0000âˆ’Sğ‘+E[GGâŠ¤]\u0001. Notice that the KL divergence is unbounded above. Thus, the optimal (minimal) solution exists. It must be Sâˆ— ğ‘=E[GGâŠ¤]to satisfy this stationary condition. B Proof of Claim 2 We will show that the optimal solution of KL minimization minSğ‘,Sğ‘KL\u0000E[ggâŠ¤],S\u0001with a two-sided precondi- tionerS=S ğ‘âŠ—Sğ‘should satisfy this condition:Sâˆ— ğ‘=1 ğ‘‘ğ‘E[G\u0000Sâˆ— ğ‘\u0001âˆ’1GâŠ¤]andSâˆ— ğ‘=1 ğ‘‘ğ‘E[GâŠ¤\u0000Sâˆ— ğ‘\u0001âˆ’1G]. Similar to the proof of Claim 1 in Sec. A, we can simplify the objective function as KL\u0000E[ggâŠ¤],S\u0001 =1 2\u0000log det(S)+E[Tr(Sâˆ’1ggâŠ¤)]\u0001+const. =1 2\u0000ğ‘‘ğ‘log det(S ğ‘)+ğ‘‘ğ‘log det(S ğ‘)+E[Tr(Sâˆ’1ggâŠ¤)]\u0001+const.(identity for a log-determinant) =1 2\u0000ğ‘‘ğ‘log det(S ğ‘)+ğ‘‘ğ‘log det(S ğ‘)+E[Tr(Sâˆ’1 ğ‘GSâˆ’1 ğ‘GâŠ¤)]\u0001+const.(identity for a Kronecker-vector-product) =1 2\u0000âˆ’ğ‘‘ğ‘log det(P ğ‘)âˆ’ğ‘‘ğ‘log det(P ğ‘)+E[Tr(P ğ‘GPğ‘GâŠ¤)]\u0001+const.,(9) wherePğ‘˜:=Sâˆ’1 ğ‘˜forğ‘˜âˆˆ{ğ‘,ğ‘}. The optimal solution must satisfy the gradient stationarity condition with respect to {Sğ‘,Sğ‘}. Notice that the gradient with respect to {Sâˆ’1 ğ‘,Sâˆ’1 ğ‘}can be expressed in terms of the gradient with respect to {Sğ‘,Sğ‘}as 14 KL-Shampoo and KL-SOAP Wu Lin, et al. (2025) ğœ•Sâˆ’1ğ‘KL=âˆ’Sğ‘\u0000ğœ•Sğ‘KL\u0001Sğ‘andğœ•Sâˆ’1 ğ‘KL=âˆ’Sğ‘\u0000ğœ•Sğ‘KL\u0001Sğ‘. Thus, the optimal solution must satisfy the following gradient stationary condition with respect to{Sâˆ’1 ğ‘,Sâˆ’1 ğ‘}: 0=ğœ•Sâˆ’1ğ‘KL\u0000E[ggâŠ¤],S\u0001,0=ğœ•Sâˆ’1 ğ‘KL\u0000E[ggâŠ¤],S\u0001. Using Eq. (9) and simplifying the left expression 0=ğœ•Sâˆ’1ğ‘KL\u0000E[ggâŠ¤],S\u0001 =ğœ•Pğ‘KL\u0000E[ggâŠ¤],S\u0001 =1 2\u0000âˆ’ğ‘‘ğ‘Pâˆ’1 ğ‘+E[GPğ‘GâŠ¤]\u0001(10) gives us",
    "the gradient with respect to {Sğ‘,Sğ‘}as 14 KL-Shampoo and KL-SOAP Wu Lin, et al. (2025) ğœ•Sâˆ’1ğ‘KL=âˆ’Sğ‘\u0000ğœ•Sğ‘KL\u0001Sğ‘andğœ•Sâˆ’1 ğ‘KL=âˆ’Sğ‘\u0000ğœ•Sğ‘KL\u0001Sğ‘. Thus, the optimal solution must satisfy the following gradient stationary condition with respect to{Sâˆ’1 ğ‘,Sâˆ’1 ğ‘}: 0=ğœ•Sâˆ’1ğ‘KL\u0000E[ggâŠ¤],S\u0001,0=ğœ•Sâˆ’1 ğ‘KL\u0000E[ggâŠ¤],S\u0001. Using Eq. (9) and simplifying the left expression 0=ğœ•Sâˆ’1ğ‘KL\u0000E[ggâŠ¤],S\u0001 =ğœ•Pğ‘KL\u0000E[ggâŠ¤],S\u0001 =1 2\u0000âˆ’ğ‘‘ğ‘Pâˆ’1 ğ‘+E[GPğ‘GâŠ¤]\u0001(10) gives us this equation 0=1 2(âˆ’ğ‘‘ğ‘Sâˆ— ğ‘+E[G\u0000Sâˆ— ğ‘\u0001âˆ’1GâŠ¤]) that the optimal solution must satisfy. This naturally leads to the following expression: Sâˆ— ğ‘=1 ğ‘‘ğ‘E[G\u0000Sâˆ— ğ‘\u0001âˆ’1GâŠ¤]. Likewise, we can obtain the following expression by simplifying the right expression of the gradient stationary condition. Sâˆ— ğ‘=1 ğ‘‘ğ‘E[GâŠ¤\u0000Sâˆ— ğ‘\u0001âˆ’1G]. C Proof of Claim 3 To simplify the notation, we define H:=E[ggâŠ¤], and re-express the objective function in the KL minimzation problem asL(S) :=KL( E[ggâŠ¤],S)=KL(H,S) . We now introduce the proximal-gradient framework (Parikh & Boyd, 2014; Khan et al., 2016) to formally state and prove Claim 3. We assume that an estimated S(ğ‘¡)is given at iterationğ‘¡. We use a non-negative function, ğ‘“(S(ğ‘¡),S(ğ‘¡+1)), to measure the closeness between the current and the next iteration. Function ğ‘“(Â·,Â·) is known as a proximal function. A (unconstrained) proximal-gradient step at iterationğ‘¡+1with a given proximal function,ğ‘“(Â·,Â·), is defined as the optimal solution of another minimization problem, S(ğ‘¡+1):=arg min XâŸ¨âˆ‡SL S=S(ğ‘¡),XâŸ©+1 ğ›½2ğ‘“(S(ğ‘¡),X), at every iteration with step-sizeğ›½ 2based on the linearization of the objective functionL. We consider a weighted quadratic function as the proximal function. ğ‘“(S(ğ‘¡),X):=1 2âˆ¥Xâˆ’S(ğ‘¡)âˆ¥2 W=1 2vec\u0000Xâˆ’S(ğ‘¡)\u0001âŠ¤Wvec\u0000Xâˆ’S(ğ‘¡)\u0001 where Wis a given weight matrix. For example, Wcan be the Hessian of the KL divergence W:=âˆ‡2 YKL(S(ğ‘¡),Y) Y=S(ğ‘¡)= âˆ’1 2\u0000ğœ•Sâˆ’1 ğœ•S\u0001 S=S(ğ‘¡). This matrix is also known as the Fisher-Rao Riemannian metric for a zero-mean Gaussian (Amari, 2016). Note that this proximal function has been used in the quasi-Newton literature (Nocedal & Wright, 2006). Indeed, we can show that this proximal function is exactly a second-order Taylor approximation of the KL divergence,KL(S(ğ‘¡),X), atX=S(ğ‘¡). When S=Sğ‘âŠ—Sğ‘admits a Kronecerk product, we can specify this weight matrix Wso that this proximal function can be separated into two terms: 1 2âˆ¥Xğ‘âŠ—Xğ‘âˆ’S(ğ‘¡)âˆ¥2 W=1 2âˆ¥Xğ‘âŠ—Xğ‘âˆ’S(ğ‘¡) ğ‘âŠ—S(ğ‘¡) ğ‘âˆ¥2 W =1 2âˆ¥Xğ‘âˆ’S(ğ‘¡) ğ‘âˆ¥2 Wğ‘+1 2âˆ¥Xğ‘âˆ’S(ğ‘¡) ğ‘âˆ¥2 Wğ‘ 15 KL-Shampoo and KL-SOAP Wu Lin, et al. (2025) Here, we define the weight matrix as the block-diagonal Hessian of the KL divergence, such as W:=\u0014Wğ‘ 0 0Wğ‘\u0015 by setting the cross-block terms highlighted in red to zero, where Wğ‘˜:=ğœ•2 Yğ‘˜KL(S(ğ‘¡),Yğ‘âŠ—Yğ‘) Y=S(ğ‘¡) ğ‘âŠ—S(ğ‘¡) ğ‘for ğ‘˜âˆˆ{ğ‘,ğ‘} . We can show that this weight matrix is exactly the block-diagonal approximation of the Fisher-Rao matrix for a zero-mean matrix Gaussian considered by Lin et al. (2019, 2024). Now, we can formally state the claim and provide proof of it. Claim 3. (formal version)The moving average scheme for S:=Sğ‘âŠ—Sğ‘in idealized KL-Shampoo is a proximal-gradient step at iterationğ‘¡+1, S(ğ‘¡+1) ğ‘,S(ğ‘¡+1) ğ‘:=arg min Xğ‘,Xğ‘âŸ¨âˆ‡Sğ‘L S=S(ğ‘¡),Xğ‘âŸ©+âŸ¨âˆ‡ Sğ‘L S=S(ğ‘¡),Xğ‘âŸ©+1 2ğ›½2âˆ¥Xğ‘âŠ—Xğ‘âˆ’S(ğ‘¡)âˆ¥2 W, â‡â‡’S(ğ‘¡+1) ğ‘=(1âˆ’ğ›½ 2)S(ğ‘¡) ğ‘+ğ›½2E[G\u0000S(ğ‘¡) ğ‘\u0001âˆ’1GâŠ¤],S(ğ‘¡+1) ğ‘=(1âˆ’ğ›½ 2)S(ğ‘¡) ğ‘+ğ›½2E[GâŠ¤\u0000S(ğ‘¡) ğ‘\u0001âˆ’1G] with step-size ğ›½2to solve the KL minimization problem in Eq. (3), if we use a proximal function using the weight matrix,W, defined above. In mini-batch cases, we approximate the expectations using a current batch gradient (Morwani et al., 2025) (see Eq. (5)), which leads to a stochastic proximal-gradient step. Proof:Because the weight matrix",
    "solve the KL minimization problem in Eq. (3), if we use a proximal function using the weight matrix,W, defined above. In mini-batch cases, we approximate the expectations using a current batch gradient (Morwani et al., 2025) (see Eq. (5)), which leads to a stochastic proximal-gradient step. Proof:Because the weight matrix is block-diagonal, we can slice this objective function for the proximal step into two terms. âŸ¨âˆ‡Sğ‘L S=S(ğ‘¡),Xğ‘âŸ©+âŸ¨âˆ‡ Sğ‘L S=S(ğ‘¡),Xğ‘âŸ©+1 2ğ›½2âˆ¥Xğ‘âŠ—Xğ‘âˆ’S(ğ‘¡)âˆ¥2 W =âŸ¨âˆ‡Sğ‘L S=S(ğ‘¡),Xğ‘âŸ©+1 2ğ›½2âˆ¥Xğ‘âˆ’S(ğ‘¡) ğ‘âˆ¥2 Wğ‘ | {z } (blockX ğ‘)+âŸ¨âˆ‡Sğ‘L S=S(ğ‘¡),Xğ‘âŸ©+1 2ğ›½2âˆ¥Xğ‘âˆ’S(ğ‘¡) ğ‘âˆ¥2 Wğ‘ | {z } (blockX ğ‘) Importantly, Wğ‘andWğ‘are independent of Xğ‘andXğ‘. Thus, we solve this objective by independently for each Xğ‘˜forğ‘˜âˆˆ{ğ‘,ğ‘}. We now show that solving this proximal problem gives rise to the estimation rule for S(ğ‘¡+1) ğ‘ at iterationğ‘¡+1. We focus on the first term since the second term does not depend on Xğ‘. We can show that Wğ‘can be expressed asWğ‘=ğœ•2 Yğ‘KL(S(ğ‘¡),Yğ‘âŠ—Yğ‘) Y=S(ğ‘¡) ğ‘âŠ—S(ğ‘¡) ğ‘=âˆ’ğ‘‘ğ‘ 2\u0000ğœ•Sâˆ’1 ğ‘ ğœ•Sğ‘\u0001 S=S(ğ‘¡). This matrix Wğ‘is also considered in Lin et al. (2024). Importantly, Wğ‘is invertible and Wâˆ’1 ğ‘=âˆ’2 ğ‘‘ğ‘\u0000ğœ•Sğ‘ ğœ•Sâˆ’1ğ‘\u0001 S=S(ğ‘¡)With this result, the optimal solution of Xğ‘ must satisfy this stationary condition 0=ğœ•Xğ‘\u0000âŸ¨âˆ‡Sğ‘L S=S(ğ‘¡),Xğ‘âŸ©+1 2ğ›½2âˆ¥Xğ‘âˆ’S(ğ‘¡) ğ‘âˆ¥2 Wğ‘\u0001 =âˆ‡Sğ‘L S=S(ğ‘¡)+1 ğ›½2Wğ‘(Xğ‘âˆ’S(ğ‘¡) ğ‘) â‡â‡’Xğ‘=S(ğ‘¡) ğ‘âˆ’ğ›½2Wâˆ’1 ğ‘âˆ‡Sğ‘L S=S(ğ‘¡) It is easy to see that the optimal solution of the proximal step is S(ğ‘¡+1) ğ‘ :=Xâˆ— ğ‘=S(ğ‘¡) ğ‘âˆ’ğ›½2Wâˆ’1 ğ‘âˆ‡Sğ‘L S=S(ğ‘¡) =S(ğ‘¡) ğ‘âˆ’ğ›½2\u0000âˆ’2 ğ‘‘ğ‘(ğœ•Sğ‘ ğœ•Sâˆ’1ğ‘ S=S(ğ‘¡) ğ‘)\u0001 | {z } =Wâˆ’1ğ‘âˆ‡Sğ‘L S=S(ğ‘¡) =S(ğ‘¡) ğ‘+2ğ›½2 ğ‘‘ğ‘âˆ‡Sâˆ’1ğ‘L S=S(ğ‘¡)(use the chain rule and utlize the Jacobian matrix contained inWâˆ’1 ğ‘) =S(ğ‘¡) ğ‘+2ğ›½2 ğ‘‘ğ‘\u00001 2(âˆ’ğ‘‘ğ‘S(ğ‘¡) ğ‘+E[G\u0000S(ğ‘¡) ğ‘\u0001âˆ’1GâŠ¤])\u0001 | {z } =âˆ‡Sâˆ’1ğ‘L S=S(ğ‘¡)(recall the definition ofLand use Eq. (10)) =(1âˆ’ğ›½ 2)S(ğ‘¡) ğ‘+ğ›½2 ğ‘‘ğ‘E[G\u0000S(ğ‘¡) ğ‘\u0001âˆ’1GâŠ¤], 16 KL-Shampoo and KL-SOAP Wu Lin, et al. (2025) which is exactly the moving average scheme in Eq. (5) for updatingS ğ‘at iterationğ‘¡+1. Likewise, we can obtain the moving average scheme forS ğ‘.â–¡ D Proof of Claim 4 We will show that the optimal solution of KL minimization minÎ»ğ‘,Î»ğ‘KL\u0000E[ggâŠ¤],S\u0001with a two-sided precondi- tioner S=(Qğ‘Diag(Î»ğ‘)QâŠ¤ ğ‘)âŠ—(Qğ‘Diag(Î»ğ‘)QâŠ¤ ğ‘)should satisfy this condition: Î»âˆ— ğ‘=1 ğ‘‘ğ‘diag\u0000QâŠ¤ ğ‘E[GPâˆ— ğ‘GâŠ¤]Qğ‘\u0001 andÎ»âˆ— ğ‘=1 ğ‘‘ğ‘diag\u0000QâŠ¤ ğ‘E[GâŠ¤Pâˆ— ğ‘G]Qğ‘\u0001, where Pâˆ— ğ‘˜:=Qğ‘˜Diag\u0010 (Î»âˆ— ğ‘˜)âŠ™âˆ’1\u0011 QâŠ¤ ğ‘˜, andQğ‘˜is known and precomputed byQRforğ‘˜âˆˆ{ğ‘,ğ‘}. LetSğ‘˜:=Qğ‘˜Diag(Î»ğ‘˜)QâŠ¤ ğ‘˜forğ‘˜âˆˆ{ğ‘,ğ‘} . Because Qğ‘˜is orthogonal, it is easy to see that Sâˆ’1 ğ‘˜:=Qğ‘˜Diag(\u0000Î»ğ‘˜\u0001âŠ™âˆ’1)QâŠ¤ ğ‘˜. Similar to the proof of Claim 2 in Sec. B, we can simplify the following objective function by substituting Sğ‘and Sğ‘. Here, we also utilize the orthogonality ofQ ğ‘˜forğ‘˜âˆˆ{ğ‘,ğ‘}. KL\u0000E[ggâŠ¤],S\u0001 =1 2\u0000ğ‘‘ğ‘log det(S ğ‘)+ğ‘‘ğ‘log det(S ğ‘)+E[Tr(Sâˆ’1 ğ‘GSâˆ’1 ğ‘GâŠ¤)]\u0001+const. =1 2\u0000ğ‘‘ğ‘log det(Q ğ‘Diag(Î»ğ‘)QâŠ¤ ğ‘)+ğ‘‘ğ‘log det(Q ğ‘Diag(Î»ğ‘)QâŠ¤ ğ‘)+E[Tr(Sâˆ’1 ğ‘GSâˆ’1 ğ‘GâŠ¤)]\u0001+const. =1 2\u0000(ğ‘‘ğ‘âˆ‘ï¸ ğ‘–log(ğœ†(ğ‘–) ğ‘))+(ğ‘‘ğ‘âˆ‘ï¸ ğ‘—log(ğœ†(ğ‘—) ğ‘))+E[Tr(Sâˆ’1 ğ‘GSâˆ’1 ğ‘GâŠ¤)]\u0001+const.(use the orthogonality ofQ ğ‘andQğ‘) =1 2\u0000(ğ‘‘ğ‘âˆ‘ï¸ ğ‘–log(ğœ†(ğ‘–) ğ‘))+(ğ‘‘ğ‘âˆ‘ï¸ ğ‘—log(ğœ†(ğ‘—) ğ‘))+E[Tr(Q ğ‘Diag(Î»âŠ™âˆ’1 ğ‘)QâŠ¤ ğ‘| {z } =Sâˆ’1ğ‘GQğ‘Diag(Î»âŠ™âˆ’1 ğ‘)QâŠ¤ ğ‘| {z } =Sâˆ’1 ğ‘GâŠ¤)]\u0001+const. (11) The optimalÎ» ğ‘andÎ»ğ‘should satisfy the gradient stationary condition. 0=ğœ•Î»ğ‘KL\u0000E[ggâŠ¤],S\u0001 =1 2\u0000ğ‘‘ğ‘Î»âŠ™âˆ’1 ğ‘+ğœ•Î»ğ‘E[Tr(Qğ‘Diag(Î»âŠ™âˆ’1 ğ‘)QâŠ¤ ğ‘G=Pğ‘z }| { Qğ‘Diag(Î»âŠ™âˆ’1 ğ‘)QâŠ¤ ğ‘GâŠ¤)]\u0001 (use Eq. (11)) =1 2\u0000ğ‘‘ğ‘Î»âŠ™âˆ’1 ğ‘+ğœ•Î»ğ‘E[Tr(Diag(Î»âŠ™âˆ’1 ğ‘)QâŠ¤ ğ‘GPğ‘GâŠ¤Qğ‘)]\u0001 =1 2\u0000ğ‘‘ğ‘Î»âŠ™âˆ’1 ğ‘+ğœ•Î»ğ‘E[Î»âŠ™âˆ’1 ğ‘âŠ™diag\u0000QâŠ¤ ğ‘GPğ‘GâŠ¤Qğ‘\u0001]\u0001 (utilize the trace and the diagonal structure) =1 2\u0000ğ‘‘ğ‘Î»âŠ™âˆ’1 ğ‘âˆ’E[Î»âŠ™âˆ’2 ğ‘âŠ™diag\u0000QâŠ¤ ğ‘GPğ‘GâŠ¤Qğ‘\u0001]\u0001 =1 2\u0000ğ‘‘ğ‘Î»âŠ™âˆ’1 ğ‘âˆ’Î»âŠ™âˆ’2 ğ‘âŠ™diag\u0000QâŠ¤ ğ‘E[GPğ‘GâŠ¤]Qğ‘\u0001\u0001 â‡â‡’0=ğ‘‘ ğ‘Î»ğ‘âˆ’diag\u0000QâŠ¤ ğ‘E[GPğ‘GâŠ¤]Qğ‘\u0001\u0001 We obtain the optimal solution by solving this equation. Î»âˆ— ğ‘=1 ğ‘‘ğ‘diag\u0000QâŠ¤ ğ‘E[GPâˆ— ğ‘GâŠ¤]Qğ‘\u0001\u0001 Similarly, we can",
    "(use Eq. (11)) =1 2\u0000ğ‘‘ğ‘Î»âŠ™âˆ’1 ğ‘+ğœ•Î»ğ‘E[Tr(Diag(Î»âŠ™âˆ’1 ğ‘)QâŠ¤ ğ‘GPğ‘GâŠ¤Qğ‘)]\u0001 =1 2\u0000ğ‘‘ğ‘Î»âŠ™âˆ’1 ğ‘+ğœ•Î»ğ‘E[Î»âŠ™âˆ’1 ğ‘âŠ™diag\u0000QâŠ¤ ğ‘GPğ‘GâŠ¤Qğ‘\u0001]\u0001 (utilize the trace and the diagonal structure) =1 2\u0000ğ‘‘ğ‘Î»âŠ™âˆ’1 ğ‘âˆ’E[Î»âŠ™âˆ’2 ğ‘âŠ™diag\u0000QâŠ¤ ğ‘GPğ‘GâŠ¤Qğ‘\u0001]\u0001 =1 2\u0000ğ‘‘ğ‘Î»âŠ™âˆ’1 ğ‘âˆ’Î»âŠ™âˆ’2 ğ‘âŠ™diag\u0000QâŠ¤ ğ‘E[GPğ‘GâŠ¤]Qğ‘\u0001\u0001 â‡â‡’0=ğ‘‘ ğ‘Î»ğ‘âˆ’diag\u0000QâŠ¤ ğ‘E[GPğ‘GâŠ¤]Qğ‘\u0001\u0001 We obtain the optimal solution by solving this equation. Î»âˆ— ğ‘=1 ğ‘‘ğ‘diag\u0000QâŠ¤ ğ‘E[GPâˆ— ğ‘GâŠ¤]Qğ‘\u0001\u0001 Similarly, we can obtain the other expression. E Proof of Claim 5 This proof is similar to the proof of Claim 4 in Sec. D. We will show that the optimal solution of KL minimization mindKL\u0000E[ggâŠ¤],S\u0001with an augmented preconditioner S=(QDiag(d)QâŠ¤)isdâˆ—=Eh\u0000vec(QâŠ¤ ğ‘GQğ‘)\u0001âŠ™2i , where dâˆˆRğ‘‘ğ‘ğ‘‘ğ‘Ã—1is an augmented eigenvalue vector, Q:=Qğ‘âŠ—Qğ‘, andQğ‘˜is given and precomputed by QR forğ‘˜âˆˆ{ğ‘,ğ‘}. 17 KL-Shampoo and KL-SOAP Wu Lin, et al. (2025) We can simplify the objective function by substituting S. Here, we also utilize the orthogonality of Qğ‘˜for ğ‘˜âˆˆ{ğ‘,ğ‘}. KL\u0000E[ggâŠ¤],S\u0001 =1 2\u0000log det(QDiag(d)QâŠ¤)+Tr(QDiag(dâŠ™âˆ’1)QâŠ¤E[ggâŠ¤])\u0001+const. =1 2\u0000âˆ‘ï¸ ğ‘–log(ğ‘‘ğ‘–))+Tr(QDiag(dâŠ™âˆ’1)QâŠ¤E[ggâŠ¤])\u0001+const.(Q=Q ğ‘âŠ—Qğ‘is orthogonal) =1 2\u0000âˆ‘ï¸ ğ‘–log(ğ‘‘ğ‘–))+E\u0002 Tr(QDiag(dâŠ™âˆ’1)QâŠ¤ggâŠ¤)\u0001\u0003 +const.(linearity of the expectation) =1 2\u0000âˆ‘ï¸ ğ‘–log(ğ‘‘ğ‘–))+E\u0002 Tr((vec(QâŠ¤ ğ‘GQğ‘))âŠ¤Diag(dâŠ™âˆ’1)vec(QâŠ¤ ğ‘GQğ‘)\u0003 +const.(identity of Kronecker-vector product) =1 2\u0000âˆ‘ï¸ ğ‘–log(ğ‘‘ğ‘–))+E\u0002 sum(dâŠ™âˆ’1âŠ™(vec(QâŠ¤ ğ‘GQğ‘))âŠ™2\u0003 +const.(leverage trace and diagonal struct.)(12) The optimaldshould satisfy the gradient stationary condition. 0=ğœ•dKL\u0000E[ggâŠ¤],S\u0001 =1 2\u0000dâŠ™âˆ’1âˆ’E\u0002 dâŠ™âˆ’2âŠ™vec(QâŠ¤ ğ‘GQğ‘)âŠ™2\u0001\u0003\u0001 (use Eq. (12) and compute its derivative) â‡â‡’0=1 2\u0000dâˆ’E\u0002 vec(QâŠ¤ ğ‘GQğ‘)âŠ™2\u0001\u0003\u0001 Notice that the KL divergence is unbounded above. Thus, the optimal (minimal) solution exists and it must be dâˆ—=E\u0002 vec(QâŠ¤ ğ‘GQğ‘)âŠ™2\u0003 to satisfy the condition. F Two-sided Shampoo Scheme based on Frobenius norm Claim 6 (Shampooâ€™s estimation scheme based on Frobenius norm)The optimal solution of the Frobenius norm minimization minSğ‘,Sğ‘Frob\u0000E[ggâŠ¤],S\u0001:=âˆ¥E[ggâŠ¤]âˆ’Sâˆ¥ Frobwith a two-sided precontioner S=Sğ‘âŠ—Sğ‘should satisfy the following condition. Sâˆ— ğ‘=1 Tr((Sâˆ— ğ‘)2)E[GSâˆ— ğ‘GâŠ¤],Sâˆ— ğ‘=1 Tr((Sâˆ—ğ‘)2)E[GâŠ¤Sâˆ— ğ‘G],(13) Remark:Although the solution can be obtained via rank-1 singular value decomposition (SVD) (Van Loan & Pitsianis, 1993) on this outer product, E[ggâŠ¤], it can be computationally expensive to compute the solution due to the high dimensionality of the product. Moreover, the optimal solution is only achievable when the expectation of the outer product is computed exactly. Obtaining the optimal solution using SVD is even more expensive in tensor-valued cases. Proof:To simplify the proof, we will consider the square of the objective function, as the optimal solution remains unchanged. We simplify the square of the objective function by substituting S. Here, we utilize the definition of the norm and re-express the norm using the matrix trace. âˆ¥E[ggâŠ¤]âˆ’Sğ‘âŠ—Sğ‘âˆ¥2 Frob =Tr\u0000(E[ggâŠ¤]âˆ’Sğ‘âŠ—Sğ‘)âŠ¤(E[ggâŠ¤]âˆ’Sğ‘âŠ—Sğ‘)\u0001 (an equivalent definition of the square of the norm) =Tr\u0000S2 ğ‘âŠ—S2 ğ‘âˆ’2E[ggâŠ¤](Sğ‘âŠ—Sğ‘)\u0001+const.(S ğ‘˜is symmetric forğ‘˜âˆˆ{ğ‘,ğ‘}) =Tr\u0000S2 ğ‘\u0001Tr\u0000S2 ğ‘\u0001âˆ’2Tr\u0000E[ggâŠ¤](Sğ‘âŠ—Sğ‘)\u0001+const.(Property of a Kronecker product) =Tr\u0000S2 ğ‘\u0001Tr\u0000S2 ğ‘\u0001âˆ’2E\u0002 Tr\u0000(ggâŠ¤)(Sğ‘âŠ—Sğ‘)\u0001\u0003 +const.(linearity of the expectation) =Tr\u0000S2 ğ‘\u0001Tr\u0000S2 ğ‘\u0001âˆ’2E\u0002 Tr\u0000gâŠ¤vec(Sğ‘GSğ‘)\u0001\u0003 +const.(Property of a Kronecker product) =Tr\u0000S2 ğ‘\u0001Tr\u0000S2 ğ‘\u0001âˆ’2E\u0002 Tr\u0000GâŠ¤Sğ‘GSğ‘\u0001\u0003 +const.(Property of a trace) 18 KL-Shampoo and KL-SOAP Wu Lin, et al. (2025) Idealized F-Shampoo: two-sided Shampoo based on Frobenius norm (ğ‘= 1/2) 1:Gradient Computationg:=âˆ‡â„“(Î¸) G:=Mat(g)âˆˆRğ‘‘ğ‘Ã—ğ‘‘ğ‘ 2:Covariance Estimation (each iter)\u0012Sğ‘ Sğ‘\u0013 â†(1âˆ’ğ›½ 2)\u0012Sğ‘ Sğ‘\u0013 +ğ›½2\u0012Î”ğ‘ Î”ğ‘\u0013 Î”ğ‘:=( GSğ‘GâŠ¤/Tr(S2 ğ‘)(v1) GQğ‘Diag(Î»ğ‘)QâŠ¤ ğ‘GâŠ¤/Ã(Î»2 ğ‘)(v2) Î”ğ‘:=( GâŠ¤Sğ‘G/Tr(S2 ğ‘)(v1) GâŠ¤Qğ‘Diag(Î»ğ‘)QâŠ¤ ğ‘G/Ã(Î»2 ğ‘)(v2) 3:Eigendecomposition (everyğ‘‡â‰¥1iters) Î»ğ‘˜,Qğ‘˜â†eig(Sğ‘˜)forğ‘˜âˆˆ{ğ‘,ğ‘} 4:Preconditioning usingQ:=Q ğ‘âŠ—Qğ‘ Î¸â†Î¸âˆ’ğ›¾(QDiag(Î» ğ‘âŠ—Î»ğ‘)âˆ’1/2QâŠ¤)gF-Shampoo: Replacing the slow eigen step with a more efficient QR step(replace Step 3) 3a:FrequentEigenvalue Estimation with EMA (each iter)\u0012Î»ğ‘ Î»ğ‘\u0013 â†(1âˆ’ğ›½ 2)\u0012Î»ğ‘ Î»ğ‘\u0013 +ğ›½2\u0012diag(QâŠ¤ ğ‘Î”ğ‘Qğ‘) diag(QâŠ¤ ğ‘Î”ğ‘Qğ‘)\u0013 3b: Infrequent Eigenbasis Estimation using QR (everyğ‘‡â‰¥1iters) Qğ‘˜â†qr(Sğ‘˜Qğ‘˜)forğ‘˜âˆˆ{ğ‘,ğ‘} Figure 6:Left:Simplified two-sided",
    "ğ‘)(v1) GâŠ¤Qğ‘Diag(Î»ğ‘)QâŠ¤ ğ‘G/Ã(Î»2 ğ‘)(v2) 3:Eigendecomposition (everyğ‘‡â‰¥1iters) Î»ğ‘˜,Qğ‘˜â†eig(Sğ‘˜)forğ‘˜âˆˆ{ğ‘,ğ‘} 4:Preconditioning usingQ:=Q ğ‘âŠ—Qğ‘ Î¸â†Î¸âˆ’ğ›¾(QDiag(Î» ğ‘âŠ—Î»ğ‘)âˆ’1/2QâŠ¤)gF-Shampoo: Replacing the slow eigen step with a more efficient QR step(replace Step 3) 3a:FrequentEigenvalue Estimation with EMA (each iter)\u0012Î»ğ‘ Î»ğ‘\u0013 â†(1âˆ’ğ›½ 2)\u0012Î»ğ‘ Î»ğ‘\u0013 +ğ›½2\u0012diag(QâŠ¤ ğ‘Î”ğ‘Qğ‘) diag(QâŠ¤ ğ‘Î”ğ‘Qğ‘)\u0013 3b: Infrequent Eigenbasis Estimation using QR (everyğ‘‡â‰¥1iters) Qğ‘˜â†qr(Sğ‘˜Qğ‘˜)forğ‘˜âˆˆ{ğ‘,ğ‘} Figure 6:Left:Simplified two-sided Shampoo schemes based on the Frobenius norm without momentum. We consider two variants. Variant 1 is inspired by Claim 6, while Variant 2 is similar to KL-Shampooâ€™s update scheme, which utilizes eigenvalues. Note that Variant 1 of the idealized F-Shampoo is known as the two-sided Shampoo in the literature (Morwani et al., 2025).Right:Adapting our exponential moving average (EMA) approach enables F-Shampoo to use the faster QR procedure and makes it more competitive, as empirically shown in Fig. 7. We can simplify the stationary condition with respect toS ğ‘as below. 0=ğœ•Sğ‘âˆ¥E[ggâŠ¤]âˆ’Sğ‘âŠ—Sğ‘âˆ¥2 Frob =ğœ•Sğ‘\u0000Tr\u0000S2 ğ‘\u0001Tr\u0000S2 ğ‘\u0001âˆ’2E\u0002 Tr\u0000GâŠ¤Sğ‘GSğ‘\u0001\u0003 +const.\u0001 =2\u0000Tr(S2 ğ‘)Sğ‘âˆ’E[GSğ‘GâŠ¤]\u0001 Thus, the optimal solution should satisfy this condition Sâˆ— ğ‘=1 Tr\u0000 (Sâˆ— ğ‘)2\u0001E[GSâˆ— ğ‘GâŠ¤]. Similarly, we can obtain the other condition. Morwani et al. (2025) also consider a similar condition (see Eq. 4 of their paper).â–¡ GKey Distinction between Shampoo with trace scaling and KL-Shampoo We will show that Shampooâ€™s estimation with trace scaling is a generalization of Adafactor. Our interpretation of Shampooâ€™s update is grounded in a generalization of the divergence used in Adafactorâ€”quantum relative entropy (Tsuda et al., 2005)â€”a Bregman divergence (Bregman, 1967) defined on the trace of the matrix logarithm. This new view of Shampooâ€™s estimation is distinct from the existing Frobenius-norm perspective. By contrast, KL-Shampooâ€™s update is based on the KL divergence (classical relative entropy)â€”another Bregman divergence, but one defined on the (scalar) logarithm of the matrix determinant. We now introduce the definition of a Bregman divergence (Bregman, 1967) to formally discuss the distinction between Shampoo with trace scaling and KL-Shampoo. Given a strictly convex and differentiable (scalar) function ğ¹(Â·), the Bregman divergence based on this function is defined as Bğ¹(X,Y):=ğ¹(X)âˆ’ğ¹(Y)âˆ’Tr\u0000[âˆ‡ğ¹(Y)](Xâˆ’Y)\u0001. As an example, the KL divergence (classical relative entropy) KL(X,Y) is a Bregman divergence with convex functionğ¹(M):=âˆ’1 2log det(M). Bğ¹(X,Y)=ğ¹(X)âˆ’ğ¹(Y)âˆ’Tr\u0000[âˆ‡ğ¹(Y)](Xâˆ’Y)\u0001 =1 2\u0000âˆ’log det(X)+log det(Y)+Tr(Yâˆ’1(Xâˆ’Y)\u0001 (defn. of functionğ¹(Â·)) =1 2\u0000log det(Y)âˆ’log det(X)+Tr(Yâˆ’1X)âˆ’dim(X)\u0001=KL(X,Y) whereâˆ‡ğ¹(M)=âˆ’1 2Mâˆ’1. The KL divergence is also known as the log-determinant divergence because function ğ¹ is defined as the logarithm of the matrix determinant. Notably, the Hessian of this ğ¹(Â·) gives rise to the Fisher-Rao metric, which is also known as the affine-invariant metric (up to a constant scalar) (Lin et al., 2023). 19 KL-Shampoo and KL-SOAP Wu Lin, et al. (2025) 3.54.04.55.0NanoGPT-123M-FineWeb1B 3.43.63.84.0Llama-134M-C4-2B 10 5 1 Iterations (thousands)3.253.303.35aa 10 5 1 Iterations (thousands)3.203.223.243.263.28Test Loss F-Shampoo (v2, QR, EMA) F-Shampoo (v1, QR, EMA) two-sided Shampoo Frob (eigen, No EMA) Shampoo (p=1 2, grafting) Figure 7: Empirical results from a random search with 150 runs per method on language models demonstrate that our exponential moving average (EMA) scheme for eigenvalue estimation, as described in Fig. 6, improves the performance of the two-sided Shampoo based on Frobenius norm (see Eq. 4 of Morwani et al. (2025) and Claim 6)â€”referred to as Variant 1 of idealized F-Shampoo. All these methods perform",
    "models demonstrate that our exponential moving average (EMA) scheme for eigenvalue estimation, as described in Fig. 6, improves the performance of the two-sided Shampoo based on Frobenius norm (see Eq. 4 of Morwani et al. (2025) and Claim 6)â€”referred to as Variant 1 of idealized F-Shampoo. All these methods perform QR or eigen decompostion at every 10 iterations. Note that F-shampoo cannot match the performance of Shampoo with step-size grafting. This also illustrates using the Frobenius norm for preconditioner estimation is not ideal. To ensure a fair comparison and eliminate implementation bias, we use our own implementation of F-Shampoo, aligned closely with that of KL-Shampoo. As a reference, we also include the best Shampoo run with power ğ‘= 1/2and grafting based on the state-of-the-art version from Meta (Shi et al., 2023). Now, we introduce quantum relative entropy, which is also known as von Neumann (VN) divergence, to show that Shampoo with trace scaling is a generalization of Adafactor. The VN divergence VN(X,Y) is defined as a Bregman divergence with convex functionğ¹(M):=Tr\u0000MLogM(M)âˆ’M\u0001: VN(X,Y):=B ğ¹(X,Y) =ğ¹(X)âˆ’ğ¹(Y)âˆ’Tr\u0000[âˆ‡ğ¹(Y)](Xâˆ’Y)\u0001 =Tr\u0000XLogM(X)âˆ’Xâˆ’YLogM(Y)+Yâˆ’LogM(Y)(Xâˆ’Y)\u0001 (defn. of functionğ¹(Â·)) =Tr\u0000XLogM(X)âˆ’Xâˆ’LogM(Y)Y+Yâˆ’LogM(Y)(Xâˆ’Y)\u0001 (property of the trace) =Tr\u0000XLogM(X)âˆ’X+Yâˆ’LogM(Y)X\u0001 =Tr\u0000X[LogM(X)âˆ’LogM(Y)]\u0001âˆ’Tr(X)+Tr(Y), where LogM(Â·) is the matrix logarithm function and Tsuda et al. (2005) show that âˆ‡ğ¹(M)=LogM(M) . The Hessian of this ğ¹(Â·) gives rise to the Bogoliubov-Kubo-Mori (BKM) metric in quantum physics (de Boer et al., 2023). Claim 7 (Shampooâ€™s estimation scheme with trace scaling)The optimal solution of the von Neumann (VN) divergence (quantum relative entropy) minimization minSğ‘,Sğ‘VN\u0000E[ggâŠ¤],S\u0001:=Tr(S)âˆ’Tr\u0000E[ggâŠ¤]LogM(S)\u0001+ const. with a two-sided precontionerS=S ğ‘âŠ—Sğ‘should satisfy the following condition. Sâˆ— ğ‘=1 Tr(Sâˆ— ğ‘)E[GGâŠ¤],Sâˆ— ğ‘=1 Tr(Sâˆ—ğ‘)E[GâŠ¤G],(14) whereLogM(Â·)is the matrix logarithm function. The optimal solutions is Shampooâ€™s estimation rule (powerğ‘=1 2) with trace scaling: Sâˆ— ğ‘=E[GGâŠ¤],Sâˆ— ğ‘=E[GâŠ¤G] Tr(E[GGâŠ¤]) If we force Sğ‘andSğ‘to be diagonal matrices and solve the minimization problem, we obtain Adafactorâ€™s update as shown below. Sâˆ— ğ‘=Diag\u0000E[GGâŠ¤]\u0001=Diag\u0000E[\u0000GâŠ™2\u00011]\u0001 Sâˆ— ğ‘=Diag\u0012E[GâŠ¤G] Tr\u0000E[GGâŠ¤]\u0001\u0013 =Diag\u0000E[1âŠ¤GâŠ™2]\u0001 Tr\u0000E[1âŠ¤\u0000GâŠ™2\u00011]\u0001=Diag\u0000E[1âŠ¤\u0000GâŠ™2\u0001]\u0001 âˆšï¸ƒ Tr\u0000E[1âŠ¤\u0000GâŠ™2\u0001\u0001Tr\u0000E[\u0000GâŠ™2\u00011]\u0001 20 KL-Shampoo and KL-SOAP Wu Lin, et al. (2025) Remark:If the expectations are not computed exactly, the resulting update scheme is not the optimal solution. For example, Adafactorâ€™s update scheme is not optimal due to the EMA scheme on the diagonal Kronecker factors. Proof:We will show that Shampooâ€™s update scheme with trace scaling is an optimal solution to this minimization problem. We first simplify the objective function when S=Sğ‘âŠ—Sğ‘. We will use this (Kronecker sum) identity, LogM(Sğ‘âŠ—Sğ‘)=LogM(S ğ‘)âŠ—Iğ‘+Iğ‘âŠ—LogM(S ğ‘), to simplify the matrix logarithm. VN(E[ggâŠ¤],S)=Tr(S)âˆ’Tr\u0000E[ggâŠ¤]LogM(S)\u0001+const. =Tr(Sğ‘)Tr(Sğ‘)âˆ’Tr\u0000E[ggâŠ¤]LogM(S)\u0001+const. =Tr(Sğ‘)Tr(Sğ‘)âˆ’Tr\u0000E[ggâŠ¤]\u0000LogM(Sğ‘)âŠ—Iğ‘+Iğ‘âŠ—LogM(S ğ‘)\u0001\u0001+const. =Tr(Sğ‘)Tr(Sğ‘)âˆ’Tr\u0000E[ggâŠ¤]\u0000LogM(Sğ‘)âŠ—Iğ‘\u0001+Tr\u0000E[ggâŠ¤]\u0000Iğ‘âŠ—LogM(S ğ‘)\u0001\u0001+const. =Tr(Sğ‘)Tr(Sğ‘)âˆ’E\u0002 Tr\u0000ggâŠ¤\u0000LogM(Sğ‘)âŠ—Iğ‘\u0001\u0003 âˆ’E\u0002 Tr\u0000ggâŠ¤\u0000Iğ‘âŠ—LogM(S ğ‘)\u0001\u0001\u0003 +const. =Tr(Sğ‘)Tr(Sğ‘)âˆ’E\u0002 Tr\u0000GâŠ¤LogM(Sğ‘)GIğ‘\u0001\u0003 âˆ’E\u0002 Tr\u0000GâŠ¤Iğ‘GLogM(S ğ‘)\u0001\u0003 +const. =Tr(Sğ‘)Tr(Sğ‘)âˆ’E\u0002 Tr\u0000GâŠ¤LogM(Sğ‘)G\u0001\u0003 âˆ’E\u0002 Tr\u0000GâŠ¤GLogM(S ğ‘)\u0001\u0003 +const. =Tr(Sğ‘)Tr(Sğ‘)âˆ’E\u0002 Tr\u0000GGâŠ¤LogM(Sğ‘)\u0001\u0003 âˆ’E\u0002 Tr\u0000GâŠ¤GLogM(S ğ‘)\u0001\u0003 +const. =Tr(ExpM(P ğ‘))Tr(ExpM(P ğ‘))âˆ’E\u0002 Tr\u0000GGâŠ¤Pğ‘\u0001\u0003 âˆ’E\u0002 Tr\u0000GâŠ¤GPğ‘\u0001\u0003 +const. (15) wherePğ‘˜:=LogM(S ğ‘˜)forğ‘˜âˆˆ{ğ‘,ğ‘}andExpM(Â·)is the matrix exponential function. Notice that the optimal solution should satisfy the gradient stationary condition. We consider the gradient with respect to Pğ‘˜because this condition must be satisfied regardless of Sğ‘˜andPğ‘˜forğ‘˜âˆˆ{ğ‘,ğ‘} . The condition for the derivative of Eq. (15) with respect toP ğ‘is 0=ğœ•Pğ‘VN(E[ggâŠ¤],S)=ExpM(P ğ‘) | {z } =Sğ‘Tr(ExpM(P ğ‘)) | {z } =Tr\u0000 Sğ‘\u0001âˆ’E\u0002 GGâŠ¤\u0003 where Tsuda et al. (2005) show thatğœ• Pğ‘˜Tr(ExpM(P ğ‘˜))=ExpM(P ğ‘˜). Thus, we can see that the optimal solution must satisfy this condition Sâˆ— ğ‘=E\u0002",
    ". The condition for the derivative of Eq. (15) with respect toP ğ‘is 0=ğœ•Pğ‘VN(E[ggâŠ¤],S)=ExpM(P ğ‘) | {z } =Sğ‘Tr(ExpM(P ğ‘)) | {z } =Tr\u0000 Sğ‘\u0001âˆ’E\u0002 GGâŠ¤\u0003 where Tsuda et al. (2005) show thatğœ• Pğ‘˜Tr(ExpM(P ğ‘˜))=ExpM(P ğ‘˜). Thus, we can see that the optimal solution must satisfy this condition Sâˆ— ğ‘=E\u0002 GGâŠ¤\u0003 Tr(Sâˆ— ğ‘) Similarly, we can obtain the second condition. Sâˆ— ğ‘=E\u0002 GGâŠ¤\u0003 Tr(Sâˆ—ğ‘) We can verify that the following solution satisfies these conditions. Sâˆ— ğ‘=E\u0002 GGâŠ¤\u0003 ,Sâˆ— ğ‘=E\u0002 GâŠ¤G\u0003 Tr(E\u0002 GGâŠ¤\u0003 ) Notice that the optimal Sğ‘andSğ‘are not unique. However, their Kronecker, which is Sâˆ—=Sâˆ— ğ‘âŠ—Sâˆ— ğ‘, is unique. Prior studies (Morwani et al., 2025; Vyas et al., 2025a; Eschenhagen et al., 2025) have shown that this solution is an optimal Kronecker approximation of the flattened gradient second moment under the Frobenius norm. In the Adafactor case, the result can be similarly derived when considering Sğ‘˜to be a diagonal matrix for ğ‘˜âˆˆ{ğ‘,ğ‘}. â–¡ H Additional Experiments We conduct three additional sets of experiments, following the same experimental setup as described in the main text, to further evaluate our approach. Due to our limited computation, we focus on two language models: NanoGPT (123M) and Llama (134M) in these additional experiments. 21 KL-Shampoo and KL-SOAP Wu Lin, et al. (2025) Idealized VN-Shampoo: Improving Shampoo (ğ‘= 1/2) with trace scaling 1:Gradient Computationg:=âˆ‡â„“(Î¸) G:=Mat(g)âˆˆRğ‘‘ğ‘Ã—ğ‘‘ğ‘ 2:Covariance Estimation (each iter)\u0012Sğ‘ Sğ‘\u0013 â†(1âˆ’ğ›½ 2)\u0012Sğ‘ Sğ‘\u0013 +ğ›½2\u0012Î”ğ‘ Î”ğ‘\u0013 Î”ğ‘:=( GGâŠ¤(variant 1) GGâŠ¤/Ã(Î»ğ‘)(variant 2) Î”ğ‘:=( GâŠ¤G(variant 1) GâŠ¤G/Ã(Î»ğ‘)(variant 2) 3:Eigendecomposition (everyğ‘‡â‰¥1iters) Î»ğ‘˜,Qğ‘˜â†eig(Sğ‘˜)forğ‘˜âˆˆ{ğ‘,ğ‘} 4:Preconditioning usingQ:=Q ğ‘âŠ—Qğ‘ Î¸â†Î¸âˆ’ğ›¾(QDiag(ğœÎ» ğ‘âŠ—Î»ğ‘)âˆ’1/2QâŠ¤)g ğœ:=( 1/âˆšï¸ Tr(Sğ‘)Tr(Sğ‘)(variant 1) 1(variant 2)VN-Shampoo: Replacing the slow eigen step with a more efficient QR step(replace Step 3) 3a:FrequentEigenvalue Estimation with EMA (each iter)\u0012Î»ğ‘ Î»ğ‘\u0013 â†(1âˆ’ğ›½ 2)\u0012Î»ğ‘ Î»ğ‘\u0013 +ğ›½2\u0012diag(QâŠ¤ ğ‘Î”ğ‘Qğ‘) diag(QâŠ¤ ğ‘Î”ğ‘Qğ‘)\u0013 3b: Infrequent Eigenbasis Estimation using QR (everyğ‘‡â‰¥1iters) Qğ‘˜â†qr(Sğ‘˜Qğ‘˜)forğ‘˜âˆˆ{ğ‘,ğ‘} Figure 8:Left:Simplified VN-Shampoo schemes motivated by Claim 7 to incorporate trace scaling. We consider two variants to incorporate trace scaling into the original Sham- poo. Variant 1 is inspired by Adafactorâ€™s update scheme, while Variant 2 is similar to KL-Shampooâ€™s update scheme. Note that Variant 1 of the idealized VN-Shampoo is known as Shampoo with trace scaling in the literature.Right:Adapting our exponential moving average (EMA) approach enables VN-Shampoo to use the faster QR procedure and makes it competitive, as empirically shown in Fig. 9 and Fig. 10. In the first additional experiment, we evaluate the two-sided Shampoo based on Frobenius norm (Morwani et al., 2025; Eschenhagen et al., 2025)â€”referred to as idealized F-Shampooâ€”and find that it performs poorly in practice even when we improve its performance using QR and EMA on the eigenvalues, as shown in Fig. 7. This indicates using the Frobenius norm for preconditioner estimation is not ideal. In the second additional experiment, we evaluate Shampoo with trace scaling (Morwani et al., 2025; Vyas et al., 2025a; Eschenhagen et al., 2025)â€”referred to as idealized VN-Shampooâ€”and find that it performs poorly in practice even when using eigendecomposition. By contrast, incorporating our moving-average scheme enables it to perform well and use the fast QR decomposition, as demonstrated in Fig. 9. In the third additional experiment, we evaluate the suitability of KL versus",
    "2025)â€”referred to as idealized VN-Shampooâ€”and find that it performs poorly in practice even when using eigendecomposition. By contrast, incorporating our moving-average scheme enables it to perform well and use the fast QR decomposition, as demonstrated in Fig. 9. In the third additional experiment, we evaluate the suitability of KL versus VN divergence for refining Shampooâ€™s estimation rule in a comparable setting, where both variants outperform SOAP while matching SOAP-level pre-iteration runtime. As shown in Fig. 10, KL-Shampoo consistently outperforms VN-Shampoo, even when VN-Shampoo is made practical and competitive using similar techniques to those employed in KL-Shampoo. These results underscore the advantages of the KL divergence over the VN divergence. 22 KL-Shampoo and KL-SOAP Wu Lin, et al. (2025) 5.05.56.06.57.0NanoGPT-123M-FineWeb1B 3.43.63.84.0Llama-134M-C4-2B 10 5 1 Iterations (thousands)3.253.303.35aa 10 5 1 Iterations (thousands)3.203.253.30Test Loss VN-Shampoo (v2, QR, EMA) VN-Shampoo (v1, QR, EMA) Shampoo-trace scaling (eigen, No EMA) Shampoo (p=1 2, grafting) Figure 9: Empirical results from a random search with 150 runs per method on language models demonstrate that our exponential moving average (EMA) scheme for eigenvalue estimation, as described in Fig. 8, makes Shampoo with trace scalingâ€”referred to as Variant 1 of idealized VN-Shampooâ€”practical and enables it to match or exceed the performance of Shampoo with step-size grafting. All these methods perform QR or eigen decompostion at every 10 iterations. Without this scheme, Shampoo with trace scaling performs poorly in practice, as shown in the figure. We implement VN-Shampoo (i.e., Shampoo with trace scaling) ourselves, as it is not available in existing implementations, including the state-of-the-art version from Meta (Shi et al., 2023). As a reference, we also include the best Shampoo run with powerğ‘= 1/2and grafting based on the implementation from Meta. 0 20 40 60 80 100 120 Minutes3.243.263.283.303.323.343.363.383.40Test LossNanoGPT-123M-FineWeb1B 0 20 40 60 80 100 120 140 Minutes3.2003.2253.2503.2753.3003.3253.3503.3753.400Llama-134M-C4-2B KL-Shampoo VN-Shampoo (v2, QR, EMA) VN-Shampoo (v1, QR, EMA) SOAP Figure 10: Empirical results (random search using 150 runs for each method) demonstrate that the advantages of KL-Shampoo over VN-Shampoo under comparable settings. In particular, we strengthen VN-Shampoo (i.e., Shampoo with trace scaling) by incorporating the QR step and the EMA scheme for eigenvalue estimation, as described in Fig. 8, to achieve SOAP-level pre-iteration runtime. To ensure a fair comparison and eliminate implementation bias, we use our own implementation of VN-Shampoo, aligned closely with that of KL-Shampoo. For runtime comparison, we include the best SOAP run as a reference. All methods take the same number of iterations in these experiments. 23"
  ]
}