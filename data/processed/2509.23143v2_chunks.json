{
  "filename": "2509.23143v2.pdf",
  "total_chunks": 6,
  "text_length": 20491,
  "chunks": [
    "MathBode: Frequency-Domain Fingerprints of LLM Mathematical Reasoning Charles L. Wang Department of Computer Science Columbia University charles.w@columbia.edu Abstract We presentMathBode, adynamic diagnosticfor mathematical reasoning in large language models (LLMs). Instead of one-shot accuracy, MathBode treats each parametric problem as a system: we drive a single parameter sinusoidally and fit first-harmonic responses of model outputs and exact solutions. This yields interpretable, frequency-resolved metrics—gain(amplitude tracking) andphase (lag)—that form Bode-style fingerprints. Across five closed-form families (linear solve, ratio/saturation, compound interest, 2×2 linear systems, similar triangles), the diagnostic surfaces systematiclow-passbehavior and growing phase lag that accuracy alone obscures. We compare several models against a symbolic baseline that calibrates the instrument ( G≈1 ,ϕ≈0 ). Results separate frontier from mid-tier models on dynamics, providing a compact, reproducible protocol that complements standard benchmarks with actionable measurements of reasoning fidelity and consistency. We open-source the dataset and code to enable further research and adoption. Code | Dataset 1 Introduction Large language models (LLMs) now score highly on math benchmarks, but final–answer accuracy obscureshowthey reason and whether behavior is stable under controlled changes. We propose a dynamicevaluation: treat each parametric problem as a system, drive one parameter sinusoidally, and summarize the model’s response bygain(amplitude tracking) andphase(lag) over frequency. MathBodeimplements this across five closed-form families, fitting first-harmonic responses to produce Bode-style fingerprints that reveal low-pass behavior and growing phase lag even when static accuracy ties. The protocol is simple (short prompts, deterministic decoding) and includes a symbolic baseline to calibrate the instrument (ideal G≈1 ,ϕ≈0 ). We report G(ω) ,|ϕ(ω)| , mid-band aggregates, residual autocorrelation, and first-harmonic fit quality ( R2), providing a complementary lens on reasoning fidelity, consistency, and prompt sensitivity that accuracy alone cannot capture. Context.Progress in mathematical reasoning is typically reported on static, final-answer datasets such as GSM8K and MATH, with domain-tuned systems (e.g., Minerva) pushing scores higher [Cobbe et al., 2021, Hendrycks et al., 2021, Lewkowycz et al., 2022]. Newer suites emphasize expert difficulty and recency—OlympiadBench, Omni-MATH, FrontierMath—yet still follow the one- input/one-answer paradigm [He et al., 2024, Gao et al., 2024, Glazer et al., 2024]. A parallel thread probes robustness: small semantic edits can flip answers (SV AMP; MATH-Perturb), while sampling strategies like self-consistency improve end accuracy withoutmeasuringstability [Patel et al., 2021, Huang et al., 2025, Wang et al., 2022]. Meta-reasoning probes and repeated-trial consistency likewise show models can be correct once yet unreliable across paraphrases or restarts [Zeng et al., 2023]. Together, these observations motivate metrics that capture reliability and invariance, not just correctness. Preprint.arXiv:2509.23143v2 [cs.AI] 30 Sep 2025 Why a frequency/phase view?Interpretability results suggest a principled bridge to the frequency domain: transformers trained on arithmetic learn sinusoidal/rotational internal codes; modular addition emerges via Fourier-like features and rotations; recent work describes clock-like number embeddings and trigonometric operations [Nanda et al., 2023, Kantamneni and Tegmark, 2025, Li et al., 2024]. If numeric reasoning is expressed in amplitude and phase, then frequency-response style probing is natural rather than metaphorical. What MathBode measures.For each family, we generate a parameter trajectory pt=p 0+ ϵsin(ωt) , decode a single numeric line with temperature 0, and fit {1,sin(ωt),cos(ωt)} to both ground",
    "Li et al., 2024]. If numeric reasoning is expressed in amplitude and phase, then frequency-response style probing is natural rather than metaphorical. What MathBode measures.For each family, we generate a parameter trajectory pt=p 0+ ϵsin(ωt) , decode a single numeric line with temperature 0, and fit {1,sin(ωt),cos(ωt)} to both ground truth and model outputs. From the fitted coefficients we recover amplitude and phase and compute G(ω) = amp(ˆy)/amp(y∗)andϕ(ω) = wrap\u0000 ϕ(ˆy)−ϕ(y∗)\u0001 . We sweep ω∈ {1,2,4,8,16} (64 steps), optionally vary start phase to assess phase stability, and include a symbolic baseline that realizes the ideal response. The resulting frequency-resolved curves and aggregates expose amplitude fidelity, timing lag, and prompt-surface sensitivity—even when static accuracy saturates or training-data familiarity blurs the line between recall and robust computation. 2 Benchmark Instrument.We probedynamicmathematical reasoning by driving one problem parameter with a sinusoid and fitting first-harmonic responses of model outputs against exact solutions. For a sweep of lengthTand angular frequencyωwe instantiate prompts with pt=p0+ϵsin(ωt+ϕ 0), t= 1, . . . , T, decode deterministically (temperature 0) to a single numeric line ( FINAL: <number> ), and parse the model series ˆytalongside the exact series y∗ t. Each series is regressed onto {sin(ωt),cos(ωt),1} ; from the fitted coefficients(a, b, c)we recover amplitude and phase amp(y) =p a2+b2, ϕ(y) = atan2(a, b). We then report G(ω) =amp(ˆy) amp(y∗), ϕ(ω) = wrap(−π,π]\u0000 ϕ(ˆy)−ϕ(y∗)\u0001 , along with first-harmonic R2(fit quality), residual RMS (normalized), residual ACF(1), and a nonlinearity proxy H2/H1from a joint fit at ωand2ω. A symbolic solver baseline runs through the identical pipeline, providing the ideal reference (G≈1,ϕ≈0). Families.We evaluate five closed-form families with fixed domains and three question variants each:Linear Solve( ax+b=c , solve x),Ratio Saturation( p/(p+k) ),Exponential Interest( A(1+p)t), Linear System(solve xin a2×2 system with a=p), andSimilar Triangles(scaling s′=s p ). Families expose(p range, p0, ϵ)via code, and inputs are clipped in-range. Frequency grid and phases.We choose T=64 and sweep Ω ={1,2,4,8,16} cycles per 64 steps. To assess phase robustness we use start phases {0◦,120◦,240◦}. Defaults set ϵto roughly 10% of the family’s half-range. Why this design?Gain and phase isolate amplitude tracking and lag—two core behaviors that final-answer accuracy obscures—while R2and residual diagnostics validate the first-harmonic approximation and expose structure left unexplained by it. The frequency grid (with tri-phase repeats) yields stability bands rather than single-shot outcomes, and the symbolic baseline calibrates the measurement end-to-end. The result is an inexpensive, reproducible instrument that complements static accuracy with a frequency-domain lens on reasoning fidelity and consistency. 3 Evaluation Why these views?Final-answer accuracy hideshowa model tracks controlled variation. We therefore summarize each family’s response along four complementary axes:(i) gain(amplitude tracking),(ii) phase error(timing/lag),(iii) residual autocorrelationACF(1) (leftover temporal structure not captured by the first harmonic), and(iv) first-harmonic fit quality R2(is a single sinusoid adequate?). Together these expose low-pass behavior, timing slippage, and prompt-surface sensitivity even when accuracy ties. Additional diagnostics (H2/H1 nonlinearity, compliance, phase- stability across start phases) appear in the appendix. 2 Figure 1:Gain vs. frequency.Panels are families; curves overlay models (unity G=1 dashed). Mid-band ({4,8}) deviations indicate under/over-reaction despite identical ground truth. Takeaway (Gain).Most models arelow-pass: gain declines with frequency",
    "prompt-surface sensitivity even when accuracy ties. Additional diagnostics (H2/H1 nonlinearity, compliance, phase- stability across start phases) appear in the appendix. 2 Figure 1:Gain vs. frequency.Panels are families; curves overlay models (unity G=1 dashed). Mid-band ({4,8}) deviations indicate under/over-reaction despite identical ground truth. Takeaway (Gain).Most models arelow-pass: gain declines with frequency inLinear Solveand Exponential Interest;Similar Trianglesstays near G≈1 (instrument check).Linear Systemamplifies between-model differences. Figure 2:Phase error vs. frequency.Signed model–truth phase (deg), wrapped to (−π, π] ;0◦ implies perfect timing. Takeaway (Phase).Phase lag typically grows with frequency (delayed tracking). Closed-form proportional families (e.g.,Similar Triangles) remain near 0◦;Linear Systemshows the largest swings (coupling sensitivity). Figure 3:Residual ACF(1) vs. frequency.Near-zero ACF(1) means little temporal structure remains after the harmonic fit; negative values align with alternating over/undershoots at higher frequencies. 3 Takeaway (Residuals).Residual ACF(1) trends toward 0or negative with frequency, indicating the first harmonic explains most structure and that remaining errors alternate rather than drift. Residual RMS and H2/H1 curves are provided in the appendix. Figure 4:First-harmonic fit quality ( R2) vs. frequency.High R2validates a single-sinusoid description; dips signal nonlinear distortion or prompt-surface effects. Takeaway ( R2).R2is near 1forSimilar Trianglesand in the mid-band elsewhere; drops in Exponential InterestandLinear Systemco-locate with the largest gain/phase deviations, pointing to emergent nonlinearities rather than random noise. Table 1: Overall MathBode scores. MB-Core uses mid-band gain/phase; MB-Plus additionally downweights poor certain features. Model MB-Core MB-Plus DeepSeek V3.1 0.834 0.656 Qwen3 235B Instruct 0.782 0.576 GPT-4o 0.778 0.566 Llama 4 Instruct 0.644 0.433 Mixtral 8×7B 0.360 0.281 Table 2: Per-family MB-Core (mean mid-band performance). Model Exponential Interest Linear Solve Linear System Ratio Saturation Similar Triangles DeepSeek V3.1 0.848 0.9950.3310.997 1.000 GPT-4o 0.497 0.993 0.418 0.9801.000 Llama 4 Instruct 0.461 0.489 0.450 0.8211.000 Mixtral 8×7B 0.500 0.494 0.029 0.000 0.779 Qwen3 235B Instruct 0.467 0.9820.4710.9901.000 4 Conclusion. MathBode reframes mathematical evaluation as a dynamic, frequency–domain probe, yielding interpretable gain/phase curves rather than only final answers, moving evaluations towards more reliable mathematical reasoning. Across five closed-form families, models consistently exhibit low- pass behavior and growing phase lag, while the symbolic baseline and our MB-Core/MB-Plus scores summarize these dynamics in a comparable and robust way. The results indicate that strong static accuracy can mask systematic amplitude and timing errors that degrade stability and consistency of reasoning. Practically, the frequency fingerprints provide a compact diagnostic for model selection and ablation studies, complementing standard benchmarks with measurements that are reproducible and easy to interpret. We release the dataset and reference code to support transparent replication and extension. Limitations include the small number of families and single-tone drives; future work will expand the task set, add richer inputs (chirps, steps), and link frequency fingerprints to internal mechanisms (e.g., attention dynamics, layer-wise delays). 4 References Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Łukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems.arXiv preprint arXiv:2110.14168, 2021. URLhttps://arxiv.org/abs/2110.14168. Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang Chen, Runxin Xu, Zhengyang Tang, Benyou Wang, Daoguang Zan, Shanghaoran Quan, Ge",
    "Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems.arXiv preprint arXiv:2110.14168, 2021. URLhttps://arxiv.org/abs/2110.14168. Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang Chen, Runxin Xu, Zhengyang Tang, Benyou Wang, Daoguang Zan, Shanghaoran Quan, Ge Zhang, Lei Sha, Yichang Zhang, Xuancheng Ren, Tianyu Liu, and Baobao Chang. Omni-math: A universal olympiad level mathematic benchmark for large language models.arXiv preprint arXiv:2410.07985, 2024. URLhttps://arxiv.org/abs/2410.07985. Elliot Glazer, Ege Erdil, Tamay Besiroglu, Diego Chicharro, Evan Chen, Alex Gunning, Caroline Falkman Olsson, Jean-Stanislas Denain, Anson Ho, Emily de Oliveira Santos, Olli Järviniemi, Matthew Barnett, Robert Sandler, Matej Vrzala, Jaime Sevilla, Qiuyu Ren, Elizabeth Pratt, Lionel Levine, Grant Barkley, Natalie Stewart, Bogdan Grechuk, Tetiana Grechuk, Shreepranav Varma Enugandla, and Mark Wildon. Frontiermath: A benchmark for evaluating advanced mathematical reasoning in AI.arXiv preprint arXiv:2411.04872, 2024. URL https://arxiv.org/abs/2411. 04872. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Yuxiang Zhang, et al. Olympiadbench: A challenging benchmark for promoting AGI with olympiad-level bilingual multimodal scientific problems.arXiv preprint arXiv:2402.14008, 2024. URL https://arxiv.org/abs/2402. 14008. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset.arXiv preprint arXiv:2103.03874, 2021. URLhttps://arxiv.org/abs/2103.03874. Kaixuan Huang, Jiacheng Guo, Zihao Li, Xiang Ji, Jiawei Ge, Wenzhe Li, Yingqing Guo, Tianle Cai, Hui Yuan, Runzhe Wang, Yue Wu, Ming Yin, Shange Tang, Yangsibo Huang, Chi Jin, Xinyun Chen, Chiyuan Zhang, and Mengdi Wang. Math-perturb: Benchmarking llms’ math reasoning abilities against hard perturbations.arXiv preprint arXiv:2502.06453, 2025. URL https://arxiv.org/abs/2502.06453. Subhash Kantamneni and Max Tegmark. Language models use trigonometry to do addition.arXiv preprint arXiv:2502.00873, 2025. URLhttps://arxiv.org/abs/2502.00873. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models. InAdvances in Neural Information Processing Systems (NeurIPS), 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/ file/18abbeef8cfe9203fdf9053c9c4fe191-Paper-Conference.pdf. Junlin Li, Zhan Sun, Jiahao Ma, Qipeng He, Qizhe Huang, Huanzhang Xu, and Yan Li. Mech- anistic interpretability of binary and ternary modular addition in transformers.arXiv preprint arXiv:2405.17703, 2024. URLhttps://arxiv.org/abs/2405.17703. Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, and Jacob Steinhardt. Progress measures for grokking via mechanistic interpretability. InInternational Conference on Learning Representations (ICLR), 2023. URLhttps://openreview.net/forum?id=9XFSbDPmdW. Arkil Patel, Satwik Bhattamishra, and Navin Goyal. Are nlp models really able to solve simple math word problems? InProceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL), 2021. URL https://aclanthology.org/ 2021.naacl-main.168/. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V . Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models.arXiv preprint arXiv:2203.11171, 2022. URL https://arxiv.org/abs/2203.11171 . 5 Zhongshen Zeng, Pengguang Chen, Shu Liu, Haiyun Jiang, and Jiaya Jia. Mr-gsm8k: A meta- reasoning benchmark for large language model evaluation.arXiv preprint arXiv:2312.17080, 2023. URLhttps://arxiv.org/abs/2312.17080. A Appendix B Dataset Overview Cardinality.MATHBODEcontains9,408 rows per familyand47,040 rows totalacross five families. Table 3:Dataset rows by family. Family",
    "arXiv:2203.11171, 2022. URL https://arxiv.org/abs/2203.11171 . 5 Zhongshen Zeng, Pengguang Chen, Shu Liu, Haiyun Jiang, and Jiaya Jia. Mr-gsm8k: A meta- reasoning benchmark for large language model evaluation.arXiv preprint arXiv:2312.17080, 2023. URLhttps://arxiv.org/abs/2312.17080. A Appendix B Dataset Overview Cardinality.MATHBODEcontains9,408 rows per familyand47,040 rows totalacross five families. Table 3:Dataset rows by family. Family Rows Exponential Interest 9,408 Linear Solve 9,408 Linear System 9,408 Ratio Saturation 9,408 Similar Triangles 9,408 Total 47,040 Attribute Type Description familystring One of {exponential_interest,linear_solve,linear_system, ratio_saturation,similar_triangles}. question_idint Variant index within a family. signal_typestring Drive label: {sinusoid,chirp,step}. amplitude_scalefloat Relative amplitude (e.g., 0.5, 1.0, 2.5). frequency_cyclesfloat Frequency label (cycles per 64 steps). phase_degfloat Start phase (degrees). time_stepint Index within the rendered sequence. p_valuefloat Concrete parameter value used to render the prompt. promptstring Fully-rendered natural-language question for the instance. ground_truthfloat Exact numerical answer. C Presets Table 4:Inference presets.Tri-phase indicates whether phases {0,120,240} are used. Preset Frequencies Phases Tri-phase coverageK(base keys/family) SMOKE{4, 8} {0} none 2 MVP{4, 8, 16} {0} none 2 MVP_PLUS{1, 2, 4, 8, 16} {0}* only for {4, 8} 2 FULL{1, 2, 4, 8, 16} {0, 120, 240} all frequencies 2 Note*InMVP_PLUS, phases {0,120,240} are applied only at mid-band frequencies {4, 8}; other frequencies use phase {0}. D Answer Format & Strict Parsing Models output [answer_start] X.YYYYYY [answer_end] where the payload is a fixed-precision decimal with exactly six places. 6 Parsing.From the raw response we (i) find thelast complete [answer_start] ... [answer_end] pair, (ii) scan inside for decimal literals (ASCII dig- its only; no scientific notation, separators, or units), (iii) take thelastliteral found, and (iv)truncate to exactly six decimals (pad with zeros if fewer; cut off if more). Non-finite values (NaN/Inf) or missing tags are non-compliant. Compliance.Rows that pass this pipeline count as compliant; only compliant rows are used for harmonic fitting and residual diagnostics. Non-compliant rows still contribute to compliance statistics. E Figures & Tables Table 5:A.1 Mean |G−1| at mid-frequencies (4 & 8 cycles).Lower is better. EI and LS dominate amplitude error; DeepSeek is best on EI gain, while Mixtral is worst on RS. DeepSeek V3.1 GPT-4o Llama 4 Instruct Mixtral 8×7B Qwen3 235B Exponential Interest 0.051 6.819 6.512 8.418 0.323 Linear Solve 0.002 0.003 0.312 0.313 0.009 Linear System 0.188 0.308 4.714 0.622 1.453 Ratio Saturation 0.002 0.010 0.087 5.059 0.005 Similar Triangles 0.000 0.000 0.000 0.110 0.000 Implications.Mid-band amplitude fidelity matters for stability:EIexposes large magnitude distortions in GPT-4o/Llama/Mixtral, so downstream pipelines that depend on accurate scaling (e.g., compounding, normalization, controller gains) will drift unless corrected. DeepSeek’s best-in-class EI gain suggests safer use when amplitude tracking dominates, whereas Mixtral’s large RS error flags sensitivity to saturating transforms. Family-level selection thus changes which model is “best” for a given deployment. Table 6:A.2 Mean |Phase Error |(deg) at mid-frequencies (4 & 8 cycles).Lower is better. LS is the timing bottleneck (largest lags/leads); Qwen is best on LS, while Mixtral collapses on RS. DeepSeek V3.1 GPT-4o Llama 4 Instruct Mixtral 8×7B Qwen3 235B Exponential Interest 4.47 0.24 3.54 0.04 2.97 Linear Solve 0.02 0.01 1.02 0.56 0.03 Linear System 26.38 7.38 4.49 42.40 2.61 Ratio Saturation 0.01 0.01 0.38 58.42 0.01 Similar Triangles",
    "Qwen is best on LS, while Mixtral collapses on RS. DeepSeek V3.1 GPT-4o Llama 4 Instruct Mixtral 8×7B Qwen3 235B Exponential Interest 4.47 0.24 3.54 0.04 2.97 Linear Solve 0.02 0.01 1.02 0.56 0.03 Linear System 26.38 7.38 4.49 42.40 2.61 Ratio Saturation 0.01 0.01 0.38 58.42 0.01 Similar Triangles 0.00 0.00 0.00 0.05 0.00 Implications.Phase governstiming consistency: large LS phase errors (Mixtral, DeepSeek) imply lag/lead that can destabilize iterative procedures (solvers, rollouts) and corrupt ablations that assume time alignment. Qwen’s low LS phase is attractive for timing-sensitive use cases even if its gain is not always best. When choosing models for pipelines with feedback or chaining, prioritize low phase on the relevant family. Figure 5:A3. Compliance by family.Compliance is perfect overall. 7 Implications.Near-perfect compliance removes formatting as a confound: observed dynamics (gain/phase/residuals) reflect model behavior rather than parse failures. This also means MB-Plus penalties primarily capture quality, not I/O brittleness, and reproductions should match our curves given the same row IDs. Figure 6:A4. H2/H1vs. frequency.Nonlinearity concentrates in EI and LS; Similar Triangles stays near zero. Implications.Elevated H2/H1indicates distortion rather than pure linear gain/phase behavior. Peaks in EI/LS suggest that prompts with compounding or coupled relations will exhibit waveform deformation under parameter sweeps—use multi-tone tests or chirps to separate memory effects from static nonlinearity, and avoid using single-sinusoid fingerprints alone to claim linearity. Figure 7:A5. Residual RMS (normalized).Single-sinusoid fits leave the largest residuals in EI and LS; simpler families fit tightly. Implications.High residuals mean a first-harmonic model is insufficient: EI/LS retain structure after removing the main tone, so downstream diagnostics should include richer inputs (chirps, steps, two-tone mixtures) before attributing errors solely to amplitude or timing. Low residuals on simpler families justify using mid-band summaries (MB-Core/MB-Plus) as compact, reliable proxies there. F API Settings For all model calls (Together and OpenAI), we used the following fixed decoding settings: •Temperature:0.0 •Max tokens:1028 To ensure stable throughput and reproducibility, we applied simple rate limiters: •Together:600 requests per minute (RPM) 8 •OpenAI:20,000 tokens per minute (TPM) These settings were held constant across all experiments unless explicitly noted elsewhere. 9"
  ]
}