{
  "filename": "2509.07123v1.pdf",
  "total_chunks": 23,
  "text_length": 72874,
  "chunks": [
    "NestGNN: A Graph Neural Network Framework Generalizing the Nested Logit Model for Travel Mode Choice Yuqi Zhoua, Zhanhong Chenga, Lingqian Hub, Yuheng Buc, and Shenhao Wanga,* aDepartment of Urban and Regional Planning, University of Florida bCollege of Architecture, Texas A&M University cDepartment of Computer Science, University of California, Santa Barbara Abstract Nested logit (NL) has been commonly used for discrete choice analysis, including a wide range of applications such as travel mode choice, automobile ownership, or location decisions. However, the classical NL models are restricted by their limited representation capability and handcrafted utility specification. While researchers introduced deep neural networks (DNNs) to tackle such challenges, the existing DNNs cannot explicitly capture inter-alternative correlations in the discrete choice context. To address the challenges, this study proposes a novel concept - alternative graph - to represent the relationships among travel mode alternatives. Using a nested alternative graph, this study further designs a nested-utility graph neural network (NestGNN) as a generalization of the classical NL model in the neural network family. Theoretically, NestGNNs generalize the classical NL models and existing DNNs in terms of model representation, while retaining the crucial two-layer substitution patterns of the NL models: proportional substitution within a nest but non-proportional substitution beyond a nest. Empirically, we find that the NestGNNs significantly outperform the benchmark models, particularly the corresponding NL models by 9.2%. As shown by elasticity tables and substitution visualization, NestGNNs retain the two-layer substitution patterns as the NL model, and yet presents more flexibility in its model design space. Overall, our study demonstrates the power of NestGNN in prediction, interpretation, and its flexibility of generalizing the classical NL model for analyzing travel mode choice. Keywords: Choice modeling, Travel behavior, Graph neural network, Deep learningarXiv:2509.07123v1 [stat.ML] 8 Sep 2025 1 Introduction Discrete choice models (DCMs) provide a powerful analytical framework to explain individual decisions and serve as the foundation for the analysis of travel mode choice in transportation planning (Ben-Akiva and Lerman, 1985b; Koppelman and Pas, 1980; Train, 1986). DCM examples include multinomial logit (MNL), Nested logit (NL), Mixed logit (MXL), among many other logit models (Manski, 1977; Walker and Ben-Akiva, 2002; Koppelman, 1981). It is critical for such models to be properly specified, and the specification often involves a trial-and-error process that relies on expert knowledge and prior assumptions (van Cranenburgh and Alwosheel, 2019). To address the limitation in handcrafted utility specification, deep neural networks (DNNs) have been adapted for choice analysis in many recent studies, which have shown their capacity to automate utility specification (van Cranenburgh and Alwosheel, 2019; Sifringer et al., 2020) and improve predictive performance upon conventional DCMs (Lee et al., 2018; Wang et al., 2021a). However, most existing DNNs in discrete choice modeling cannot explicitly capture the dependence among alternatives, while this inter-alternative correlations is a key structure for enhancing the performance of classical DCMs. For example, when commuters choose between automobile, transit, bike, and walking, the NL model can capture the alternative correlations (e.g., bike and walking are similar travel modes), whereas no study has explored how to incorporate alternative correlations into the utility function",
    "key structure for enhancing the performance of classical DCMs. For example, when commuters choose between automobile, transit, bike, and walking, the NL model can capture the alternative correlations (e.g., bike and walking are similar travel modes), whereas no study has explored how to incorporate alternative correlations into the utility function by DNNs. As a result, alternative correlations are either hidden in the feedforward structure (Han et al., 2022; Wang et al., 2021b), or captured with an overly simplified constraint on the independence of irrelevant alternatives (IIA), such as ASU-DNN (Wang et al., 2020b). These research gaps motivate us to further synergize the DCM and DNN approaches by designing a DNN-based framework for travel demand modeling by explicitly incorporating NL’s nested structure. This study introduces Nested-Utility Graph Neural Networks (NestGNNs), which generalize the classical NL models and yet retain their crucial substitution patterns. To design NestGNNs, we first propose a novel concept —alternative graph— to represent inter-alternative correlations and enable GNN computation. Figure 1 compares three alternative representations: alternative set in classical DCMs (Fig. 1a), nest representation in NL models (Fig. 1b), and alternative graph representation for NestGNNs (Fig. 1c). The alternative graph representation generalizes alternative set through the node connections, while being more flexible than the nest representation because it accommodates more complex network structure. Using the alternative graph with disconnected components in Fig. 1c, we design NestGNN models to capture inter-alternative correlations and replicate the unique two-layer substitution patterns in NL models. We theoretically demonstrate that the NestGNNs can reduce to MNL, NL, and ASU-DNN with appropriate hyperparameter configurations. Empirically, we compare the NestGNNs to benchmark models, including MNL, NL, and ASU-DNN models to predict travel mode choice among four automobile, transit, bike, and walking, using the London Passenger Dataset (Hillel et al., 2018). Empirical results demonstrate that the top five NestGNNs consistently outperform all benchmark models—including MNL, NL, and ASU-DNN—in terms of prediction accuracy. Meanwhile, the NestGNNs exhibit elasticity and substitution patterns aligning with those of the NL, and yet represent a much more flexible model design space. Overall, this study makes three contributions. (1) Conceptually, it introduces alternative graph, extending the classical alternative set and enabling the application of GNNs into discrete choice models. (2) Theoretically, it introduces the NestGNN family, which belongs to DNN families but is inspired by the classical NL structure. This family generalizes MNL, NL, and ASU-DNN models within a flexible design space while preserving the NL’s two-layer substitution pattern. (3) Empirically, NestGNNs outperform benchmark models in prediction and design flexibility, while retaining the interpretable substitution patterns of NL models. Such three-layer contributions create a large number of opportunities for future studies, as they integrate classical DCMs, 1 deep learning, and network modeling through one unified NestGNN framework, potentially inspiring future researchers to advance DCMs from diverse theoretical perspectives. Driver Passenger Walking Bike Train/Metro Bus Taxi/Carpool DriverPassenger WalkingBike Train/Metro BusTaxi/CarpoolTaxi/Carpool Driver PassengerWalking Bike Bus Train/Metro (a) Alternative set representation Driver Passenger Walking Bike Train/Metro Bus Taxi/Carpool DriverPassenger WalkingBike Train/Metro BusTaxi/CarpoolTaxi/Carpool Driver PassengerWalking Bike Bus Train/Metro (b) Nest representation and computation in nested logit models Driver",
    "from diverse theoretical perspectives. Driver Passenger Walking Bike Train/Metro Bus Taxi/Carpool DriverPassenger WalkingBike Train/Metro BusTaxi/CarpoolTaxi/Carpool Driver PassengerWalking Bike Bus Train/Metro (a) Alternative set representation Driver Passenger Walking Bike Train/Metro Bus Taxi/Carpool DriverPassenger WalkingBike Train/Metro BusTaxi/CarpoolTaxi/Carpool Driver PassengerWalking Bike Bus Train/Metro (b) Nest representation and computation in nested logit models Driver Passenger Walking Bike Train/Metro Bus Taxi/Carpool DriverPassenger WalkingBike Train/Metro BusTaxi/CarpoolTaxi/Carpool Driver PassengerWalking Bike Bus Train/Metro(c) Alternative graph representation and mes- sage passing in NestGNN Figure 1: Comparison of alternative representations with seven alternatives as an example. The arrow with the dotted line means the direction of message passing. In NestGNN’s alternative graph, the solid line represents the edge between nodes. The remainder of this paper is organized as follows. Section 2 briefly reviews the literature on theory- and data-driven DCMs. Section 3 proposes NestGNN, which integrates a nested alternative graph, GNNs, computational utility specification, and its substitution patterns. Section 4 introduces our experiment setup, while Section 5 presents the empirical results. Finally, Section 6 concludes the study and looks ahead to future research. To facilitate future research, we uploaded this work to the following GitHub repository: https://github.com/urbanailab/GNN_travel_mode_choice. 2 Literature Review Section 2.1 reviews the evolution of DCMs in estimating travel mode choices, especially how to address alternative correlations. Section 2.2 reviews neural network approaches for travel mode analysis, where DNNs predict mode choices without modeling inter-alternative correlations. In contrast, Section 2.3 discusses GNNs, which can capture such correlations but have been applied in only a limited number of travel behavior studies. 2.1 Nested logit model Traditional DCMs, such as MNL models (Ben-Akiva and Lerman, 1985b; Train, 2009), NL models (Wen and Koppelman, 2001), and the Cross-Nested Logit (CNL) models (Kwigizile et al., 2011), have been foundational in travel demand modeling. These models provide a basis for predicting travel mode choices (Dissanayake and Morikawa, 2002), travel frequencies, and activity-based travel patterns (Bowman and Ben-Akiva, 2001), based on the random utility maximization framework (Heiss, 2002; Al-Salih and Eszterg´ ar-Kiss, 2021). In DCMs, alternative correlations are often addressed in either the random or deterministic components of utility functions. One example includes generalized extreme value (GEV) model family (Wen and Koppelman, 2001), the Probit models (Dow and Endersby, 2004), or hybrid models (Ben-Akiva et al., 2002), where correlations are captured by multivariate distributed random errors in the utility function. In the benchmark 2 MNL model, the error terms follow the independent and identically distributed (i.i.d.) Gumbel distribution, and this assumption means the MNL cannot deal with the correlation between the different alternatives (Ben-Akiva and Lerman, 1985b). The NL model is a classical extension of MNL, which partitions correlated alternatives into nests and thus relaxes the IIA assumption (Koppelman and Bhat, 2006; Heiss, 2002). The second approach follows an approximate procedure to explicitly correct the systematic utility with a similarity measure, such that the IIA assumption is relaxed while retaining model tractability. For example, path size logit (PSL) models (Ben-Akiva and Bierlaire, 1999) include in the systematic utility a size term for capturing correlation between alternatives, which can be derived from the aggregation of alternatives. The aggregation",
    "with a similarity measure, such that the IIA assumption is relaxed while retaining model tractability. For example, path size logit (PSL) models (Ben-Akiva and Bierlaire, 1999) include in the systematic utility a size term for capturing correlation between alternatives, which can be derived from the aggregation of alternatives. The aggregation of alternatives method (Ben-Akiva and Lerman, 1985a) assumes a nested structure where each nest corresponds to an aggregate alternative grouping elemental alternatives, whose systematic utility is the expected maximum utility of its elemental alternatives. This method encompasses NL as a special case by assuming Gumbel distribution, and reduces to the PSL if assuming mean utility of the within-nest elemental alternatives are the same (Bovy et al., 2008). However, such approaches are typically limited in their ability to capture complex, high-dimensional interactions. 2.2 Deep learning for choice modeling Machine learning (ML) methods have been increasingly popular for choice analysis because of their robust prediction power and automated feature learning capability. Unlike conventional DCMs that require manual exploration of model specifications under certain modeling assumptions, ML methods automatically capture the intrinsic relation between input attributes and the choices directly from the data (Hillel et al., 2021). Specifically, most ML methods allow non-linear utility specifications for better approximation of commuter utilities, contrasting to the commonly used linear-in-parameter specification in DCMs (Wang et al., 2021b). Among all ML methods, DNN models are particularly powerful owing to their universal approximation power (Wang et al., 2021a). Deep learning offers an alternative data-driven approach for modeling choice behavior and is increasingly drawing interests in this field (van Cranenburgh et al., 2022). Early studies using DNNs for choice modeling mostly focus on predicting individual choices like mode choice (Cantarella and de Luca, 2005), car ownership (Tanwanichkul et al., 2021), and activity choice (Wang et al., 2021b). In these studies, DNNs often outperform traditional models in terms of predictive performance, especially when working with large datasets (Karlaftis and Vlahogianni, 2011; Tanwanichkul et al., 2021; Wang et al., 2020b; Lee et al., 2018). However, these DNN models were considered “black-box” classifiers, so challenges remain regarding hyperparameter sensitivity, non-identifiability, and potential inconsistencies in behavior representation (Feng et al., 2024). Recent studies in developing DNNs for choice modeling focus on improving model interpretability by synergizing economic theories and behavioral assumptions (Cantarella and de Luca, 2005; Subba Rao et al., 1998). These methods could be broadly divided into three categories depending on their level of integration with DCMs: fully integrated, partially integrated, and surrogate. Fully integrated DNN models are essentially their DCM counterparts. (Wang et al., 2020a) crafted DNN architecture with alternative-specific utility functions (ASU-DNN) and embedded the MNL model as its classification layer. Following this notion, (Wong and Farooq, 2021) proposed to correct the systematic utility for cross effects using a residual DNN, which implicitly encodes utilities across all alternatives. Some other studies follow a more conservative approach by partially integrating DNNs into DCMs, such that only part of the model specification is learned by DNNs (Han et al., 2022). For example, DNNs have been used to model systematic taste heterogeneity and augment the",
    "implicitly encodes utilities across all alternatives. Some other studies follow a more conservative approach by partially integrating DNNs into DCMs, such that only part of the model specification is learned by DNNs (Han et al., 2022). For example, DNNs have been used to model systematic taste heterogeneity and augment the handcrafted utility function (Han et al., 2022; Sifringer et al., 2020). Alternatively, DNNs have been used as a surrogate for DCMs to learn behavioral representations, such as the decision rule (van Cranenburgh and Alwosheel, 2019), value of time (van Cranenburgh et al., 2022), and consideration set (Yao and Bekhor, 2022). 3 However, none of these studies has investigated how to capture the inter-alternative correlations through the deep learning framework. Since inter-alternative correlations can be leveraged to improve model performance and interpretation as in the classical framework, this research gap motivates us to explore a deep learning approach with full flexibility in capturing alternative correlation. 2.3 Graph neural networks GNNs can potentially address the limitations above through its inherent network perspective. GNNs encompass three primary tasks, including node classification, link prediction, and graph classification (Hamilton, 2020). GNNs employ a concept analogous to aggregation, specifically the message passing mechanism, which constitutes the core of GNNs (Corso et al., 2024): Each node aggregates information from its neighboring nodes and updates its representation accordingly. Through multi-layer message passing, the model effectively captures long-range dependencies, enabling a more comprehensive understanding of the underlying graph structure. GATs learn the importance of neighboring alternatives through data-driven attention weights, while GCNs model alternative correlations using adjacency-based convolution and Laplacian regularization to enforce smoothness in node representations (Veliˇ ckovi´ c et al., 2018; Kipf and Welling, 2017). To handle the correlation between nodes, other GNN models with different message-passing mechanisms, such as Graph Isomorphism Network (GIN), Message Passing Neural Network (MPNN), and Principal Neighborhood Aggregation (PNA), have also emerged (Xu et al., 2019; Gilmer et al., 2017; Corso et al., 2020). In addition, You et al. (2020) systematically explored all possible combinations of GNN structures, constructing a 12- dimensional design space to combine diverse architectural components (e.g., aggregation functions, activation functions, layer connection methods, etc.). Different from DNN with implicit node dependency, graph topology is used in GNN to explicitly represent the correlation between nodes, such that information from neighboring nodes are aggregated and shared. Owing to its capability in capturing spatial dependency, GNN have been applied for demand prediction (Lin et al.; Tygesen et al.; Wang et al.; Zhou et al., 2020) and choice analysis (Tomlinson and Benson, 2024). However, in previous studies (Tomlinson and Benson, 2024), graph structures have predominantly been constructed to describe individuals’ social networks, while the potential correlations among alternatives have largely been overlooked. The integration of GNNs into choice modeling with an alternatives-based graph structure remains an open research question, warranting further exploration. In summary, existing approaches present three limitations. Classical discrete choice models rely on predefined structures and have limited capacity for handling complex input-output relationships. Deep learning models are more flexible but often lack interpretability and do not explicitly capture correlations among alternatives.",
    "remains an open research question, warranting further exploration. In summary, existing approaches present three limitations. Classical discrete choice models rely on predefined structures and have limited capacity for handling complex input-output relationships. Deep learning models are more flexible but often lack interpretability and do not explicitly capture correlations among alternatives. Although GNNs have recently been applied to choice modeling, their use remains limited and have never been used to capture alternative dependency. This study addresses the gaps by proposing a GNN framework that incorporates utility aggregation over an alternative graph, aiming to balance model flexibility with theoretical interpretability. 3 Methodology In this section, we first highlight two unique properties of the NL model. Next, we define the concept of the alternative graph and propose NestGNN–a GNN architecture that is inspired by the NL model. We further show that NestGNN aligns with the theory of random utility maximization (RUM), generalizes the MNL, NL, and ASU-DNN models, and yet preserves the unique substitution patterns in the NL model. Lastly, we present four NestGNN examples, thus demonstrating the expressive power and the richness of the NestGNN framework. 4 3.1 Reinterpreting nested logit models Letnrepresent the individual index and irepresent the alternative index. For every individual n∈N , each alternative i∈ V is characterized by its utility function Vni(xni,zn), wherex nidenotes the alternative- specific variables of individual nfor alternative i, such as travel time and costs, andz nrepresents individual n’s characteristics such as income and education. The utility of an alternative iis defined as Uni= Vni(xni,zn) +ϵni, in which Vnirepresents the deterministic utility and ϵnithe random one. Under a RUM framework (Manski, 1977; McFadden, 1972), the choice probability of alternativeiis defined as: Pni= Prob(V ni+ϵni> Vnj+ϵnj,∀j∈ V, j̸=i).(1) Whenϵ nifollows an i.i.d. Gumbel distribution, the probability of individualnchoosingiis Pni=eVni P j∈VeVnj.(2) As shown by McFadden (1972), Eq. (2) is a necessary and sufficient condition for the RUM framework, which enables the economic interpretation of deep learning models. When Vniis specified as a linear form such as Vni=βxxni+βzzn, Eq. (2) reduces to an MNL model, which follows the IIA constraint (Luce, 1959). This IIA constraint leads to a proportional substitution pattern, as shown by the ratio of choice probability functions:Pni Pnj=eVni eVnj,∀i, j∈ V , in which the substitution between alternatives iandjis independent of the attributes in the irrelevant alternatives. When the attribute of an alternative mvaries, the proportion between PniandPnjremains constant. This IIA constraint, along with the proportional substitution pattern, has been criticized as lacking flexibility. As in the blue bus and red bus context, such a proportional substitution pattern would be highly unrealistic, failing to capture the market dynamics in the real world. The NL model enhances the flexibility in the substitution pattern by releasing the restrictive IIA constraint in the MNL (McFadden, 1978). Its choice probability function is specified as: Pni=P(i|B k)P(B k|V) =eVni/µk P j∈BkeVnj/µk×(P j∈BkeVnj/µk)µk PK l=1(P j∈BleVnj/µl)µl,(3) where BkandBlrepresent the alternative sets in nests kandl, and µkandµlrepresent their corresponding scale factors (Train, 2009). Although Eq. (3) is the most common NL form, its terms can be reorganized as Eq. (4). The",
    "choice probability function is specified as: Pni=P(i|B k)P(B k|V) =eVni/µk P j∈BkeVnj/µk×(P j∈BkeVnj/µk)µk PK l=1(P j∈BleVnj/µl)µl,(3) where BkandBlrepresent the alternative sets in nests kandl, and µkandµlrepresent their corresponding scale factors (Train, 2009). Although Eq. (3) is the most common NL form, its terms can be reorganized as Eq. (4). The equivalence between Eq. (3) and Eq. (4) has been formally established in our ongoing work (Cheng et al., 2025) and is detailed in Appendix A. Here we would like to reinterpret NL by highlighting two interrelated properties as shown in Eq. (4). Pni=exp\u0010 Vni/µk+ (µ k−1) log(P j∈Bkexp(V nj/µk))\u0011 P m∈Vexp\u0000 Vnm/µl+ (µ l−1) log(P m∈B lexp(V nm/µl))\u0001.(4) NL Property 1.Additive Nest Aggregation.The NL model is characterized by its additive nest utility with log-sum-exponential (LSE) aggregation. This property is a straightforward description of the numerator in Eq. (4), as its first term Vni/µkrepresents the alternative i’s self utility and the second term ( µk−1)log(P j∈Bkexp(Vnj/µk)) is added to the first term by aggregating all the utilities in nest Bkusing a log-sum-exponential (LSE) form with µkfor linear scaling. This property will be leveraged to design our NestGNN models in Section 3.3. NL Property 2.Two-Layer Substitution.The NL model is characterized by its two-layer substitu- tion patterns: proportional substitution of alternatives iandjwithin every nest Bkwhen i, j∈B k, and disproportional substitution of alternativesiandjacross nestsB kandB lwheni∈B kandj∈B l. 5 Mathematically, the two-layer substitution property can be shown straightforwardly. When i, j∈B k, the ratio of choice probabilities Pni Pnj=eVni/µk eVnj/µk,(5) which does not depend on any alternatives other than iandj. When i∈B kandj∈B l, the ratio of choice probabilities equals to: Pni Pnj=exp\u0000 Vni/µk+ (µ k−1) log(P m∈B keVnm/µk)\u0001 exp\u0000 Vnj/µl+ (µ l−1) log(P m∈B leVnm/µl)\u0001.(6) This two-layer substitution property characterizes the uniqueness of NL because it captures how the NL model retains the proportional substitution pattern of MNL within each nest and yet releases the IIA restriction across nests. Eq. (6) releases the IIA constraint only partially because the choice probability ratio depends on all the alternatives within nests BkandBl, but still is independent of the alternatives beyond the two nests. Comparing the two properties, Property 2 can be seen as a consequence of Property 1 because the additive LSE aggregation cancels out in Eq. (5) and remains in Eq. (6). In the following section, we will demonstrate that the NestGNN model can fully replicate this unique two-layer substitution pattern by integrating the concept of alternative graph and message passing, and yet it generalizes the NL model through a much richer model design space. 3.2 Representation of alternative graph The alternative graph is denoted as G= (V,E), where Vrepresents the set of choice alternatives, Eis a set of edges representing the relationship among choice alternatives. When two alternatives are related, an edge exists between them. The edge set Ecan be represented by an adjacency matrix A|V|×|V| , where, aij= 1,∀i, j∈ V , if choice alternatives iandjare connected, and aij= 0 otherwise. In our alternative graph, the edge connections are undirected, so the adjacency matrix is symmetric. Fig. 2 illustrates the concept of an",
    "them. The edge set Ecan be represented by an adjacency matrix A|V|×|V| , where, aij= 1,∀i, j∈ V , if choice alternatives iandjare connected, and aij= 0 otherwise. In our alternative graph, the edge connections are undirected, so the adjacency matrix is symmetric. Fig. 2 illustrates the concept of an alternative graph using four travel modes as an example ( |V|= 4). Each node represents a travel mode, grouped into two categories: vehicle and active travel modes. The edges in the alternative graph are derived from the nested logit structure shown in Fig. 2b, where the two mode categories correspond to distinct nests. By connecting all pairs of alternatives within each nest, the nested structure is transformed into an alternative graph, as shown in Fig. 2c. The NL model is thus represented by an alternative graph in which each nest corresponds to a fully connected subgraph. The representation of the alternative graph (Fig. 2c) has advantages over classical concepts for at least three reasons. First, since the alternative graph does not impose an ex-ante hierarchical nest structure, it can potentially capture much more complex relationships among the alternatives by assigning edges across the subnetworks in Fig. 2c and features for each edge. From a network theoretical perspective, Fig. 2c is a more general and flexible graph representation than Fig. 2b. Second, the alternative graph generalizes the notion of an alternative set through edge-based connections. Fig. 2a represents the four independent alternatives as disconnected nodes, which is a specific case of an alternative graph as aij= 0. Lastly, this alternative graph can be directly used to construct a computational graph for the message passing algorithm in GNN (Fig. 2d), which is illustrated in our Section 3.3 below. 3.3 Graph neural network with nest-specific utility Before introducing NestGNN, we introduce the message passing algorithm in a general-purpose GNN model. For simplicity, we remove the individual index nin the notations below. A general message passing algorithm 6 Automobile Walking Bike Transit(a) Alternative set Automobile Walking Bike Transit (b) Nest structure Automobile Transit WalkingBike (c) Alternative graph structure Automobile TransitBike Walking (d) Message passing on the automobile node Figure 2: Alternative graph representations and message passing algorithm consists of three steps: m(t+1) i,j =M(t)(x(t) i, x(t) j, ei,j), a(t+1) i =A\u0010n m(t+1) i,j,∀j∈ N(i)o\u0011 , x(t+1) i =U\u0010 x(t) i,a(t+1) i\u0011 . The three functions serve three goals. The first equation creates an edge messagem(t+1) i,j by combining two node features; the second equation aggregates the edge messagesm(t+1) i,j around the neighborhoods of nodei; the third equation update the node featurex(t) iasx(t+1) i by integrating the initial node featurex(t) i and the aggregated messagea(t+1) i. The algorithm is initialized with node features x(0) iand ends with x(L) i after L-layer updating. The final node feature x(L) icould be used for predicting output yiwith a readout function R(x(L) i), which can take a simple linear form (e.g., yi=wTx(L) i). This message passing algorithm can successfully aggregate the graph information into individual nodes, and it has been shown to generalize many existing GNN algorithms (Gilmer et al., 2017;",
    "icould be used for predicting output yiwith a readout function R(x(L) i), which can take a simple linear form (e.g., yi=wTx(L) i). This message passing algorithm can successfully aggregate the graph information into individual nodes, and it has been shown to generalize many existing GNN algorithms (Gilmer et al., 2017; Kipf and Welling, 2017). Definition 1.The graph neural network with nest-specific utility (NestGNN) is defined as a GNN with its message passing algorithm applied to an alternative graph consisting of |C|disconnected subnetworks. Its model design space is captured by its hyperparameter space H={L, M, A, U, R} , which are the Number of Layers, Message, Aggregate, Update, and Readout functions. This NestGNN definition combines a general-purpose GNN algorithm and an alternative graph with disconnected components, so it is a unique GNN structure adapted to the choice modeling context. The NestGNNs enable the messages to propagate and aggregate across the alternative nodes while limited to each 7 subnetwork. The message passing process is illustrated in Fig. 2d, where the information of node automobile is aggregated with its neighbor transit, but not with bike or walk, which are disconnected. The formula of NestGNN can be written down explicitly when it has only one layer. Let the updating function take an additive form: U(x(0) i,a(1) i) =ϕ(x(0) i) +A(n m(1) i,j,∀j∈ N(i)o ). Then the utility function and the choice probability functions of the NestGNN become: Vni=ϕ(x ni) +A(n m(1) i,j,∀j∈ N(i)o ),(7) Pni=exp\u0010 ϕ(xni) +A(n m(1) i,j,∀j∈ N(i)o )\u0011 P m∈Vexp\u0010 ϕ(xnm) +A(n m(1) i,m,∀j∈ N(i)o )\u0011.(8) Comparing Eq. (4) and Eq. (8), we can observe the parallel structure in the GNN and NL models, generating mutually beneficial perspectives. Eq. (8) can be seen as a one-step message passing process, aggregating the self-utility ϕ(xni) and the nest-utility A({m(1) i,j,∀j∈ N (i)}), resembling the two properties in NL. The softmax activation function in Eq. (8) ensures that this NestGNN approach is consistent with the RUM framework. Specifically, as the NestGNN design is inspired by the two properties in NL, correspondingly, it also has the following two properties: NSU-GNN Property 1.Additive Nest Aggregation.The NestGNN model is characterized by its additive nest utility with a general aggregation functionA. This is a simple description of the numerator in Eq. (8), as its first term represents the alternative i’s self utility and the second term the nest utility from i’s neighbors. Since each subnetwork in Fig. 2d is fully connected, even one-step neighborhood aggregation here can summarize all the information within a nest. Different from NL, the NestGNN has a more general aggregation function A, rather than limited to LSE function only. This difference enables us to design specific NestGNN examples to test the effectiveness of the LSE aggregation. NSU-GNN Property 2.Two-Layer Substitution.The NestGNN model is characterized by its two-layer substitution patterns, same as the corresponding property in NL. This property highlights the uniqueness of the NestGNN design because it resembles the substitution patterns in NL. The mathematical derivation is also straightforward, similar to the derivation process in NL. When two alternativesi, j∈B k, the ratio of choice probabilities",
    "two-layer substitution patterns, same as the corresponding property in NL. This property highlights the uniqueness of the NestGNN design because it resembles the substitution patterns in NL. The mathematical derivation is also straightforward, similar to the derivation process in NL. When two alternativesi, j∈B k, the ratio of choice probabilities Pni Pnj=eϕ(xni) eϕ(xnj),(9) which does not depend on any alternatives other than iandj. When i∈B kandj∈B l, the ratio of choice probabilities equals to: Pni Pnj=exp\u0010 ϕ(xni) +A(n m(1) i,m,∀m∈ N(i)o )\u0011 exp\u0010 ϕ(xnj) +A(n m(1) j,m,∀m∈ N(j)o )\u0011.(10) The two properties of NestGNN demonstrate the structural controls on a general GNN model family. Instead of using a vast GNN family, we would like to define a relatively restrictive NestGNN, which structurally resembles the classical NL so that its regularity is enhanced and it can potentially generate more interpretable results. Despite the similarities between NL and NestGNN as shown by their properties, we can also compare Eq. (4) and Eq. (8) to identify their crucial differences. First, the NL model uses a low-dimensional edge 8 message passing since the aggregated message A(mi,j) is a one-dimensional utility value wT jxnj/µk, as opposed to a typical high-dimensional message passing approach, where A(mi,j) is typically a element-wise operator, retaining the rich information from the surrounding features. Second, the NL model uses a unique LSE form for neighborhood aggregation, different from the typical min, max, and sum aggregation functions in a typical GNN model. Third, the NL model, as reframed in the GNN framework, heavily relies on parameter sharing, as the scale factor µkis repeatedly used in linear scaling across all the steps. Lastly, the NL model is limited to a one-layer GNN aggregation, rather than repeatedly using GNN aggregations for at least two or three steps, as commonly done in GNN models. Such differences in message dimension, aggregation, parameter sharing, and number of layers prompt us to design variants of NestGNN examples, thus testing performance hypotheses. 3.4 NestGNN examples Besides the similarity, NestGNN also significantly generalizes the classical NL model family because of the inherent richness in its model design space. Here we present that this NestGNN framework can unify MNL, NL, and ASU-DNN models, which appear quite different model families at the first glance. While NestGNN is a restrictive GNN model family, its hyperparameter space provides sufficient flexibility for designing its function forms, yielding the following examples as its unique cases. Example 1.NestGNN reduces to a MNL model when its hyperparameter space Htakes the following form: {L= 0, M(x i) =∅, A=∅, U=∅, R(x i) =wT ix(0) i}. Here the NestGNN does not use message passing algorithm but only initializes the zero-layer node feature x(0) i. Without message passing algorithm, the utility function in the NestGNN enables the inputs from only alternative i’s own attributes. The choice probability function of alternative iequals to Pi= ewT ix(0) i/P j∈VewT jx(0) j, which is the same as the MNL model. Example 2.NestGNN reduces to a ASU-DNN model when its hyperparameter space Htakes the following form:{L= 0, M(x i) =∅, A=∅, U=∅, R(x(0) i) =MLP(x(0) i)}. Again,",
    "attributes. The choice probability function of alternative iequals to Pi= ewT ix(0) i/P j∈VewT jx(0) j, which is the same as the MNL model. Example 2.NestGNN reduces to a ASU-DNN model when its hyperparameter space Htakes the following form:{L= 0, M(x i) =∅, A=∅, U=∅, R(x(0) i) =MLP(x(0) i)}. Again, the NestGNN does not use message passing but only initializes the zero-layer node feature x(0) i. Different from the linear mapping in MNL, here the multilayer perception (a.k.a., feedforward neural networks) is used as the readout function. As a result, the choice probability function of alternative iequals toPi=eMLP(x(0) i)/P j∈VeMLP(x(0) j), which is the same as Wang et al. (2020a)’s work. In both examples, the MNL and ASU-DNN models follow the IIA constraint and thus exhibit the proportional substitution pattern among alternatives. Example 3.NestGNN reduces to a NL model when its hyperparameter space Htakes the following form: L= 1, M(x(0) j) =w⊤ jx(0) j/ µk, A(mi) = (µk−1)LSE j∈N∗(i)(M(x(0) j), U(xi, ai) =M(x(0) i) +A(mi), R(x(1) i) = x(1) i}. This example demonstrates the crucial connection between the NL model and the NestGNN model. It demonstrates that the NL model can be decomposed following the GNN steps. First, an edge message is created with a linear transformation asM(x(0) j) =w⊤ jx(0) j. Second, the edge messages are aggregated using the log-sum-exponential (LSE) function from the neighborhoods of the target node i, as shown by the equation A(mi) = (µk−1)LSE j∈N∗(i)(M(x(0) j). Third, the node feature xiis updated by combining the node i’s self message M(x(0) i) and the neighborhood information A(mi), as shown by Equation U(xi, ai) =M(x(0) i)+A(mi). Lastly, the updated node message x(1) ican be directly used as the utility value for calculating the choice 9 probability function. Connecting all four steps, the utility value and the choice probability function of alternativeiequals to: Vni=wT ixni/µk+ ((µ k−1)LSE j∈N∗(i)(wT jxnj/µk)),(11) Pni=exp\u0000 wT ixni/µk+ ((µ k−1)LSE j∈N∗(i)(wT jxnj/µk)\u0001 P m∈Vexp\u0000 wTmxnm/µl+ ((µ l−1)LSE j∈N∗(i)(wTmxnm/µl)\u0001.(12) which is the same as the classical NL model in Eq. (4). This GNN interpretation of NL leverages NL’s property of additive LSE aggregation to illustrate their equivalence. Example 4.NestGNN can be specified as an example with high-dimensional message passing along edges when its hyperparameter space Htakes the following form: {L= 1, M(x(0) j) =WT jx(0) j, A(mi) = LSE j∈N∗(i)(M(x(0) j), U(x i, ai) =CONCAT(M(x(0) i), ai), R(x(1) i) =wT ix(1) i}. Different from Example 3, here the LSE aggregation is applied in an element-wise manner, thus enabling the message passing in a high-dimensional space. The update function concatenates the transformed input feature of node iand its edge message ai, incorporating both individual and contextual information. Only in the last readout function, the dimension of the node feature xiis reduced to one dimension for computing choice probabilities. Using the specification in Example 4, the utility value and the choice probability functions are: Vni=wT 1i(WTxni) +wT 2iLSE j∈N∗(i)(WTxnj),(13) Pni=exp\u0000 wT 1i(WTxni) +wT 2iLSE j∈N∗(i)(WTxnj)\u0001 P m∈Vexp\u0000 wT 1m(WTxnm) +wT 2mLSE j∈N∗(m)(WTxnm)\u0001.(14) Overall, the four examples demonstrate the flexibility of the design space in the NestGNN model family. The",
    "Using the specification in Example 4, the utility value and the choice probability functions are: Vni=wT 1i(WTxni) +wT 2iLSE j∈N∗(i)(WTxnj),(13) Pni=exp\u0000 wT 1i(WTxni) +wT 2iLSE j∈N∗(i)(WTxnj)\u0001 P m∈Vexp\u0000 wT 1m(WTxnm) +wT 2mLSE j∈N∗(m)(WTxnm)\u0001.(14) Overall, the four examples demonstrate the flexibility of the design space in the NestGNN model family. The first three examples demonstrate how the NestGNN models reduce to the existing MNL, NL, and ASU-DNN mdoels, while the fourth example showcases how the NestGNN can generalize the existing models and create new specification. Due to the expressive power of NestGNN, we anticipate that this model family will achieve higher predictive performance than the benchmark models, while maintaining the regularity in its substitution patterns, as demonstrated by our empirical experiments below. 4 Experiment Design We use theLondon Passenger Dataset(Hillel et al., 2018), which includes individual- and alternative-specific attributes for modeling travel mode choices. After data processing, the total number of samples is 81,086 and each sample records a person’s choice among four transportation modes, along with individual characteristics such as age, gender, household income, number of vehicles, household size, and education level. To facilitate efficient computational comparison, we randomly sampled 8,000 trips from the full dataset. The distribution of travel modes is relatively balanced, with automobile (45.0%) and transit (34.8%) comprising the majority of choices, while walking (17.3%) and bike (2.9%) are comparatively underrepresented. Table 1 presents the summary statistics of the sampled dataset, travel times and costs differ by mode, with walking time having the longest average travel time with greatest variability. The average commuter is 39.6 years old, lives in a household of 2.4 people, and has access to approximately one vehicle. The sampled data were subsequently partitioned into training and testing sets using an 80/20 split. To examine the influence of graph assumptions on model performance, we designed three alternative graphs by assigning the four travel modes—automobile, transit, bike, and walking (ordered as indices 0 through 3)—to different subgraphs. The three nesting structures can be denoted as the following group label 10 Table 1: Summary Statistics of Sample Dataset (N = 8,000) Variable Mean Std Min 25% 50% 75% Max Age (years) 39.61 19.17 5.00 26.00 38.00 53.00 94.00 Male (0/1) 0.46 0.50 0.00 0.00 0.00 1.00 1.00 Number of vehicle 0.98 0.75 0.00 0.00 1.00 2.00 2.00 Household size 2.43 1.25 1.00 1.00 2.00 3.00 9.00 Transit time (min) 27.98 18.74 1.00 13.40 23.55 38.42 141.83 Walking time (min) 68.02 67.02 1.72 21.00 43.56 90.61 528.27 Driving time (min) 17.02 15.22 0.53 6.42 11.55 22.27 120.65 Biking time (min) 21.81 21.11 0.43 6.93 14.03 29.27 160.35 Transit cost ($) 1.55 1.51 0.00 0.00 1.50 2.40 10.49 Driving cost ($) 1.90 3.47 0.02 0.28 0.58 1.30 15.38 vectors: [0, 0, 1, 2], where automobile and transit are grouped while bike and walking are separated, as shown in Fig. 3c and the corresponding graph structure in Fig. 3d; [0, 0, 0, 1], where automobile, transit, and bike are grouped with walking isolated, as shown in Fig. 3a and Fig. 3b; and [0, 0, 1, 1], where automobile and transit form",
    "bike and walking are separated, as shown in Fig. 3c and the corresponding graph structure in Fig. 3d; [0, 0, 0, 1], where automobile, transit, and bike are grouped with walking isolated, as shown in Fig. 3a and Fig. 3b; and [0, 0, 1, 1], where automobile and transit form one group while bike and walking form another, as illustrated in Fig. 2b and Fig. 2c. In NL, alternatives assigned to the same group are assumed to share a common unobserved utility component, capturing substitution patterns within the nest. In NestGNN, the group assignment defines the adjacency structure: alternatives within the same subgraph are fully connected, enabling message passing among them, while alternatives across the subnetworks remain disconnected. Besides graph structures, we also conducted an exhaustive grid search over the hyperparameters of NestGNNs, including aggregation function, update mechanism, readout function, number of layers, and hidden units—under each of these structural configurations. All other hyperparameters, such as the number of training epochs, learning rate, and batch size, were kept constant throughout the experiments at 100, 0.001, and 64, respectively (with MNL and NL trained using the full batch size). For comparison, we also evaluated MNL, NL, and ASU-DNN models, which adopt the same NestGNN structure but differ in the hyperparameter settings of the four components, as illustrated in Section 3.4. As shown in Table 2, a total of 294 models were evaluated, including 288 NestGNNs with three graph structures, four ASU-DNNs, one MNL model, and three NL models under three nesting structures. Table 2: Summary of model configurations in the experiments Model Type Hyperparameter Configuration Total Models NestGNN 3 graph structures; Aggregation function:mean,LSE,max Update mechanism:concat,plus Readout function:linear,mlp Number of message passing layers: 1, 2 Hidden units: 1, 64, 128, 512288 ASU-DNN Hidden units: 1, 64, 128, 512 4 NL 3 nesting structures 3 MNL – 1 Total–294 11 Automobile Walking Bike Transit(a) Nest structure under the nest ids of [0,0,0,1] Automobile Transit WalkingBike (b) Alternative graph under the nest ids of [0,0,0,1] Automobile Walking Bike Transit (c) Nest structure under the nest ids of [0,0,1,2] Automobile Transit WalkingBike (d) Alternative graph under the nest ids of [0,0,1,2] Figure 3: Nest structure and alternative graph design using different nest ids 5 Results 5.1 NestGNNs outperforming benchmark models After comparing all 294 models, we present the performance of eight representative models across four model classes. Table 3 compares representative models, including their architecture settings and evaluation metrics. Specifically, we report the results for the MNL model, the NL model with the highest performance, the top-performing ASU-DNN configuration, and the top five NestGNNs, selected based on the log-likelihood in the testing set. The evaluation metrics include log-likelihood (LLL), F1 score (F1), and accuracy (Acc) on both training and testing sets. Table 3 yields three key findings. First, machine learning models (i.e., ASU-DNN and NestGNN) consistently outperform traditional discrete choice models (MNL and NL) across all evaluation metrics. Both the ASU-DNN and all five top-performing NestGNN configurations achieve higher log-likelihoods, F1 scores, and accuracies than MNL and NL on the testing set, indicating superior predictive performance. For",
    "First, machine learning models (i.e., ASU-DNN and NestGNN) consistently outperform traditional discrete choice models (MNL and NL) across all evaluation metrics. Both the ASU-DNN and all five top-performing NestGNN configurations achieve higher log-likelihoods, F1 scores, and accuracies than MNL and NL on the testing set, indicating superior predictive performance. For instance, the highest-performing NestGNN model achieves a test log-likelihood of –625.17 and an accuracy of 0.729, compared to –692.36 and 0.691 for MNL, and –688.54 and 0.686 for NL. This represents a 9.7% improvement over MNL and a 9.2% improvement over NL in log-likelihood, and a 5.5% and 6.3% increase in accuracy, respectively. It demonstrates that the graph-based structure of NestGNN can capture inter- alternative correlations via message passing, enhancing its predictive performance even comparing to the existing ASU-DNN benchmark. This finding is aligns with the past studies, where researchers found that machine learning models consistently outperform the classical DCMs in predictive performance (Wang et al., 2021a; Hillel et al., 2021). 12 Table 3: Comparison of MNL, NL, ASU-DNN, and top five NestGNN models AttributeMNL NL ASU- DNNTop-1 NestGNNTop-2 NestGNNTop-3 NestGNNTop-4 NestGNNTop-5 NestGNN Hyperparameter Configuration Aggregation – LSE – mean mean max mean max Update – plus – plus plus concat plus concat Readout linear identity mlp mlp mlp linear mlp linear Layers 0 1 0 2 2 1 1 2 Hidden Units 0 1 512 64 128 512 128 64 Nest IDs 0-0-1-2 0-0-1-1 0-0-0-1 0-0-1-1 0-0-1-1 0-0-1-1 0-0-1-1 0-0-1-1 Performance Metrics Train LLL -5197.52 -5222.88 -4755.34 -4674.48 -4560.27 -4646.83 -4640.77-4510.74 Train F1 0.523 0.524 0.543 0.550 0.555 0.551 0.5500.559 Train Acc 0.710 0.713 0.730 0.744 0.749 0.741 0.7450.750 Test LLL -692.36 -688.54 -663.01-625.17-626.12 -628.21 -630.42 -630.62 Test F1 0.508 0.502 0.519 0.5300.5360.540 0.539 0.535 Test Acc 0.691 0.686 0.703 0.7190.7290.7270.7290.720 Second, the graph assumptions - either in the form of alternative graphs in NestGNNs or nesting structures in NL models - have a notable influence on model performance. For example, all top-5 performing NestGNN models have the same nest-ids [0011], which separates motorized (automobile and transit) and non-motorized (bike and walking) modes as two subnetworks as shown in Fig. 2c. The highest-performing NL model also uses the corresponding nesting structure in Fig. 2b, suggesting that this graph representations captures behavioral intuition more effectively than other two graph structures in Fig. 3. This finding is consistent with past research showing that an appropriate nest structure is essential for obtaining meaningful parameter estimates in NL models (Heiss, 2002). In fact, this separation between motorized vs. non-motorized nests is the most common practice in NL models. Third, besides graph structures, other hyperparameters such as the readout function and number of layers also contribute to model performance. All top-5 NestGNN models use an MLP readout function rather than a linear one, highlighting the benefit of non-linear transformations in the final utility representation. The number of layers also plays an important role: for example, the top-performing model uses 2 layers, while other high-performing models (e.g., Top-4) achieve strong results with only 1 layer. By contrast, the ASU-DNN model can be viewed as a zero-layer NestGNN",
    "of non-linear transformations in the final utility representation. The number of layers also plays an important role: for example, the top-performing model uses 2 layers, while other high-performing models (e.g., Top-4) achieve strong results with only 1 layer. By contrast, the ASU-DNN model can be viewed as a zero-layer NestGNN with a MLP (512 hidden units), yet it yields lower accuracy (0.703) and lower test log-likelihood (–663.01) than all top-5 NestGNN models. That’s because while the readout function shapes how information becomes the final utility and affects substitution pattern, deeper GNN layers can effectively propagate information further across alternatives. These results demonstrate the advantages of our NestGNN models. By incorporating a graph-based structure over alternatives, NestGNN captures inter-alternative correlations through message passing, enriching the utility representation beyond what NL and ASU-DNN can provide. Specifically, NestGNN combines the aggregation mechanism of NL with the MLP readout function of ASU-DNN, leveraging the strengths of both structural modeling and flexible function approximation. 5.2 Comparing elasticities of NestGNN and Nested Logit To analyze substitution patterns, we first compare the elasticities of the top-ranked MNL, ASU-DNN, NL, and NestGNN models. The elasticity results are summarized in Table 4, Table 5 and Table 6. Each row 13 corresponds to a mode-specific variable (e.g., automobile time, transit cost), and each column represents the elasticity of the choice probability for a particular mode, holding all other variables at their average values. Each cell reports the mean (standard deviation) of the elasticity across all the test samples. As demonstrated in past studies (Wang et al., 2020a), ASU-DNN serves as the counterpart of MNL in the DNN world because it resembles MNL in its elasticity patterns. As shown in Table 4, the elasticities of automobile, bike, and walking with respect to transit time are consistently 0.48 (std = 0.63) in the MNL model and 0.62 (std = 1.03) in the ASU-DNN model. Similar patterns appear across other variables. For example, increases in automobile cost lead to equal elasticities of 0.10 for transit, bike, and walking in MNL, and 0.02 in ASU-DNN. This pattern arises because both models treat alternatives as independent and identically distributed (IID), lacking structural mechanisms for inter-mode interaction. Consequently, changes in a mode-specific variable (e.g., transit time) affect all other modes in a uniform and symmetric manner,reflecting the IIA property in both MNL and ASU-DNN. Table 4: Elasticity Comparison: MNL vs. ASU-DNN MNL ASU-DNN Automobile Transit Bike Walking Automobile Transit Bike Walking Auto time -0.94 (1.33) 0.69 (0.76) 0.69 (0.76) 0.69 (0.76) -0.90 (0.89) 0.78 (0.86) 0.78 (0.86) 0.78 (0.86) Auto cost -0.24 (0.54) 0.08 (0.12) 0.08 (0.12) 0.08 (0.12) -0.26 (0.93) -0.00 (0.13) -0.00 (0.13) -0.00 (0.13) Transit time 0.48 (0.63) -0.73 (0.66) 0.48 (0.63) 0.48 (0.63) 0.62 (1.03) -0.80 (1.13) 0.62 (1.03) 0.62 (1.03) Transit cost 0.14 (0.25) -0.23 (0.25) 0.14 (0.25) 0.14 (0.25) -0.01 (0.31) -0.12 (0.35) -0.01 (0.31) -0.01 (0.31) Bike time 0.35 (0.39) 0.35 (0.39) -7.87 (8.20) 0.35 (0.39) 0.44 (0.50) 0.44 (0.50) -5.57 (4.62) 0.44 (0.50) Walk time 0.05 (0.07) 0.05 (0.07) 0.05 (0.07) -1.87 (1.88) 0.05 (0.09) 0.05 (0.09) 0.05 (0.09) -2.33",
    "(0.25) -0.23 (0.25) 0.14 (0.25) 0.14 (0.25) -0.01 (0.31) -0.12 (0.35) -0.01 (0.31) -0.01 (0.31) Bike time 0.35 (0.39) 0.35 (0.39) -7.87 (8.20) 0.35 (0.39) 0.44 (0.50) 0.44 (0.50) -5.57 (4.62) 0.44 (0.50) Walk time 0.05 (0.07) 0.05 (0.07) 0.05 (0.07) -1.87 (1.88) 0.05 (0.09) 0.05 (0.09) 0.05 (0.09) -2.33 (2.49) Similar to the correspondence between ASU-DNN and MNL, NestGNNs are the counterpart of NL models in the GNN family because of their similarities in elasticity patterns. As shown in Table 5, in the NL model, the elasticity of bike and walking (both in the non-motorized component) with respect to automobile time is 0.01 (std = 0.01), whereas transit, which belongs to the same component as automobile, has a much higher elasticity of 0.78 (std = 0.78). In the Top-1 NestGNN model, a similar component-based pattern is observed: bike and walking have equal elasticities of 0.34 (std = 1.07) with respect to automobile time, while transit shows a higher elasticity of 1.00 (std = 0.95). Therefore in both NL and NestGNN, the alternatives within the same component, i.e., nest in NL or subnetwork in NestGNN, tend to have identical elasticities with respect to an alternative-specific variable in another component, while alternatives’ elasticities in the same component are different. This finding is no surprise because the NestGNNs were designed to replicate NL’s unique substitution patterns, as illustrated in Section 3.3. Table 5: Elasticity Comparison: NL vs. Top-1 NestGNN NL Top-1 NestGNN Automobile Transit Bike Walking Automobile Transit Bike Walking Auto time -0.92 (1.42) 0.78 (0.78) 0.01 (0.01) 0.01 (0.01) -1.03 (1.17) 1.00 (0.95) 0.34 (1.07) 0.34 (1.07) Auto cost -0.21 (0.49) 0.08 (0.13) 0.00 (0.00) 0.00 (0.00) -0.12 (0.41) 0.02 (0.12) 0.04 (0.20) 0.04 (0.20) Transit time 0.60 (0.72) -0.76 (0.78) 0.01 (0.01) 0.01 (0.01) 0.88 (1.36) -0.90 (0.86) 0.49 (1.33) 0.49 (1.33) Transit cost 0.13 (0.19) -0.15 (0.18) 0.00 (0.00) 0.00 (0.00) -0.08 (0.57) -0.24 (0.51) 0.07 (0.71) 0.07 (0.71) Bike time 0.28 (0.31) 0.28 (0.31) -6.29 (6.55) 0.28 (0.31) 0.11 (0.17) 0.11 (0.17) -0.83 (0.62) -0.17 (0.31) Walk time 0.01 (0.02) 0.01 (0.02) 0.01 (0.02) -0.60 (0.60) 0.36 (0.44) 0.36 (0.44) -2.84 (2.21) -1.24 (1.48) Our experiments also demonstrate that a vast number of NestGNN models consistently represent the same elasticity patterns as above.Table 6 compares the elasticity results of the Top-2 NestGNN model with 14 the average elasticities computed across the top five NestGNN models. In both cases, the Top-2 NestGNN and the Top-5 ensemble NestGNN have the same elasticity patterns as the Top-1 NestGNN and NL models. For example, in the average NestGNN results, the elasticity of choosing transit with respect to automobile travel time is 1.04 (std = 0.98). It means that, holding other factors constant, a 1% increase in automobile travel time is associated with an average 1.04% increase in the probability of choosing transit, with a standard deviation of 0.98 across observations. Compared with the Top-2 and Top-1 NestGNN models, which report elasticities of 1.07 (std = 1.02) and 1.00 (std = 0.95) respectively, the average results show only marginal differences. Similar patterns are observed across",
    "1.04% increase in the probability of choosing transit, with a standard deviation of 0.98 across observations. Compared with the Top-2 and Top-1 NestGNN models, which report elasticities of 1.07 (std = 1.02) and 1.00 (std = 0.95) respectively, the average results show only marginal differences. Similar patterns are observed across other variables, where the average elasticities remain close to those of the top individual models. Table 6: Elasticity Comparison: Top-2 NestGNN vs. Average NestGNN Top-2 NestGNN Average NestGNN Automobile Transit Bike Walking Automobile Transit Bike Walking Auto time -1.25 (2.06) 1.07 (1.02) 0.39 (1.54) 0.39 (1.54) -1.08 (1.26) 1.04 (0.98) 0.48 (1.01) 0.48 (1.01) Auto cost -0.15 (0.70) 0.02 (0.19) -0.02 (0.39) -0.02 (0.39) -0.16 (0.59) 0.01 (0.15) 0.03 (0.24) 0.03 (0.24) Transit time 0.99 (1.80) -0.98 (1.02) 0.62 (1.69) 0.62 (1.69) 0.88 (1.30) -0.96 (0.97) 0.51 (1.23) 0.51 (1.23) Transit cost -0.04 (0.62) -0.21 (0.55) -0.03 (0.80) -0.03 (0.80) -0.08 (0.49) -0.22 (0.49) 0.09 (0.55) 0.09 (0.55) Bike time 0.08 (0.12) 0.08 (0.12) -0.76 (0.67) -0.04 (0.21) 0.08 (0.12) 0.08 (0.12) -0.30 (0.89) -0.02 (0.37) Walk time 0.36 (0.46) 0.36 (0.46) -3.19 (2.71) -1.59 (2.24) 0.39 (0.45) 0.39 (0.45) -3.63 (2.85) -1.57 (2.02) 5.3 Comparing substitution patterns of NestGNN and Nested Logit Comparing to elasticities, it is more intuitive to demonstrate the correspondence between NestGNNs and NL models by visualizing their substitution patterns. In Fig. 4, Fig. 5, and Fig. 6, solid lines represent the choice probabilities varying with automobile costs, and dashed lines visualize the choice probability ratios between pairs of travel modes. Similar to the findings above, ASU-DNN resembles MNL’s substitution pattern as shown by the flat lines of choice probability ratios. In Fig. 4, as the cost of automobile increases, the predicted probability of choosing automobile (the blue solid line) decreases, while the probabilities of choosing transit, bike, and walking (the yellow, green, and red solid lines, respectively) all increase. However, the relative proportions among the three non-automobile modes remain unchanged. The choice probability ratios between transit, bike, and walking stay constant across the full range of automobile costs, as visually shown by the falt blue, yellow, and green dashed lines. This invariance property holds the same to both MNL and ASU-DNN models, reflecting the shared IIA constraint. 15 0.0 0.2 0.4 0.6 0.8 1.0 Automobile cost0.00.20.40.60.8Choice probability 010203040506070 Probability ratio Automobile Transit Bike Walking Transit/Walking Bike/Walking Transit/Bike(a) MNL 0.0 0.2 0.4 0.6 0.8 1.0 Automobile cost0.00.20.40.6Choice probability 051015202530 Probability ratio Automobile Transit Bike Walking Transit/Walking Bike/Walking Transit/Bike (b) ASU-DNN Figure 4: Substitution patterns in MNL and ASU-DNN Visually, the NestGNN resembles NL in its unique two-layer substitution pattern: when the mode-specific variable in one component (e.g., automobile cost) changes, the probability ratio among modes within the other component (e.g., bike/walking) remains constant. As reflected in Fig. 5, the dashed lines representing cross-nest probability ratios (e.g., transit/walking and transit/bike, shown as the blue and green dashed lines) change significantly, while the yellow dashed line—representing the within-nest ratio between bike and walking—remains relatively stable. This substitution behavior aligns with the elasticity results in Table 5, where transit exhibits a",
    "the dashed lines representing cross-nest probability ratios (e.g., transit/walking and transit/bike, shown as the blue and green dashed lines) change significantly, while the yellow dashed line—representing the within-nest ratio between bike and walking—remains relatively stable. This substitution behavior aligns with the elasticity results in Table 5, where transit exhibits a different response compared to bike and walking when automobile-specific variables change. This substitution pattern holds the same across NL and NestGNN in Fig. 5, demonstrating the success of NestGNN in replicating NL’s substitution patterns. However, the NL model appears more regular than the Top-1 NestGNN model. Within the NL model, the ratios of transit/walking and transit/bike exhibit a strictly monotonic decline as automobile cost increases, a pattern consistent with the theoretical implication of Eq. (10). In contrast, the corresponding curves in the Top-1 NestGNN model display non-monotonic variations with noticeable fluctuations, due to its nonlinear readout function rather than a linear specification. 16 0.0 0.2 0.4 0.6 0.8 1.0 Automobile cost0.00.20.40.60.8Choice probability 0255075100125150 Probability ratio Automobile Transit Bike Walking Transit/Walking Bike/Walking Transit/Bike(a) NL 0.0 0.2 0.4 0.6 0.8 1.0 Automobile cost0.00.10.20.30.40.50.60.7Choice probability 246810121416 Probability ratio Automobile Transit Bike Walking Transit/Walking Bike/Walking Transit/Bike (b) Top-1 NestGNN Figure 5: Substitution patterns in NL and Top-1 NestGNN This irregularity can be mitigated by model ensemble because the average NestGNN curves demonstrate stable and consistent substitution patterns over the top-performing NestGNN models. The average NestGNN in Figure 6b is created by averaging the choice probability and probability ratio curves of the top five NestGNN models across the same auto-cost range. Compared with Fig. 5b and Fig. 6a, the average NestGNN curves exhibit a more smooth and regular trend as automobile cost increases: (1) the choice probability of automobile decreases while those of the other three modes increase; and (2) the probability ratio curves are notably smoother than those from the Top-1 and Top-2 NestGNN models. This result indicates that averaging across multiple model specifications enhances the stability of the estimated substitution patterns. This is because individual models capture sample-specific noise, leading to fluctuations in predictions. Averaging reduces these fluctuations by canceling out uncorrelated errors, leaving a more stable and smoother estimate (Breiman, 1996). 0.0 0.2 0.4 0.6 0.8 1.0 Automobile cost0.00.20.40.60.8Choice probability 2.55.07.510.012.515.017.5 Probability ratio Automobile Transit Bike Walking Transit/Walking Bike/Walking Transit/Bike (a) Top-2 NestGNN 0.0 0.2 0.4 0.6 0.8 1.0 Automobile cost0.00.10.20.30.40.50.60.7Choice probability 0510152025 Probability ratio Automobile Transit Bike Walking Transit/Walking Bike/Walking Transit/Bike (b) Average NestGNN Figure 6: Substitution patterns in Top-2 NestGNN and average NestGNN 17 As a summary, our empirical results demonstrate the success of the NestGNN design for mode choice analysis. On the one side, it retains the regularity of the substitution and elasticity patterns by resembling the two-layer substitution patterns in the NL model. On the other side, it presents significant flexibility in model specification due to the high-dimensional hyperparameter space. While we tested only three NL specifications, we can easily examine hundreds of NestGNN models, all of which retain the core substitution properties of NL models. Naturally, such a large model design space in NestGNNs leads to higher predictive performance.",
    "flexibility in model specification due to the high-dimensional hyperparameter space. While we tested only three NL specifications, we can easily examine hundreds of NestGNN models, all of which retain the core substitution properties of NL models. Naturally, such a large model design space in NestGNNs leads to higher predictive performance. As ASU-DNN serves as the counterpart of MNL in the DNN model family, here we present NestGNN as the counterpart of NL through the innovative concept of alternative graph and message passing algorithm. 6 Conclusions While previous studies have introduced DNNs for discrete choice modeling, their ability to account for inter-alternative dependence remains limited. This study proposes NestGNN, a GNN framework for discrete choice modeling, which captures the inter-alternative correlations through an alternative graph and message passing algorithms. The NestGNNs can be specified along five key components—number of layers, message function, aggregation function, update function, and readout function—enabling highly flexible model design. By appropriately configuring these components, NestGNN can replicate classical models such as MNL, NL, and ASU-DNN as special cases, while also extending beyond them to capture richer substitution patterns. Empirically, NestGNN demonstrates superior predictive performance, consistently outperforming both traditional and deep learning baselines in terms of accuracy and log-likelihood, indicating strong generalization capability. Furthermore, elasticity analysis and substitution patterns extracted from NestGNN align closely with those of the NL model, indicating that NestGNN not only improves performance but also preserves meaningful economic interpretation. Despite the promising results, this study has several limitations. First, the alternative graph in NestGNN is pre-specified and fixed across individuals, rather than learned directly from data. This design prioritizes interpretability and leverages domain knowledge (e.g., known similarities between bike and walking), but may constrain the model’s ability to capture latent or context-dependent correlations. Second, although our experimental framework supports constructing sample-specific alternative graphs—where the connections between alternatives vary by individual or trip context—we did not explore this direction for the sake of modeling simplicity and comparability. Therefore, this work creates a large number of future research opportunities. While we fix the graph structure across all the individuals, future studies could investigate how to automatically learn the alternative graph structures or account for more individualized substitution patterns. Additionally, such GNN models could become more heterogeneous by incorporating dynamic or sample-specific graph topologies across contexts or population segments. Researchers could also explore theoretical connections between utility maximization principles and GNN architectures, thus further enriching AI models’ interpretability and policy relevance from a theoretical perspective. Overall, this work presents opportunities for researchers to advance the methodological frontier of travel demand modeling by synergizing DCM theory, network perspectives, or deep learning. Contributions of the authors Yuqi Zhou: Writing – review & editing, Writing – original draft, Visualization, Validation, Methodology, Investigation, Formal analysis, Data curation. Zhanhong Cheng: Writing – review & editing, Writing – 18 original draft , Validation, Methodology, Investigation. Lingqian Hu: Writing – review & editing, Supervision, Project administration. Yuheng Bu: Writing – review & editing. Shenhao Wang: Writing – review & editing, Writing – original draft, Validation, Supervision, Project administration, Methodology, Investigation, Conceptualization. Acknowledgment The authors acknowledges the",
    "editing, Writing – 18 original draft , Validation, Methodology, Investigation. Lingqian Hu: Writing – review & editing, Supervision, Project administration. Yuheng Bu: Writing – review & editing. Shenhao Wang: Writing – review & editing, Writing – original draft, Validation, Supervision, Project administration, Methodology, Investigation, Conceptualization. Acknowledgment The authors acknowledges the support from the Research Opportunity Seed Fund 2023 at the University of Florida and the U.S. Department of Energy’s Office of Energy Efficiency and Renewable Energy (EERE) under the Vehicle Technology Program Award Number DE-EE0011186. The views expressed herein do not necessarily represent the views of the U.S. Department of Energy or the United States Government. The authors acknowledge the early discussions with Dr. Kara Kockelman, Dr. Joan Walker, and Dr. Jinhua Zhao in the research seminars at UT Austin, UC Berkeley, and MIT. The authors also acknowledge the support from Dr. Yao Rui and Siqi Feng at the early stage of this research. References Al-Salih, W.Q., Eszterg´ ar-Kiss, D., 2021. Linking Mode Choice with Travel Behavior by Using Logit Model Based on Utility Function. Sustainability 13, 4332. doi:10.3390/su13084332. Ben-Akiva, M., Bierlaire, M., 1999. Discrete Choice Methods and their Applications to Short Term Travel Decisions, in: Hall, R.W. (Ed.), Handbook of Transportation Science. Springer US, Boston, MA, pp. 5–33. doi: 10.1007/ 978-1-4615-5203-1_2. Ben-Akiva, M., Mcfadden, D., Train, K., Walker, J., Bhat, C., Bierlaire, M., Bolduc, D., Boersch-Supan, A., Brownstone, D., Bunch, D.S., Daly, A., De Palma, A., Gopinath, D., Karlstrom, A., Munizaga, M.A., 2002. Hybrid Choice Models: Progress and Challenges. Marketing Letters 13, 163–175. doi:10.1023/A:1020254301302. Ben-Akiva, M.E., Lerman, S.R., 1985a. Aggregation and Sampling of Alternatives, in: Discrete Choice Analysis: Theory and Application to Travel Demand. MIT Press, pp. 253–275. Ben-Akiva, M.E., Lerman, S.R., 1985b. Multinomial Choice, in: Discrete Choice Analysis: Theory and Application to Travel Demand. MIT Press, pp. 100–128. Bovy, P.H.L., Bekhor, S., Prato, C.G., 2008. The Factor of Revisited Path Size: Alternative Derivation. Transportation Research Record 2076, 132–140. doi:10.3141/2076-15. Bowman, J.L., Ben-Akiva, M.E., 2001. Activity-based disaggregate travel demand model system with activity schedules. Transportation Research Part A: Policy and Practice 35, 1–28. doi:10.1016/S0965-8564(99)00043-9. Breiman, L., 1996. Bagging predictors. Machine learning 24, 123–140. Cantarella, G.E., de Luca, S., 2005. Multilayer feedforward networks for transportation mode choice analysis: An analysis and a comparison with random utility models. Transportation Research Part C: Emerging Technologies 13, 121–155. doi:10.1016/j.trc.2005.04.002. Cheng, Z., Hu, L., Bu, Y., Zhou, Y., Wang, S., 2025. Graph neural networks for residential location choice: connection to classical logit models. URL:https://arxiv.org/abs/2507.21334,arXiv:2507.21334. Corso, G., Cavalleri, L., Beaini, D., Li` o, P., Veliˇ ckovi´ c, P., 2020. Principal Neighbourhood Aggregation for Graph Nets. doi:10.48550/arXiv.2004.05718,arXiv:2004.05718. Corso, G., Stark, H., Jegelka, S., Jaakkola, T., Barzilay, R., 2024. Graph neural networks. Nature Reviews Methods Primers 4, 17. Dissanayake, D., Morikawa, T., 2002. Household Travel Behavior in Developing Countries: Nested Logit Model of Vehicle Ownership, Mode Choice, and Trip Chaining. Transportation Research Record 1805, 45–52. doi: 10.3141/ 1805-06. Dow, J.K., Endersby, J.W., 2004. Multinomial probit and multinomial logit: A comparison of choice models for voting research. Electoral Studies 23, 107–122. doi:10.1016/S0261-3794(03)00040-4. 19 Feng, S., Yao, R., Hess, S., Daziano,",
    "Nested Logit Model of Vehicle Ownership, Mode Choice, and Trip Chaining. Transportation Research Record 1805, 45–52. doi: 10.3141/ 1805-06. Dow, J.K., Endersby, J.W., 2004. Multinomial probit and multinomial logit: A comparison of choice models for voting research. Electoral Studies 23, 107–122. doi:10.1016/S0261-3794(03)00040-4. 19 Feng, S., Yao, R., Hess, S., Daziano, R.A., Brathwaite, T., Walker, J., Wang, S., 2024. Deep neural networks for choice analysis: Enhancing behavioral regularity with gradient regularization. Transportation Research Part C: Emerging Technologies 166, 104767. doi:10.1016/j.trc.2024.104767,arXiv:2404.14701. Gilmer, J., Schoenholz, S.S., Riley, P.F., Vinyals, O., Dahl, G.E., 2017. Neural message passing for Quantum chemistry, in: Proceedings of the 34th International Conference on Machine Learning - Volume 70, JMLR.org, Sydney, NSW, Australia. pp. 1263–1272. Hamilton, W.L., 2020. Introduction, in: Synthesis Lectures on Artificial Intelligence and Machine Learning. volume 14, pp. 1–7. Han, Y., Pereira, F.C., Ben-Akiva, M., Zegras, C., 2022. A neural-embedded discrete choice model: Learning taste representation with strengthened interpretability. Transportation Research Part B: Methodological 163, 166–186. doi:10.1016/j.trb.2022.07.001. Heiss, F., 2002. Structural Choice Analysis with Nested Logit Models. The Stata Journal 2, 227–252. doi: 10.1177/ 1536867X0200200301. Hillel, T., Bierlaire, M., Elshafie, M.Z.E.B., Jin, Y., 2021. A systematic review of machine learning classification methodologies for modelling passenger mode choice. Journal of Choice Modelling 38, 100221. doi: 10.1016/j.jocm. 2020.100221. Hillel, T., Elshafie, M.Z., Jin, Y., 2018. Recreating passenger mode choice-sets for transport simulation: A case study of London, UK. Proceedings of the Institution of Civil Engineers - Smart Infrastructure and Construction 171, 29–42. Karlaftis, M.G., Vlahogianni, E.I., 2011. Statistical methods versus neural networks in transportation research: Differences, similarities and some insights. Transportation Research Part C: Emerging Technologies 19, 387–399. doi:10.1016/j.trc.2010.10.004. Kipf, T.N., Welling, M., 2017. Semi-Supervised Classification with Graph Convolutional Networks. doi: 10.48550/ arXiv.1609.02907,arXiv:1609.02907. Koppelman, F., Pas, E.I., 1980. Travel-choice behavior: models of perceptions, feelings, preference, and choice. Transportation Research Record . Koppelman, F.S., 1981. Non-linear utility functions in models of travel choice behavior. Transportation 10, 127–146. doi:10.1007/BF00165262. Koppelman, F.S., Bhat, C.R., 2006. A Self Instructing Course in Mode Choice Modeling: Multinomial and Nested Logit Models . Kwigizile, V., Chimba, D., Sando, T., 2011. A cross-nested logit model for trip type-mode choice: An application. Advances in Transportation Studies 23, 29–40. Lee, D., Derrible, S., Pereira, F.C., 2018. Comparison of Four Types of Artificial Neural Network and a Multinomial Logit Model for Travel Mode Choice Modeling. Transportation Research Record 2672, 101–112. doi: 10.1177/ 0361198118796971. Lin, L., He, Z., Peeta, S., . Predicting station-level hourly demand in a large-scale bike-sharing network: A graph convolutional neural network approach 97, 258–276. doi:10.1016/j.trc.2018.10.011. Luce, R.D., 1959. The choice axiom, in: Individual Choice Behavior: A Theoretical Analysis. Wiley, New York, pp. 5–16. Manski, C.F., 1977. The Structure of Random Utility Models. Theory and Decision 8. McFadden, D., 1972. Conditional logit analysis of qualitative choice behavior . McFadden, D., 1978. Modeling the choice of residential location. Transportation Research Record Type: Journal Article. Sifringer, B., Lurkin, V., Alahi, A., 2020. Enhancing discrete choice models with representation learning. Transportation Research Part B: Methodological 140, 236–261. doi:10.1016/j.trb.2020.08.006. Subba Rao, P.V., Sikdar, P.K., Krishna Rao, K.V., Dhingra, S.L., 1998. Another insight into artificial",
    "1978. Modeling the choice of residential location. Transportation Research Record Type: Journal Article. Sifringer, B., Lurkin, V., Alahi, A., 2020. Enhancing discrete choice models with representation learning. Transportation Research Part B: Methodological 140, 236–261. doi:10.1016/j.trb.2020.08.006. Subba Rao, P.V., Sikdar, P.K., Krishna Rao, K.V., Dhingra, S.L., 1998. Another insight into artificial neural networks through behavioural analysis of access mode choice. Computers, Environment and Urban Systems 22, 485–496. doi:10.1016/S0198-9715(98)00036-2. Tanwanichkul, L., Kaewwichian, P., Pitaksringkarn, J., 2021. Car ownership demand modeling using Machine Learning: 20 decision trees and neural networks. GEOMATE Journal 17, 219–230. Tomlinson, K., Benson, A.R., 2024. Graph-based methods for discrete choice. Network Science 12, 21–40. doi: 10. 1017/nws.2023.20. Train, K., 1986. Auto Ownership and Use: An Intergrated System of Disaggregate Demand Models, in: Qualitative Choice Analysis: Theory, Econometrics, and an Application to Automobile Demand. MIT Press, pp. 134–145. Train, K.E., 2009. Logit, in: Discrete Choice Methods with Simulation. Cambridge University Press, pp. 76–93. Tygesen, M.N., Pereira, F.C., Rodrigues, F., . Unboxing the graph: Towards interpretable graph neural networks for transport prediction through neural relational inference 146, 103946. URL: https://www.sciencedirect.com/ science/article/pii/S0968090X2200359X, doi:10.1016/j.trc.2022.103946. van Cranenburgh, S., Alwosheel, A., 2019. An artificial neural network based approach to investigate travellers’ decision rules. Transportation Research Part C: Emerging Technologies 98, 152–166. doi:10.1016/j.trc.2018.11.014. van Cranenburgh, S., Wang, S., Vij, A., Pereira, F., Walker, J., 2022. Choice modelling in the age of machine learning - Discussion paper. Journal of Choice Modelling 42, 100340. doi:10.1016/j.jocm.2021.100340. Veliˇ ckovi´ c, P., Cucurull, G., Casanova, A., Romero, A., Li` o, P., Bengio, Y., 2018. Graph Attention Networks. doi:10.48550/arXiv.1710.10903,arXiv:1710.10903. Walker, J., Ben-Akiva, M., 2002. Generalized random utility model. Mathematical Social Sciences 43, 303–343. doi:10.1016/S0165-4896(02)00023-9. Wang, Q., Wang, S., Zheng, Y., Lin, H., Zhang, X., Zhao, J., Walker, J., . Deep hybrid model with satellite imagery: How to combine demand modeling and computer vision for travel behavior analysis? 179, 102869. URL: https://www.sciencedirect.com/science/article/pii/S0191261523001947, doi:10.1016/j.trb.2023.102869. Wang, S., Mo, B., Hess, S., Zhao, J., 2021a. Comparing hundreds of machine learning classifiers and discrete choice mod- els in predicting travel behavior: An empirical benchmark. doi:10.48550/arXiv.2102.01130,arXiv:2102.01130. Wang, S., Mo, B., Zhao, J., 2020a. Deep neural networks for choice analysis: Architecture design with alternative- specific utility functions. Transportation Research Part C: Emerging Technologies 112, 234–251. doi: 10.1016/j. trc.2020.01.012. Wang, S., Wang, Q., Bailey, N., Zhao, J., 2021b. Deep neural networks for choice analysis: A statistical learning theory perspective. Transportation Research Part B: Methodological 148, 60–81. doi: 10.1016/j.trb.2021.03.011 . Wang, S., Wang, Q., Zhao, J., 2020b. Deep neural networks for choice analysis: Extracting complete economic information for interpretation. Transportation Research Part C: Emerging Technologies 118, 102701. doi: 10.1016/ j.trc.2020.102701. Wen, C.H., Koppelman, F.S., 2001. The generalized nested logit model. Transportation Research Part B: Methodological 35, 627–641. doi:10.1016/S0191-2615(00)00045-X. Wong, M., Farooq, B., 2021. ResLogit: A residual neural network logit model for data-driven choice modelling. Transportation Research Part C: Emerging Technologies 126, 103050. doi:10.1016/j.trc.2021.103050. Xu, K., Hu, W., Leskovec, J., Jegelka, S., 2019. How Powerful are Graph Neural Networks? doi: 10.48550/arXiv. 1810.00826,arXiv:1810.00826. Yao, R., Bekhor, S., 2022. A variational autoencoder approach for choice set generation and implicit perception of alternatives in choice modeling. Transportation",
    "modelling. Transportation Research Part C: Emerging Technologies 126, 103050. doi:10.1016/j.trc.2021.103050. Xu, K., Hu, W., Leskovec, J., Jegelka, S., 2019. How Powerful are Graph Neural Networks? doi: 10.48550/arXiv. 1810.00826,arXiv:1810.00826. Yao, R., Bekhor, S., 2022. A variational autoencoder approach for choice set generation and implicit perception of alternatives in choice modeling. Transportation Research Part B: Methodological 158, 273–294. doi: 10.1016/j.trb. 2022.02.015. You, J., Ying, Z., Leskovec, J., 2020. Design space for graph neural networks. Advances in Neural Information Processing Systems 33, 17009–17021. Zhou, J., Cui, G., Hu, S., Zhang, Z., Yang, C., Liu, Z., Wang, L., Li, C., Sun, M., 2020. Graph neural networks: A review of methods and applications. AI Open 1, 57–81. doi:10.1016/j.aiopen.2021.01.001. Appendix A Choice Probabilities in Nested Logit Models Proposition 1.The classical formula of the NL model given by Eq.(3)is equivalent to Eq.(4)–a special type of GNN. 21 Proof.The NL model in Eq. (3) can by reorganized as Pni=P(i|B k)P(B k|V) =exp (V ni/µk)P j∈Bkexp (V nj/µk)×\u0010P j∈Bkexp (V nj/µk)\u0011µk PKn l=1\u0010P j∈Blexp (V nj/µl)\u0011µl =exp (V ni/µk)\u0010P j∈Bkexp (V nj/µk)\u0011µk−1 PKn l=1\u0010P j∈Blexp (V nj/µl)\u0011µl =exp (V ni/µk)\u0010P j∈Bkexp (V nj/µk)\u0011µk−1 PKn l=1\u0010P m∈Blexp(V nm/µl)P m∈Blexp(V nm/µl)\u0010P j∈Blexp (V nj/µl)\u0011µl\u0011 =exp (V ni/µk)\u0010P j∈Bkexp (V nj/µk)\u0011µk−1 PKn l=1P m∈B l\u0010 exp(V nm/µl)P m∈Blexp(V nm/µl)\u0010P j∈Blexp (V nj/µl)\u0011µl\u0011 =exp (V ni/µk)\u0010P j∈Bkexp (V nj/µk)\u0011µk−1 PKn l=1P m∈B l\u0012 exp (V nm/µl)\u0010P j∈Blexp (V nj/µl)\u0011µl−1\u0013 =exp\u0010 Vni/µk+ (µ k−1) log\u0010P j∈Bkexp (V nj/µk)\u0011\u0011 P m∈Vexp\u0010 Vnm/µl+ (µ l−1) log(P j∈Blexp (V nj/µl))\u0011. This yields to the formula in Eq. (4). 22"
  ]
}