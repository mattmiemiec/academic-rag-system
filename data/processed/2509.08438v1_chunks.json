{
  "filename": "2509.08438v1.pdf",
  "total_chunks": 11,
  "text_length": 33055,
  "chunks": [
    "JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 1 CommonV oice-SpeechRE and RPG-MoGe: Advancing Speech Relation Extraction with a New Dataset and Multi-Order Generative Framework Jinzhong Ning, Paerhati Tulajiang, Yingying Le, Yijia Zhang, Yuanyuan Sun, Hongfei Lin, Haifeng Liu Abstract—Speech Relation Extraction (SpeechRE) aims to extract relation triplets directly from speech. However, existing benchmark datasets rely heavily on synthetic data, lacking sufficient quantity and diversity of real human speech. Moreover, Existing models also suffer from rigid single-order generation templates and weak semantic alignment, substantially limiting their performance. To address these challenges, we introduce CommonVoice-SpeechRE, a large-scale dataset comprising nearly 20,000 real-human speech samples from diverse speakers, estab- lishing a new benchmark for SpeechRE research. Furthermore, we propose the Relation Prompt-Guided Multi-Order Generative Ensemble (RPG-MoGe), a novel framework that features: (1) a multi-order triplet generation ensemble strategy, leveraging data diversity through diverse element orders during both training and inference, and (2) CNN-based latent relation prediction heads that generate explicit relation prompts to guide cross-modal alignment and accurate triplet generation. Experiments show our approach outperforms state-of-the-art methods, providing both a benchmark dataset and an effective solution for real-world SpeechRE. The source code and dataset are publicly available at https://github.com/NingJinzhong/SpeechRE RPG MoGe. Index Terms—Speech Relation Extraction, Multimodal Infor- mation Extraction, Cross-modal Alignment, Triple Extraction I. INTRODUCTION RELATION Extraction (RE), a fundamental task in infor- mation extraction, aims to extract structured knowledge in the form of relational triples (head entity, relation, tail entity) from unstructured data. RE plays a pivotal role in down- stream applications such as knowledge graph construction and search engine optimization [1]. Despite its importance, most existing research focuses onTextRE, which extracts relational triples solely from plain text [2]–[4]. However, with the exponential growth of speech data from sources such as news broadcasts, online meetings, and social media, there is a pressing need to extend RE to the speech Jinzhong Ning and Yijia Zhang are with the School of Information Science and Technology, Dalian Maritime University, Dalian 116026, China (e-mail: ningjinzhong@dlmu.edu.cn, zhangyijia@dlmu.edu.cn). Paerhati Tulajiang is with the School of Computer Science and Technology, Dalian University of Technology, Dalian 116024, China; and with the College of Computer Science and Technology, Xinjiang Normal University, Urumqi 830054, China (e-mail: prht@mail.dlut.edu.cn). Haifeng Liu is with the School of Computer and Electronic Information, Nanjing Normal University, Nanjing 210046, China; and with the Adolescent Education and Intelligence Support Lab of Nanjing Normal University, Labo- ratory of Philosophy and Social Sciences at Universities in Jiangsu Province (e-mail: liuhaifeng@nnu.edu.cn). Yingying Le, Yuanyuan Sun, and Hongfei Lin are with the School of Computer Science and Technology, Dalian University of Technology, Dalian 116024, China (e-mail: 29354772@mail.dlut.edu.cn, syuan@dlut.edu.cn, hflin@dlut.edu.cn).TABLE I COMPARISON OFKEYSTATISTICS BETWEEN EXISTING DATASETS AND THE DATASET PROPOSED IN THIS PAPER Dataset CoNLL04 ReTACRED Ours #Rel. 5 40 45 #Train Sam. 922/usr33,47714,557/usr #Dev Sam. 231/usr9,3502,495/usr #Test Sam. 288/usr5,805/usr2,494/usr #Speaker 4 8∼20,000 Notes: “#Rel”: Number of Relations; “Sam.”: Samples; /usr: Indicates samples with real-human speech;: Indi- cates samples with TTS synthetic speech domain. Speech data contains rich structured knowledge that can enhance knowledge graphs and support speech-related ap- plications. This",
    "#Train Sam. 922/usr33,47714,557/usr #Dev Sam. 231/usr9,3502,495/usr #Test Sam. 288/usr5,805/usr2,494/usr #Speaker 4 8∼20,000 Notes: “#Rel”: Number of Relations; “Sam.”: Samples; /usr: Indicates samples with real-human speech;: Indi- cates samples with TTS synthetic speech domain. Speech data contains rich structured knowledge that can enhance knowledge graphs and support speech-related ap- plications. This has led to the emergence ofSpeech Relation Extraction (SpeechRE), a task that directly extracts relational triples from audio recordings. Overall, SpeechRE is a relatively new research topic and remains underexplored. However, two notable works, LNA- ED [5] and MCAM [6], have already made significant con- tributions. Wu et al. [5] introduced the SpeechRE task by applying text-to-speech (TTS) to TextRE datasets, creating two synthetic speech benchmarks. They also provided the first SpeechRE baseline, LNA-ED, which uses a CNN-based length adapter to bridge a speech encoder and text decoder. Building on this, Zhang et al. [6] developed two real-human-speech SpeechRE datasets and proposed MCAM, a more powerful model that employs a Multi-Level Cross-Modal Alignment Adapter to align tokens, entities, and sentences across speech and text. Despite these advancements, existing approaches suffer from several limitations: (1)Issue-1: In their datasets, real- human speech data mainly covers the test set, leaving limited training examples with few speakers (see Table I). This may reduce the model’s performance and generalization in real- world scenarios. (2)Issue-2: Current methods generate rela- tional triples in a fixed order, ignoring the inherent diversity in the order of triple elements within the data. This restricts the model’s ability to fully exploit the data. (3)Issue-3: Existing approaches primarily rely on semantic similarity for cross- modal alignment, overlooking high-level structured semantic cues such as entity relations. To address these challenges, we propose a comprehensive solution that encompasses both data and model innovations.arXiv:2509.08438v1 [cs.CL] 10 Sep 2025 JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 2 “This Must Be the Place” is a song by new wave band Talking Heads, released in November 1983 as the second single from its fifth album “Speaking in Tongues”Relational Triples: (This Must Be the Place, performer, Talking Heads) (Talking Heads, genre, new wave) (This Must Be the Place, part of, Speaking in Tongues) (Speaking in Tongues, performer, Talking Heads) Entities and Relations: ??:This Must Be the Place , ??:new wave , ??:Talking Heads , ??:Speaking in Tongues ??:performer , ??:genre , ??:part of , ??:performer Root① <h> → <r> → <t> ?? ?? ?? ?? ?? ?? ???? ???? ??<h> <r> <t> Linearization:<h> ??<r> ?? <t> ??<r> ??<t> ?? <h> ??<r> ??<t> ??<h> ??<r> ??<t> ??Root② <h> → <t> → <r> ?? ?? ?? ?? ?? ?? ???? ???? ??<h> <t> <r> Linearization:<h> ?? <t> ??<r> ??<t> ??<r> ?? <h> ??<t> ??<r> ??<h> ??<t> ??<r> ??Root③ <r> → <h> → <t> ?? ?? ?? ???? ?? ???? ?? <h> <t> <r> ?? ???? Linearization:<r>??<h>??<t>?? <r> ?? <h> ??<t> ?? <r> ??<h> ?? <t> ?? <r> ??<h> ?? <t> ??Root④ <r> → <t> → <h> ?? ?? ?? ???? ?? ???? ?? <t> <h> <r> ?? ???? Linearization:<r>??<t>??<h>?? <r>??<t>??<h>??<r>??<t>??<h> ?? <r>??<t> ?? <h> ??⑤ <t> → <r>",
    "???? ?? <h> <t> <r> ?? ???? Linearization:<r>??<h>??<t>?? <r> ?? <h> ??<t> ?? <r> ??<h> ?? <t> ?? <r> ??<h> ?? <t> ??Root④ <r> → <t> → <h> ?? ?? ?? ???? ?? ???? ?? <t> <h> <r> ?? ???? Linearization:<r>??<t>??<h>?? <r>??<t>??<h>??<r>??<t>??<h> ?? <r>??<t> ?? <h> ??⑤ <t> → <r> → <h> Root ?? ?? ?? ?? ?? ?? ???? ???? ??<t> <r> <h> Linearization:<t> ??<r> ?? <h> ??<t>??<r>??<h>??<t>??<t>??<t> ?? <r>??<h>??⑥ <t> → <h> → <r> Root ?? ?? ?? ?? ?? ?? ???? ???? ??<t> <h> <r> Linearization: <t>??<h>??<r>?? <t> ??<h>??<r>?? <h>??<r>??<t>?? <h>??<r>?? Order view control tokens Order view control tokens Order view control tokens Order view control tokens Order view control tokensOrder view control tokens Fig. 1. Explanation of the multi-view relation tree and its linearization process. Here, “< h >”, “< r >”, and “< t >” are special tokens representing the head entity, relation type, and tail entity of the relational triple respectively. For the data limitation (Issue-1), we introduce CommonV oice-SpeechRE,the first large-scale dataset with real human speech for SpeechRE, comprising nearly 20,000 naturally spoken recordings from diverse speakers. Our dataset establishes an authentic human speech benchmark with substantial variety of speaker profiles and scenarios (see TABLE I). For the model architecture, we propose theRelation Prompt-GuidedMulti-OrderGenerativeEnsemble (RPG- MoGe) framework, which systematically addresses the iden- tified challenges through two key innovations: (1) To over- come the template rigidity in triplet generation (Issue-2), we introduce an innovative multi-view relation tree structure (illustrated in Figure 1) that comprehensively captures diverse element ordering patterns. Through tree linearization as gener- ation targets, our model implements amulti-order triplet gen- eration ensemble strategyacross both training and inference phases, thereby maximizing the utilization of inherent data diversity. (2) To resolve the alignment deficiency (Issue-3), we design aCNN-based latent relation prediction headthat extracts implicit relational cues from speech signals. These are transformed intoexplicit relation promptsthat dynamically guide the text decoder during both triple generation and cross- modal alignment. To our knowledge, RPG-MoGe is the first SpeechRE frame- work that holistically address cross-modal relation extraction challenges through three three novel components (multi-order ensemble, latent relation prediction, and explicit prompt guid- ance for decoder). Our contributions can be summarized as follows: •We introduce the first large-scale, diverse real-human- speech dataset–CommonV oice-SpeechRE, establishing a new benchmark for SpeechRE research. •We propose RPG-MoGe, a novel framework that inte- grates multi-order triple generation and explicit relationprompts to fully exploit data diversity and high-level semantic cues. •Extensive experiments on multiple SpeechRE bench- marks show that our approach outperforms state-of-the- art baselines, validating the effectiveness of our dataset and model design. II. RELATEDWORK A. Speech Relation Extraction Relation Extraction (RE) is a fundamental task in informa- tion extraction that aims to extract structured knowledge in the form of (head entity, relation, tail entity) triplets from unstructured data. Extensive research has established three dominant paradigms for text-based RE (TextRE): (1) sequence labeling approaches [2], [7], (2) table-filling methods [3], [8], and (3) text generation-based frameworks [4]. With the exponential growth of speech data from sources like podcasts and video content, Speech Relation Extraction (SpeechRE)",
    "triplets from unstructured data. Extensive research has established three dominant paradigms for text-based RE (TextRE): (1) sequence labeling approaches [2], [7], (2) table-filling methods [3], [8], and (3) text generation-based frameworks [4]. With the exponential growth of speech data from sources like podcasts and video content, Speech Relation Extraction (SpeechRE) has emerged as a critical yet underexplored task bridging Information Extraction (IE) and Spoken Language Understanding (SLU) [9]. While substantial progress has been made in related tasks like Speech Named Entity Recognition [10]–[12], SpeechRE remains nascent. Two seminal works have shaped this field: [5] pioneered the SpeechRE task by synthesizing benchmarks through TTS conversion of TextRE datasets, proposing the LNA-ED model with a CNN-based length adapter to bridge speech encoders and text decoders. Subsequently, [6] advanced the field by constructing the first real-human-speech dataset and developing the MCAM frame- work featuring hierarchical cross-modal alignment at token, entity, and sentence levels. B. Multi-view Prompt Text Generation Recent work in aspect-based sentiment analysis has shown that leveraging element order diversity in triples [13] or JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 3 Multi-order view generation token sequence containing explicit relation promptSpeech EncoderDecoder layerDecoder layerDecoder layer Latent Relation Prediction Head Learnable position encoding ... ...Start token Relation type prompt tokens Order view control tokens Linearized relation tree tokens... ... Cross-attentionNext token prediction Latent Relation Prediction Head 4 - layer CNN feature extractorFlattenLinear & sigmoidspeech features Log-Mel spectrogram featuresprediction results Text Decoder Fig. 2. The overall architecture of RPG-MoGe. Relation prompt tokens Text Decoder Speech featuresVote Triple generation results under multiple order viewsFinal results Fig. 3. Implementation details for the Inference Phase in RPG-MoGe. quadruples [14] during training and inference can enhance model performance and generalization. Inspired by this, we are the first to explore the impact of element order diversity in relational triplets on model performance in SpeechRE, a cross- modal text generation task involving both speech and text. This approach distinguishes our work from prior research and opens new avenues for improving SpeechRE through structured data diversity. III. THENEWDATASET We present CommonV oice-SpeechRE, a novel dataset de- rived from the English subset of the Common V oice 17.0 corpus [15]. Common V oice 17.0 is a large-scale, multilingual speech dataset comprising 20,408 validated hours of record- ings across 124 languages, contributed by volunteers globally. Released under the CC-0 license, it permits unrestricted use, modification, and redistribution, making it an ideal founda- tion for secondary annotation tasks such as Speech Relation Extraction (SpeechRE).Most samples in Common V oice 17.0 are negative examples lacking entities or relations. To identify potential positive samples, we employed a pre-trained BERT NER tagger1to analyze transcriptions and filter relevant data. We adopted entity and relation type definitions from the ACE04 and ACE05 datasets, crafting a tailored annotation guide. A team of 10 graduate students (all CET-6 certified) manually labeled approximately 20,000 transcriptions using Label Studio2. The annotation process involved dividing the data into batches of no more than 1,000 sentences, with 10% randomly selected for verification. Experienced annotators ensured sentence- level accuracy exceeded 95%; otherwise, the",
    "annotation guide. A team of 10 graduate students (all CET-6 certified) manually labeled approximately 20,000 transcriptions using Label Studio2. The annotation process involved dividing the data into batches of no more than 1,000 sentences, with 10% randomly selected for verification. Experienced annotators ensured sentence- level accuracy exceeded 95%; otherwise, the batch was re- annotated. IV. METHODOLOGY In this section, we formally define the Speech Relation Extraction (SpeechRE) task and present the detailed imple- mentation of our proposed RPG-MoGe framework. A. Task Definition Given a speech signalS, the SpeechRE task aims to directly extract a set of relational triplesΓ={(h i, ri, ti)|h i, ti∈ E, ri∈R}from the speech signal, where E denotes the set of entities in the speech transcript, and R represents the set of predefined relations. B. Details of the RPG-MoGe Framework The ERP-MoGe framework consists of three core modules: a Speech Encoder, a Latent Relation Prediction Head, and a Text Decoder. The detailed structure is illustrated in Figure 2. 1https://huggingface.co/flair/ner-english-ontonotes 2https://labelstud.io JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 4 TABLE II DATASET STATISTICS. Datasets #Relations#Instances #Triplets#Avg. audio lengthtrain dev test train dev test CoNLL04-SpeechRE 5 922 231 288 1,283 343 422 11.3s ReTACRED-SpeechRE 40 33,477 9,350 5,805 58,465 19,584 13,418 12.9s /usrCommonV oice-SpeechRE 45 14,557 2,495 2,494 15,948 2,696 2,728 11.6s Notes:: TTS-synthesized speech;/usr: real human speech. ReTACRED-SpeechRE enumerates all entity pairs as triplets, including “no relation” type, while the other two datasets only contain positive triplets. 1) Speech Encoder:Given an input raw speech signal S, we first convert it into log-mel spectrogram featuresX. Subsequently, the featuresXare fed into the Whisper speech encoder [16] to extract high-level speech featuresHof the speech: H=WhisperEncoder(X)∈RLH×dh(1) where WhisperEncoder(·)represents the encoding operation of the Whisper encoder model,L Handd hare sequence length and dimension of speech featuresH. 2) Latet Relation Prediction Head:The Latent Relation Prediction Head (LRPH) is designed to leverage semantic entity-relation cues by predicting latent relations in the speech signal. It consists of the following steps: CNN Layers: We passHthrough four CNN layers with ReLU activation to capture local patterns: Hcnn=Conv 4(H)(2) Flattening and Linear Transformation: The CNN output is flattened and fed into a linear layer to compute relation prediction scores: Hflat=Flatten(H cnn)(3) score(R)=σ(W lrpHflat+b lrp)(4) whereσis the sigmoid function,W lrp∈R|R|×d handb lrp∈ R|R|are learnable parameters, andscore(R)∈R|R|represents the scores for all predefined relation types. Loss Function: We employ the Binary Cross Entropy (BCE) loss for training the LRPH module: Llrp=−1 |R||R|X i=1h y(R) ilog(score(R) i) +(1−y(R) i) log(1−score(R) i)i(5) where y(R)denotes the ground-truth relation labels. Since each sample may contain multiple relations, this prediction task is a multi-label classification problem. In y(R), each element y(R) i can be either 0 or 1, indicating the absence or presence of thei-th relation type, respectively. This approach enables the model to predict multiple relations simultaneously for each given input. 3) Multi-view Relation Tree and Linearization:To model the diversity introduced by permutations of triplet element orders, we propose the Multi-view Relation Tree structure. As depicted in Figure 1, each tree consists of four layers, with each layer (excluding the first) corresponding to",
    "predict multiple relations simultaneously for each given input. 3) Multi-view Relation Tree and Linearization:To model the diversity introduced by permutations of triplet element orders, we propose the Multi-view Relation Tree structure. As depicted in Figure 1, each tree consists of four layers, with each layer (excluding the first) corresponding to anelement of the triple. For a given sample, we can generate P(3,3) = 6distinct relation trees by permuting the order of triplet elements. Formally, for a speech signalSwith a set of relation triplets T, we apply the Treeify(·,·)function to construct a relation treeG ψifrom a specific order perspectiveψ i: Gψi=Treeify(T, ψ i)(6) whereψ i∈Ψrepresents an order perspective, andΨencom- passes all six possible order perspectives. The relation treeG ψiis then linearized into a token sequence using the SeqLin(·)operation: Tψi lin=SeqLin(G ψi)(7) 4) Text Decoder:The Text Decoder uses relation prompts and multi-order triplet generation to decode relational triplets. We utilize the pre-trained Whisper decoder [16] for this purpose. The input token sequence to the decoder consists of three parts: Relation type prompt tokens:T rel= [trel 1, . . . , trel n], where trel iare special tokens representing the predicted relation types generated by the Latent Relation Prediction Head. These tokens guide the decoder by incorporating latent relational cues from speech. Order view control tokens:Tctrl ψi= permute([⟨h⟩,⟨r⟩,⟨t⟩], ψ i), which specify the order of special tokens⟨h⟩,⟨r⟩,⟨t⟩for a given perspectiveψ i, as illustrated in Figure 1. Linearized relation tree tokens:Tlin ψi, which represent the linearized token sequence of the relation tree. This component encodes the hierarchical structure of the relation tree into a sequential format suitable for the decoder. These components are concatenated into the decoder input sequenceT dec= [T rel,Tctrl ψi,Tlin ψi]. At thei-th decoding step, the probability distributionptdec iof the output tokentdec iis computed as: htdec i=WhisperDecoder\u0000 H,T<i dec\u0001 (8) ptdec i=Softmax\u0010 Wlmhtdec i+blm\u0011 (9) wherehtdec iis the hidden state, andW lm,blmare learnable parameters. The decoder is trained using the Cross-Entropy Loss: Ldec=−1 NNX i=1|V|X j=1ytdec i[j] log(ptdec i[j])(10) JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 5 TABLE III F1-SCORE(%)COMPARISON: RPG-MOGE VERSUS BASELINES. ModelExternal ResourcesCoNLL04-SpeechREReTACRED-SpeechRE/usrCommonVoice-SpeechRE Entity Relation Triplet Entity Relation Triplet Entity Relation Triplet TextREGPT-3.5(LLM) - 58.74 49.45 22.27 40.46 17.63 3.22 53.74 28.41 10.73 GPT-4(LLM) - 61.36 62.67 28.83 47.4 39.12 9.12 57.33 38.32 15.35 TP-Linker - 78.63 83.49 58.56 50.46 51.83 20.39 64.61 69.31 46.61 Spert - 76.38 81.83 63.45 60.26 63.48 21.46 66.34 70.82 47.26 REBEL - 85.36 89.86 71.46 60.09 65.15 25.15 71.32 74.32 49.81 BiRTE - 79.34 87.34 64.61 61.93 65.51 20.76 67.61 72.35 47.10 OD-RTE - 81.81 82.35 60.57 59.37 60.94 21.08 70.61 69.67 47.34 SpeechRE (Pipline)GPT-3.5 pipe(LLM) - 28.21 69.61 6.31 16.61 43.84 1.32 21.30 46.81 3.34 GPT-4 pipe(LLM) - 29.41 70.31 7.13 19.76 46.31 4.23 23.61 44.35 4.94 TP-Linker pipe - 35.21 78.21 9.76 30.27 50.01 6.59 31.06 64.13 7.61 Spert pipe - 30.43 75.95 11.88 34.36 57.17 6.89 32.61 64.48 7.54 REBEL pipe - 37.06 83.35 14.01 32.07 51.97 6.49 31.54 66.10 7.92 BiRTE pipe - 31.94 76.61 12.34 32.55 56.63 6.73 32.56 65.12 7.64 OD-RTE pipe - 34.36",
    "- 35.21 78.21 9.76 30.27 50.01 6.59 31.06 64.13 7.61 Spert pipe - 30.43 75.95 11.88 34.36 57.17 6.89 32.61 64.48 7.54 REBEL pipe - 37.06 83.35 14.01 32.07 51.97 6.49 31.54 66.10 7.92 BiRTE pipe - 31.94 76.61 12.34 32.55 56.63 6.73 32.56 65.12 7.64 OD-RTE pipe - 34.36 79.23 9.83 31.34 51.32 6.64 31.67 65.15 7.56 SpeechRE (End2End)GPT-4o-audio(LLM) - 31.21 59.57 5.64 13.21 41.61 1.14 29.33 31.70 3.12 Qwen2-audio(LLM) - 36.74 16.31 2.31 10.50 23.61 0.31 31.16 14.92 0.85 LNA-ED(520M) ori PL-FT 18.87 55.66 10.41 17.21 43.37 3.20 26.34 37.31 5.37 LNA-ED(770M) whi - 19.13 56.32 11.12 18.26 43.15 3.67 27.61 38.51 6.01 MCAM(520M) ori ASR-PTC 40.13 77.89 22.07 35.34 58.96 8.07 43.94 48.37 14.96 MCAM(770M) whi - 40.66 77.61 22.71 35.6159.138.21 45.34 50.34 15.71 RPG-MoGe(250M) whi - 43.16 76.91 22.17 36.00 57.46 8.09 45.59 49.60 15.32 RPG-MoGe(770M) whi -50.21 79.64 24.67 36.7658.389.18 47.20 53.48 18.29 Notes: Subscriptpipedenotes ASR+TextRE pipeline methods; ‘PL-FT’ indicates fine-tuning with pseudo-labeled data; ‘ASR-PTC’ refers to pre-training with ASR data. Subscriptorirepresents the original LNA-ED [5]/MCAM [6] backbone: 24-layer Wave2vec encoder + 12-layer BART-large decoder (520M). Subscriptwhidenotes Whisper [16] backbones: 24-layer encoder/decoder (770M) or 12-layer encoder/decoder (250M). whereNis the sequence length,|V|is the vocabulary size, andytdec iis the token label at thei-th decoding step. 5) Training and Inference Strategies:During training, each sample is expanded into multiple generation targets corresponding to all possible order views for participation in training. The total loss combines theL lrpandL dec: Ltotal=L lrp+Ldec (11) To address potential discrepancies between training and testing performance in the Latent Relation Prediction Head (LRPH), we implemented two key regularization techniques: (1) Incorporated dropout layers (p=0.5) within the LRPH module, and (2) Adopted a training strategy that randomly masks 0-50% of positive relations predicted by LRPH during training (disabled during inference). During inference, as illustrated in Figure 3, the text decoder takes the speech featuresHand relation prompt tokensT rel as initial inputs. By varying the order view control tokens, the decoder autoregressively generates triplets under all order views. A triplet is included in the final results if it appears in more thanλ vote order views. V. EXPERIMENTS A. Datasets & Evaluation Metrics We conducted experiments on three datasets: CoNLL04- SpeechRE, ReTACRED-SpeechRE and the CommonV oice- SpeechRE dataset proposed in this paper. The CommonV oice- SpeechRE dataset includes diverse real human speech in its training, development, and test sets. For CoNLL04-SpeechRE and ReTACRED-SpeechRE, since the real human speech test set and partial real human speech training set proposed by [6] have not yet been released, we used the fully TTS- generated speech version released by [5]. Detailed statistics ofthe datasets are provided in Table II. For evaluation metrics, following previous work [5], [6], we used the micro-F1 score to assess the performance of models in entity recognition, relation prediction, and relation triplet extraction. For an entity, relation or triple to be considered correct, it must exactly match its counterpart in the ground truth tags. B. Experimental Settings Our model was implemented using PyTorch-Lightning3and PyTorch [17], with OpenAI’s Whisper4[16] as the backbone, specifically the whisper-small5(244M) and whisper-medium6 (769M) versions. We optimized",
    "relation triplet extraction. For an entity, relation or triple to be considered correct, it must exactly match its counterpart in the ground truth tags. B. Experimental Settings Our model was implemented using PyTorch-Lightning3and PyTorch [17], with OpenAI’s Whisper4[16] as the backbone, specifically the whisper-small5(244M) and whisper-medium6 (769M) versions. We optimized the model parameters using the Adam optimizer with a learning rate of 1e-5, a batch size of 12. Training epochs were set to 50 for CoNLL04-SpeechRE, 20 for ReTACRED-SpeechRE, and 10 for CommonV oice- SpeechRE. For the relation prediction head, we employed a four-layer CNN with 2D convolutions (kernel size = 3) and progressively increasing channel dimensions (16, 32, 64, 128). During inference, the voting thresholdλ vote for all order views was set to 2. All hyperparameters were tuned on the development set, and the best-performing checkpoint was selected for test set evaluation. Training was conducted on a single NVIDIA A40 GPU, while inference was performed on a single NVIDIA GeForce RTX 4090 GPU. C. Baselines To comprehensively evaluate the performance of our pro- posed model, we compare it with three categories of com- 3https://github.com/Lightning-AI/pytorch-lightning 4Whisper has become a standard backbone in speech processing, similar to BERT and BART in NLP. 5https://huggingface.co/openai/whisper-small.en 6https://huggingface.co/openai/whisper-medium.en JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 6 petitive baselines: (1)TextRE Models.These models are designed to jointly extract entities and relations from input text. For a fair comparison, following prior works [5], [6], we adopt three strong TextRE models: TP-Linker [3], Spert [2], and REBEL [4] augmented with two recent advances (BiRTE [7] and OD-RTE [8]) Additionally, to explore the potential of large language models (LLMs) in relation extrac- tion, we include GPT-3.57and GPT-48as baselines, leveraging their in-context learning capabilities for TextRE tasks. (2) Pipeline SpeechRE Models.These models follow a two- stage pipeline: first, an Automatic Speech Recognition (ASR) module transcribes the input speech into text; second, a TextRE module extracts relation triplets from the transcribed text. To ensure a fair comparison, we follow the setup of prior works [5], [6] and employ the pre-trained wav2vec-large model as the ASR module. For the TextRE module, we use the same five TextRE models mentioned above, resulting in seven pipeline models: TP-Linker pipe, Spert pipe, REBEL pipe, BiRTE pipe, OD-RTE pipe, GPT-3.5 pipe, and GPT-4 pipe. (3) End-to-End SpeechRE Models.These models are designed to directly extract relation triplets from input speech, without the intermediate step of text transcription. Our proposed RPG- MoGe also falls into this category. As baselines, we include two existing end-to-end SpeechRE models: LNA-ED [5] and MCAM [6]. Additionally, to explore the capabilities of recent advancements in speech-based LLMs, we introduce two in- context learning baselines: GPT-4o-audio9and Qwen2-audio10 [18]. D. Main Results We conducted a comprehensive performance compari- son between our proposed RPG-MoGe model and several strong baselines, including TextRE, SpeechRE (Pipeline), and SpeechRE (End2End). The experimental results, presented in Table III, reveal the following key observations: (1) RPG-MoGe outperforms all SpeechRE (End2End) base- lines, achieving state-of-the-art performance in entity, rela- tion, and triplet F1 scores across all datasets. Notably, RPG- MoGe with",
    "model and several strong baselines, including TextRE, SpeechRE (Pipeline), and SpeechRE (End2End). The experimental results, presented in Table III, reveal the following key observations: (1) RPG-MoGe outperforms all SpeechRE (End2End) base- lines, achieving state-of-the-art performance in entity, rela- tion, and triplet F1 scores across all datasets. Notably, RPG- MoGe with a 250M parameter Whisper backbone surpasses the SOTA baseline MCAM using a 520M backbone and matches MCAM’s performance with a 770M backbone. This demonstrates RPG-MoGe’s ability to leverage the diversity of relation triplet element orders and effectively utilize high-level semantic cues through its potential relation prediction head and explicit relation prompts. (2) RPG-MoGe consistently outperforms all SpeechRE models in triplet extraction, highlighting the limitations of the pipeline approach, where cascading ASR with TextRE intro- duces significant errors. The end-to-end approach effectively mitigates error accumulation, improving entity, relation, and triplet extraction accuracy. (3) Large language models without fine-tuning (e.g., GPT- 3.5, GPT-4, GPT-4o-audio, Qwen2-audio) perform signifi- cantly worse on the datasets compared to fine-tuned smaller 7gpt-3.5-turbo-0125 8gpt-4-turbo-2024-04-09 9gpt-4o-audio-preview-2024-12-17 10Qwen2-Audio-7B-Instructmodels, emphasizing the continued importance of developing fine-tuned models in TextRE and SpeechRE domains. (4) Replacing the non-aligned Wave2vec and BART en- coders in LNA-ED and MCAM with the pre-trained and aligned Whisper encoder and decoder eliminates the need for extensive external corpus alignment and improves per- formance. This also ensures a fairer comparison with RPG- MoGe, which utilizes Whisper as its backbone. VI. CONCLUSION In this work, we address the limitations of existing datasets and models in Speech Relation Extraction (SpeechRE) by introducing CommonV oice-SpeechRE, a large-scale dataset with diverse real-human speech samples, and proposing RPG- MoGe, a novel framework that leverages a multi-order triplet generation ensemble strategy and CNN-based latent relation prediction heads to enhance triple generation and cross-modal alignment. Extensive experiments demonstrate the superiority of our approach, outperforming state-of-the-art baselines and setting a new benchmark for SpeechRE research. Our con- tributions provide both a valuable resource and an effective methodology, advancing the field toward real-world applica- tions. REFERENCES [1] Z. Nasar, S. W. Jaffry, and M. K. Malik, “Named entity recognition and relation extraction: State-of-the-art,”ACM Computing Surveys (CSUR), vol. 54, no. 1, pp. 1–39, 2021. [2] M. Eberts and A. Ulges, “Span-based joint entity and relation extraction with transformer pre-training,” inECAI 2020. IOS Press, 2020, pp. 2006–2013. [3] Y . Wang, B. Yu, Y . Zhang, T. Liu, H. Zhu, and L. Sun, “Tplinker: Single- stage joint extraction of entities and relations through token pair linking,” inProceedings of the 28th International Conference on Computational Linguistics, 2020, pp. 1572–1582. [4] P.-L. H. Cabot and R. Navigli, “Rebel: Relation extraction by end-to-end language generation,” inFindings of the Association for Computational Linguistics: EMNLP 2021, 2021, pp. 2370–2381. [5] T. Wu, G. Wang, J. Zhao, Z. Liu, G. Qi, Y . F. Li, and G. Haffari, “Towards relation extraction from speech,” inEmpirical Methods in Natural Language Processing 2022. Association for Computing Machinery (ACM), 2022, pp. 10 751–10 762. [6] L. Zhang, Z. Yang, B. Fu, Z. Lu, L. Shao, S. Liu, F. Meng, J. Zhou, X. Wang, and J. Su, “Multi-level cross-modal alignment for speech relation",
    "“Towards relation extraction from speech,” inEmpirical Methods in Natural Language Processing 2022. Association for Computing Machinery (ACM), 2022, pp. 10 751–10 762. [6] L. Zhang, Z. Yang, B. Fu, Z. Lu, L. Shao, S. Liu, F. Meng, J. Zhou, X. Wang, and J. Su, “Multi-level cross-modal alignment for speech relation extraction,” inProceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, 2024, pp. 11 975–11 986. [7] F. Ren, L. Zhang, X. Zhao, S. Yin, S. Liu, and B. Li, “A simple but effective bidirectional framework for relational triple extraction,” inProceedings of the fifteenth ACM international conference on web search and data mining, 2022, pp. 824–832. [8] J. Ning, Z. Yang, Y . Sun, Z. Wang, and H. Lin, “Od-rte: a one-stage object detection framework for relational triple extraction,” inProceed- ings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2023, pp. 11 120–11 135. [9] S. Shon, A. Pasad, F. Wu, P. Brusco, Y . Artzi, K. Livescu, and K. J. Han, “Slue: New benchmark tasks for spoken language understanding evaluation on natural speech,” inICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2022, pp. 7927–7931. [10] H. Yadav, S. Ghosh, Y . Yu, and R. R. Shah, “End-to-end named entity recognition from english speech,” inInterspeech 2020, 21st Annual Conference of the International Speech Communication Association, Virtual Event, Shanghai, China, 25-29 October 2020. ISCA, 2020, pp. 4268–4272. [11] S. Ghannay, A. Caubri `ere, Y . Est `eve, N. Camelin, E. Simonnet, A. Lau- rent, and E. Morin, “End-to-end named entity and semantic concept extraction from speech,” in2018 IEEE Spoken Language Technology Workshop (SLT). IEEE, 2018, pp. 692–699. JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 7 [12] B. Chen, G. Xu, X. Wang, P. Xie, M. Zhang, and F. Huang, “Aishell- ner: Named entity recognition from chinese speech,” inICASSP 2022- 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2022, pp. 8352–8356. [13] Z. Gou, Q. Guo, and Y . Yang, “Mvp: Multi-view prompting improves aspect sentiment tuple prediction,” inProceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2023, pp. 4380–4397. [14] Y . Bai, Y . Xie, X. Liu, Y . Zhao, Z. Han, M. Hu, H. Gao, and R. Cheng, “Bvsp: Broad-view soft prompting for few-shot aspect sentiment quad prediction,” inProceedings of the 62nd Annual Meeting of the Associ- ation for Computational Linguistics, 2024, pp. 8465–8482. [15] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler, J. Meyer, R. Morais, L. Saunders, F. M. Tyers, and G. Weber, “Common voice: A massively-multilingual speech corpus,” inProceedings of the 12th Conference on Language Resources and Evaluation (LREC 2020), 2020, pp. 4211–4215. [16] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever, “Robust speech recognition via large-scale weak supervi- sion,” inInternational conference on machine learning. PMLR, 2023, pp. 28 492–28 518. [17] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein,",
    "J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever, “Robust speech recognition via large-scale weak supervi- sion,” inInternational conference on machine learning. PMLR, 2023, pp. 28 492–28 518. [17] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antigaet al., “Pytorch: An imperative style, high-performance deep learning library,”Advances in neural information processing systems, vol. 32, 2019. [18] Y . Chu, J. Xu, Q. Yang, H. Wei, X. Wei, Z. Guo, Y . Leng, Y . Lv, J. He, J. Lin, C. Zhou, and J. Zhou, “Qwen2-audio technical report,”arXiv preprint arXiv:2407.10759, 2024."
  ]
}