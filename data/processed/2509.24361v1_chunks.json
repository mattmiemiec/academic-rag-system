{
  "filename": "2509.24361v1.pdf",
  "total_chunks": 20,
  "text_length": 62296,
  "chunks": [
    "UI-UG: A Unified MLLM for UI Understanding and Generation Hao Yang1, Weijie Qiu1,2, Ru Zhang1,3, Zhou Fang1, Ruichao Mao1, Xiaoyu Lin1, Maji Huang1, Zhaosong Huang1, Teng Guo1, Shuoyang Liu1, Hai Rao1 1Ant Group 2Beijing University of Posts and Telecommunications 3Zhejiang University Abstract Although Multimodal Large Language Models (MLLMs) have been widely applied across domains, they are still fac- ing challenges in domain-specific tasks, such as User Inter- face (UI) understanding accuracy and UI generation qual- ity. In this paper, we introduceUI-UG(a unified MLLM forUI Understanding andGeneration), integrating both ca- pabilities. For understanding tasks, we employ Supervised Fine-tuning (SFT) combined with Group Relative Policy Op- timization (GRPO) to enhance fine-grained understanding on the modern complex UI data. For generation tasks, we fur- ther use Direct Preference Optimization (DPO) to make our model generate human-preferred UIs. In addition, we pro- pose an industrially effective workflow, including the design of an LLM-friendly domain-specific language (DSL), train- ing strategies, rendering processes, and evaluation metrics. In experiments, our model achieves state-of-the-art (SOTA) per- formance on understanding tasks, outperforming both larger general-purpose MLLMs and similarly-sized UI-specialized models. Our model is also on par with these larger MLLMs in UI generation performance at a fraction of the computational cost. We also demonstrate that integrating understanding and generation tasks can improve accuracy and quality for both tasks. Code & Model: https://github.com/neovateai/UI-UG Introduction With the advent of the modern mobile Internet era, User Interface (UI) plays an increasingly significant role in our daily lives. Whenever an app page is opened, a new UI is presented to the user, which can significantly influence user experience and key business metrics. Consequently, research in the UI domain has been a focal point for both academia and industry. Front-end developers can also benefit from research fo- cused on UI understanding and generation tasks in twofold. First, these skills improve development efficiency by en- abling automated tools like design-to-code (D2C) and low- code UI construction. Additionally, they support real-time user interactions in AI-powered conversational interfaces, allowing features such as automated interactions (e.g., but- ton clicks) and dynamic UI generation during LLM-chat conversations. Many general-purpose multimodal large language models (MLLMs), such as GPT-4o (Hurst et al. 2024), Claude (An- Densely arranged iconsCommercial banners/ pop-upsFunctional icons Nested UI elements Previous UIfrom RICO Dataset Modern Complex UIFigure 1: With the development of UI resolution and design, modern apps now include more icons, ads, and complex el- ements, creating new challenges in understanding UIs. thropic 2025), Gemini (Comanici et al. 2025), and Qwen VL series (Bai et al. 2023; Wang et al. 2024a; Bai et al. 2025) possess certain capabilities in the UI domain, but their ac- curacy and stability remain suboptimal for domain-specific tasks. Meanwhile, recent works leveraging MLLMs in the UI domain have focused on specified UI tasks. For example, Ferret-UI (You et al. 2024) and its successor Ferret-UI2 (Li et al. 2024) have developed general-purpose models for UI understanding, such as referring, grounding, and extended their applicability across platforms. Concurrently, studies like DCGen (Wan et al. 2024b), DeclarUI (Ting Zhou 2024) have optimized UI generation by using",
    "For example, Ferret-UI (You et al. 2024) and its successor Ferret-UI2 (Li et al. 2024) have developed general-purpose models for UI understanding, such as referring, grounding, and extended their applicability across platforms. Concurrently, studies like DCGen (Wan et al. 2024b), DeclarUI (Ting Zhou 2024) have optimized UI generation by using multimodal models. With the continued increase in UI resolution and improve- ment of design complexity, the challenges in UI-related tasks have escalated significantly. As shown in Figure 1, modern UIs now tend to feature abundant commercial con- tent (banner, pop-up windows) and pack UIs in a dense way (smaller arranged icons, nested elements). Moreover, this evolution demands a finer-grained understanding of UI el- ements, for instance, customized icon recognition require- ments for specialized icons (e.g., ’close’, ’return’, ’more’) toarXiv:2509.24361v1 [cs.CV] 29 Sep 2025 Two-Stage Training Pipeline InferenceDataPreparation UIImage Collection UIElements Detection UIDSLGeneration •Category(text, icon, button…) •Coordinate ( xmin, ymin, xmax, ymax) •Extra Info(text content, color…) Screenshot Requirements Reference Image + + Requirement -Image -DSL (tuple)Generate… MLLM Fine-tuningVQA Dataset •UIReferring •UIGrounding •UIGeneration UIData Vision Encoder LLMAdapter 1. Describe theregion (20,20,200,180). 2. Describe the text present in area (20,20,200,180). 3. Listallthe UI /icons in the image. UI DPO Dataset Task -specific Reward Functions JudgementUnderstanding Generation0.9 0.7 0.3 0.1 GRPO DPO Preferred RejectedStage1: Supervised Fine -tuning Stage2: Reinforcement Learning UIUnderstanding UIGeneration 1. Generate a UI poster for an art show. 2. I want to query the summary of my historical expense records. Example of real -time UI generationData query Qwen2.5 VL Trainable Frozen … JSON Gen. Results Und. Results Figure 2: The workflow for UI-UG includes 1) Data preparation (UI image collection + element detection + DSL generation); 2) Two-stage training: SFT with VQA dataset, then RL optimization using GRPO and DPO for each task. The model supports UI understanding tasks (referring and grounding) and enables both offline and real-time UI generation. support automated agent operations. Consequently, legacy open-source UI datasets such as RICO (Deka et al. 2017) and VINS (Bunian et al. 2021) have become increasingly outdated in visual style and insufficient to meet current de- mands. Compared to UI understanding tasks, generation tasks present a more comprehensive and complex chal- lenge. They require not only image-type perception but also a deeper global understanding of UI hierarchical struc- tures. While most prior work Pix2Code (Beltramelli 2018), Sketch2Code (Jain et al. 2019), DCGen (Wan et al. 2024b), DeclarUI (Ting Zhou 2024), only focused on offline or development-phase generation, the era of multimodal large language models enables real-time, interactive UI genera- tion during conversational workflows. In this paper, we proposeUI-UG, the first unified MLLM forUI Understanding and Generation. Figure 2 illustrates the whole pipeline of our work, including data preparation, two-stage model training and model inference. To address these issues above, we first enhanced our dataset by collecting screenshots of over 30,000 images from modern mobile apps, covering homepages, feeds, mar- keting modules, etc. We designed our fine-grained UI clas-sification categories, including pop-up windows, functional icons, etc., and added advanced tasks like OCR and color recognition to expand our UI understanding capabilities. Building upon this enhanced",
    "by collecting screenshots of over 30,000 images from modern mobile apps, covering homepages, feeds, mar- keting modules, etc. We designed our fine-grained UI clas-sification categories, including pop-up windows, functional icons, etc., and added advanced tasks like OCR and color recognition to expand our UI understanding capabilities. Building upon this enhanced understanding capability, we started our core task of UI generation. Since UI generation is about producing renderable code instead of generating im- ages, we designed a frontend-friendly domain-specific lan- guage (DSL) in JSON format containing UI types, hierar- chical structures, mock data placeholders, and CSS tokens. Our DSL design allows the model to consume data queried from the backend during user interactions, which supports dynamic, real-time, and incremental UI rendering. Next, we used larger MLLMs with customized-designed system prompts to generate requirements and corresponding DSLs from UI images, and then further constructed the UI genera- tion dataset (Requirement-Image-DSL tuples) by validating DSL rendering and selecting high-quality renders. Subsequently, based on fundamental annotations for UI understanding and generation, we constructed a VQA dataset, which includes referring, grounding, and generat- ing tasks. We started our fine-tuning with Qwen2.5-VL-7B model, while experimenting with data processing and rein- forcement learning methods like GRPO (Guo et al. 2025) and DPO (Rafailov et al. 2023) to further improve perfor- mance. For instance, we enhanced detection accuracy by grounding sorting and alignment techniques, leveraging the regularity and repetitive patterns in UI. And we optimized referring and grounding tasks by GRPO using format, accu- racy, and IoU rewards. Additionally, we improved the visual quality and stability of generated outputs through our well- designed DPO preference dataset. We compared our model UI-UG with current general- purpose MLLMs (e.g., GPT-4o, Qwen2.5-VL-72B) and UI- specific models (e.g., OmniParser (Wan et al. 2024a; Yu et al. 2025), Ferret-UI2 (Li et al. 2024)) on understanding tasks, and our model achieved markedly superior perfor- mance. Finally, we also observed the enhancements on our generation task through reinforcement learning optimiza- tion, outperforming the generation capabilities of leading general-purpose large models. During our experiments, we also observed that combining UI understanding and genera- tion tasks in training yielded better performance than train- ing them separately, which indicates a potential synergistic effect between these two tasks. In summary, our key contributions are: • We proposedUI-UG, a first unified MLLM for UI under- standing and generation, tailored for modern UI tasks. • We broadened the task scope to advance UI structural comprehension, and used more methods (SFT, GRPO, DPO) to improve performance for both understanding and generation tasks. • We developed a practical workflow for real-world ap- plications, including LLM-friendly DSL design, gener- ating, evaluation, and progressive rendering capabilities. • Our model achieved SOTA performance on modern com- plex UI understanding tasks, while attaining comparable UI generation quality to larger models at a fraction of their computational cost. Related Work UI Understanding.Understanding UIs is fundamental for intelligent automation and enhancing human-computer interaction. Early efforts were supported by large-scale datasets like Rico (Deka et al. 2017), ERICA (Deka, Huang, and Kumar 2016), while later benchmarks like WebSRC (Chen et al. 2021)",
    "models at a fraction of their computational cost. Related Work UI Understanding.Understanding UIs is fundamental for intelligent automation and enhancing human-computer interaction. Early efforts were supported by large-scale datasets like Rico (Deka et al. 2017), ERICA (Deka, Huang, and Kumar 2016), while later benchmarks like WebSRC (Chen et al. 2021) and VINS (Bunian et al. 2021) intro- duced more complex challenges. Traditional methods un- derstand UI from structural methods that parsed Document Object Models (DOM) or view hierarchies (Cai et al. 2003; Song et al. 2004). The advent of computer vision and large multimodal models, models like ScreenParsing (Wu et al. 2021), Pix2Struct (Lee et al. 2023) and the vision-based Om- niParser (Wan et al. 2024a; Yu et al. 2025) demonstrated parsing UIs directly from pixels. For a more granular un- derstanding, specialized models like Ferret-UI (You et al. 2024) and Ferret-UI2 (Li et al. 2024) unified various UI tasks across platforms. In this work, we adopted MLLMs and built upon similar frameworks established in prior stud- ies, and conducted comparisons with these works.UI Generation.Automated UI code generation speeds up development by automatically turning visual designs into working code. DL-based approaches like Pix2code (Bel- tramelli 2018) used CNNs and LSTMs to convert UI im- ages into code, often a Domain-Specific Language (DSL). In parallel, CV-based methods like Sketch2Code (Jain et al. 2019) and REMAUI (Nguyen and Csallner 2015) focused on explicitly identifying UI components. The latest meth- ods utilize MLLMs to overcome persistent issues like ele- ment omission and misarrangement. For instance, DCGen (Wan et al. 2024b) and DeclarUI (Ting Zhou 2024) em- ploy a divide-and-conquer approach, prompting an MLLM to generate code for submodules separately and then assem- bling the final webpage. Works like Web2Code(Yun et al. 2024), LiveCodeBench (Jain et al. 2024) also build bench- marks for evaluating UI generation from LLMs. In our work, we leveraged MLLMs to generate UI DSL code from user requirements and reference images, with quantitative evalu- ation against benchmark models from prior studies. Reinforcement Learning with MLLMs.The fine-tuning of large models has shifted from traditional Reinforce- ment Learning from Human Feedback (RLHF) (Chris- tiano et al. 2017; Ziegler et al. 2019) towards more di- rect approaches like Direct Preference Optimization (DPO) (Rafailov et al. 2023), eliminating the complex reward- modeling step. Group Relative Policy Optimization (GRPO) (Guo et al. 2025) advances AI training by using a more efficient group-wise optimization to find the best-of-N re- sponse, significantly boosting the performance and align- ment of LLMs. These reinforcement learning techniques also extend powerfully to Multimodal Large Language Mod- els (MLLMs). Key developments include Visual-RFT (Liu et al. 2025), applying visual reinforcement fine-tuning for vision-language tasks; R1-VL (Zhang et al. 2025) and VLM- R1 (Shen et al. 2025), utilizing step-wise GRPO for sta- ble and generalizable multimodal reasoning. In this work, we implemented the aforementioned reinforcement learning methods (GRPO/DPO) and adapted them to multimodal UI task frameworks. Data Preparation For UI understanding and generation tasks, we constructed two datasets: UI Elements Dataset and UI DSL Dataset. UI Elements Dataset UI Screenshots Collection.First, we collected screen- shots",
    "multimodal reasoning. In this work, we implemented the aforementioned reinforcement learning methods (GRPO/DPO) and adapted them to multimodal UI task frameworks. Data Preparation For UI understanding and generation tasks, we constructed two datasets: UI Elements Dataset and UI DSL Dataset. UI Elements Dataset UI Screenshots Collection.First, we collected screen- shots randomly from widely-used, popular apps. Notably, to overcome the limitation of standard screenshots often trun- cating UI contents, we developed engineering solutions to generate full-length, scrollable screenshots. Subsequently, we encoded the images using the CLIP (Radford et al. 2021) model and performed deduplication based on image embed- ding similarity. Ultimately, our dataset contains over 30,000 images. All images have resolutions within the range of [380*720, 1550*3800], with approximately 76% being stan- dard smartphone-sized images and 24% being full-length scrollable screenshots. Pipeline for DSL Dataset Creation Cropped UI Components UIDSLs Inpainting Image captioning Requirement GenerationShufflingOriginal RequirementReference ImageDSLRendering Check Design Evaluation Data Tuples Figure 3: Pipeline for UI DSL Dataset creation. The final data tuples consist of requirements, reference images, and corresponding UI DSLs. UI Element Classes.Building upon basic UI element cat- egories (text, image, icon, button), we introduced higher- level semantic UI classes that frequently appear in our col- lected screenshots, such as pop-up and progress bar. Addi- tionally, to better address agent-oriented tasks, we incorpo- rated a deeper semantic understanding of screen contents, including the identification of interactive elements such as functional icons and checkboxes. UI Element Annotation.To accelerate the annotation process, we first employed crowdsourced manual labeling on approximately 3,000 samples (10% of the dataset). Using this annotated data, we trained YOLOv8 (Jocher, Chaura- sia, and Qiu 2023) grounding models to generate bounding box annotations automatically for the remaining data. After- wards, we further predicted textual content using OCR(Du et al. 2021) methods and text color via color clustering for text elements, aiming to enrich the dataset with fine-grained content and stylistic information. Finally, human annotators refined these automated annotations to construct the high- quality final UI dataset. UI DSL Dataset UI Element Cropping and Description.In practical in- dustrial scenarios for UI generation tasks, engineers typi- cally focus on generating individual UI components or their combinations rather than reconstructing entire pages. To align with this workflow, we cropped the original screen- shots using a UI component detector. Subsequently, we uti- lized Qwen2.5-VL-72B to generate captions for the cropped UI components to further simulate user requirements. DSL Design.Our UI DSL utilizes JSON as its core struc- ture. For detailed UI style descriptions, it incorporates Tail- wind CSS-like tokens such as color, font and alignment. Be-sides, a key feature of our DSL is mock fields, which al- low engineers to dynamically replace text, data or images at runtime via dynamic inputs. This design also leverages the prevalence of JSON partial parsers, enabling streaming- based progressive rendering even before the full UI is com- pletely generated. DSL Generation.Considering the cost, we utilized the open-source models Qwen2.5-VL-72B to generate DSL specifications corresponding to each UI component. For each input, we treated the generation requirement as a query and an optional UI image as a reference",
    "based progressive rendering even before the full UI is com- pletely generated. DSL Generation.Considering the cost, we utilized the open-source models Qwen2.5-VL-72B to generate DSL specifications corresponding to each UI component. For each input, we treated the generation requirement as a query and an optional UI image as a reference for color schemes, styles, and layout. In the system prompt, beyond basic task instructions, we explicitly defined our DSL structure, demonstration examples to guide the MLLM to strictly ad- here to the UI DSL specifications. Style Mixing.To enhance the model’s style transfer capa- bilities, we randomly applied inpainting or non-inpainting operations to reference images, and performed random com- binations of reference images and user requirements. This approach not only improves style adaptability but also pre- vents textual content in reference images from dominating generation results during subsequent model training. DSL Validation.We employed rendering checks (using a UI DSL renderer) and designer evaluations to validate and filter suitable generated DSL outputs. Finally, this process generated a dataset of tuples containing UI requirements, reference images, and corresponding UI DSL specifications as shown in Figure 3. Method Supervised Finet-tuning (SFT) VQA Dataset.We provided 10 prompt templates per task to generate Visual Question Answering (VQA) data for re- ferring, grounding, and generating tasks. For consistency and easy parsing, all final dataset results are in JSON for- mat. All images are smart resized following Qwen2.5-VL’s image processing method, scaling image dimensions to the nearest multiple of 28 while adhering to min pixels with 64*28*28 and max pixels with 1280*28*28 constraints. To enhance the model’s spatial perception, all coordinates were wrapped with special tokens, specifically< box start > (xmin, ymin),(x max, ymax)< box end >. For grounding tasks, considering the structural regularity and alignment in UI elements (e.g., icons or components arranged in grid-like layouts), we first grouped bounding boxes by category and sorted them by category frequency. Subsequently, we ordered them by left-top coordinates. Ac- cording to ROPE (Su et al. 2024) position embedding, coor- dinate outputs for adjacent regions tend to be encoded with rotational transformations that preserve their relative spa- tial order. This ensures adjacent boxes exhibit greater prox- imity in category or spatial structure, enabling the model to better capture attention on relevant elements and reduce missed or false detections. For generating tasks, we addition- ally employed LLMs to rewrite requirements, condensing them into concise one-sentence or multi-sentence descrip- tions. This improves adaptability for ambiguous user re- quirements. During dataset construction, we also generated both image-referenced and non-image-referenced datasets. Model Fine-tuning.We post-train our modelUI-UG-7B- SFTby fine-tuning from Qwen2.5-VL-7B model on our VQA dataset for understanding and generation tasks. Dur- ing training, we froze the ViT (Dosovitskiy et al. 2020) module and trained the Large Language Model (LLM) and Vision-Language Adapter (VL Adapter) modules, setting the max learning rate for 1e-5, and max-tokens for 8192. We also employed a standard warmup-stable-decay learning rate schedule to ensure stable training and better convergence. Through fine-tuning, we sampled approximately 180,000 training VQA data and 0.8 billion tokens in total. The data ratio was approximately 4:3:2, and the trained",
    "the max learning rate for 1e-5, and max-tokens for 8192. We also employed a standard warmup-stable-decay learning rate schedule to ensure stable training and better convergence. Through fine-tuning, we sampled approximately 180,000 training VQA data and 0.8 billion tokens in total. The data ratio was approximately 4:3:2, and the trained tokens ra- tio was 2:6:2 for each task (referring:grounding:generating). The training process for 3 epochs took about 2 days on 8 NVIDIA A100 GPUs with a batch size of 2 per GPU (total global batch size is 16). Reinforcement Learning (GRPO, DPO) Although the model has achieved decent results on various tasks during the SFT phase, the success rate and quality have not yet reached a particularly high standard. For example, the model fails on rare UI components in referring tasks and lacks detail and precise alignment in grounding tasks. There is also a need to further enhance the visual quality of the gen- erations. Given that the generated outputs may be lengthy, the stability of format following also requires improvement. To address these issues, we designed a series of verifiable and task-specific rewards and trained our model using the GRPO method for understanding tasks. And we constructed a preference dataset and trained our model using the DPO method for generation tasks. Referring.We observed that significant data imbalance adversely affected the model’s classification performance during the SFT phase, where text, icons, and images account for over 80%. Thus, we identified 8 challenging compo- nent categories (SelectabletextButton, closeButton, popup, checkedText, checkTextView, etc.) to further strengthen. Additionally, we defined hard samples by manually select- ing difficult samples and running inference five times with the SFT model; if it is incorrect even once, it is considered a hard sample. Next, we defined the rewards, primarily focusing on classification accuracy for these challenging component categories and adherence to JSON format. The final GRPO reward for the referring task is defined as follows: Rref=R acc+R format (1) Grounding.We used dual Intersection over Union (dual IOU) (Wang et al. 2025) to measure the quality of predicted boxes against ground-truths. Specifically, the dual IOU func- tion consists of two parts (recall IOU rewardRR IOUand pre- cision IOU rewardRP IOU). forNground-truth bounding boxes{b i}N i=1andMpre- dicted bounding boxesn ˆbjoM j=1, The recall IOU reward isdetermined as follows: RR IOU=1 NNX i=1IoU\u0010 ˆbj,bi\u0011 (2) which means each ground-truth boxb iis greedily matched with the predicted bounding box ˆbjyielding the highest (IoU). Similarly, we also define a precision IoU re- ward,RR IOU, by greedily matching each predicted box ˆbj with the ground-truth box that yields the highest IoU: RP IOU=1 MMX j=1IoU\u0010 bi,ˆbj\u0011 (3) According to the practice of (Luo et al. 2025), we didn’t train the chain-of-thought(CoT) for the grounding task to get better performance. As JSON format reward is also con- cerned, the final GRPO reward for the grounding task is de- fined as follows: Rground =RR IOU+RP IOU+R format (4) Generation.We referred Web2Code (Yun et al. 2024), for images rendered from UI generations, employed Qwen2.5- VL-72B to query scores across four categories scores as fol- lows: 1) Visual Structure",
    "con- cerned, the final GRPO reward for the grounding task is de- fined as follows: Rground =RR IOU+RP IOU+R format (4) Generation.We referred Web2Code (Yun et al. 2024), for images rendered from UI generations, employed Qwen2.5- VL-72B to query scores across four categories scores as fol- lows: 1) Visual Structure & Alignment, 2) Color & Aes- thetic Design, 3) Textual & Content Consistency, 4)Inter- face & Interactivity. The scores for each category range from 0 to 10. We then constructed a DPO dataset totaling 8,000 samples, where positive samples primarily consist of high- scoring rendered data, and negative samples include low- scoring rendered data as well as data that failed rendering due to incorrect JSON format. We observe that using only the DPO component may lead to a decrease in the prob- ability of positive samples. Inspired by work (Wang et al. 2024b), we additionally introduced SFT loss, which helps the model learn how to generate high-quality responses. The DPO loss function is defined as follows: Ltotal=L DPO+λL SFT (5) DPO Loss Component: LDPO=−E (q,yw,yl)∼Dlog σ\u0014 βlogπθ(yw|q) πref(yw|q)−βlogπθ(yl|q) πref(yl|q)\u0015 (6) SFT Loss Component: LSFT(θ) =−E (x,y)∼D SFT\u0002 logπ θ(y|x)\u0003 (7) whereπ θandπ refdenote the learnable and reference model,yw, yl∼πθ(·|q)represent winning (preferred) and lossing (rejected) response for queryq,σis the sigmoid function.λcontrols balance for SFT loss and DPO loss and βcontrols KL divergence penalty. For the entire reinforcement learning stage, we sequen- tially applied DPO for generating, GRPO for referring, and GRPO for grounding to generate our final modelUI-UG- 7B. The learning rate is set to 3e-6,βis set to 0.05, andλ is set to 1.0. For GRPO training, we set the learning rate to 1e-6 and the global batch size to 64. We adopt GRPO for 2 epochs and DPO for 3epochs on the same GPUs as SFT phase. The whole process took about 6 hours. ModelReferring Grounding Format Category OCR TextColor Format mAP AP50 AP75 GPT-4o 1 0.273 0.193 0.350 0.932 0.018 0.029 0.008 Claude 3.7 Sonnet 0.953 0.680 0.400 0.462 0.939 0.010 0.020 0.001 Gemini 2.5 Pro 1 0.400 0.157 0.514 0.959 0.013 0.021 0.004 Qwen2.5-VL-7B 0.419 0.527 0.586 0.649 0.923 0.092 0.120 0.064 Qwen2.5-VL-32B 0.317 0.754 0.841 0.447 0.918 0.183 0.258 0.107 Qwen2.5-VL-72B 0.575 0.744 0.826 0.686 0.918 0.266 0.345 0.187 Ferret-UI2 (Gemma-2B) 1 0.789 - - 0.715 0.070 0.108 0.032 Ferret-UI2 (Llama3-8B) 1 0.758 - - 0.855 0.083 0.119 0.046 OmniParser V2 - - - - - 0.290 0.382 0.198 UI-UG-7B-SFT (U Only 1 0.968 0.850 0.932 1 0.527 0.547 0.507 UI-UG-7B-SFT 1 0.9720.859 0.9371 0.544 0.567 0.527 UI-UG-7B 10.9740.854 0.933 10.559 0.578 0.540 Table 1: Performance comparison of different models on referring and grounding tasks. Experiment We conducted comprehensive comparisons with existing models on the UI Elements Dataset across two core tasks: UI understanding and UI generation. Our evaluation en- compasses both general-purpose MLLMs (including closed- source models like GPT-4o, Claude 3.7 Sonnet, and Gem- ini 2.5 Pro, as well as open-source alternatives from the Qwen2.5-VL series) and the latest well-known UI domain- specific models (Ferret-UI2 and OmniParser V2). UI Understanding Evaluation Evaluation Metrics.For UI understanding evaluation, we utilized",
    "en- compasses both general-purpose MLLMs (including closed- source models like GPT-4o, Claude 3.7 Sonnet, and Gem- ini 2.5 Pro, as well as open-source alternatives from the Qwen2.5-VL series) and the latest well-known UI domain- specific models (Ferret-UI2 and OmniParser V2). UI Understanding Evaluation Evaluation Metrics.For UI understanding evaluation, we utilized our modern complex UI dataset described before, focusing on two primary tasks. The referring task requires describing element attributes given ground-truth bounding boxes, evaluated through three metrics: 1) category recog- nition accuracy for element classification, 2) OCR accu- racy computed as1−Ndiff NtotalwhereN Ndiffdenotes differ- ing characters, 3) color recognition accuracy measured byPh (rpred−rgt 255)2+ (gpred−ggt 255)2+ (bpred−bgt 255)2i . We evaluated UI element localization performance for grounding tasks using standard object detection metrics: mean Average Precision (mAP) and Average Precision at IoU thresholds of 0.5 and 0.75 (AP50, AP75). We also evaluated the instruction following capabilities by JSON format accuracy for both tasks. Implementation Details.All general-purpose MLLMs received prompt-based instructions specifying UI categories and output formats with inference temperature 0. We imple- mented model-specific coordinate normalization: 0-1 range normalization for GPT-4o and Claude 3.7 Sonnet versus 1000*scaling for Gemini 2.5 Pro. For domain-specific mod- els, we developed categorical mappings to align disparate UI taxonomies. Special handling includes excluding Ferret-UI2 from OCR/color tasks due to its lack of multilingual support and limiting OmniParser V2 evaluation to grounding tasks as its traditional vision model architecture.Results.As summarized in Table 1, our UI-UG-7B model achieved SOTA performance on both referring tasks (Format:1, Category:0.974, OCR:0.854, TextColor:0.933) and grounding tasks (Format:1, mAP:0.559, AP50:0.578, AP75:0.54). Closed-source MLLMs (GPT-4o, Claude 3.7 Sonnet, Gemini 2.5 Pro) performed poorly on referring tasks and exhibited fundamental limitations in spatial un- derstanding and perception for regional information. They also demonstrated significant detection misses and coordi- nate errors in grounding tasks, resulting in predicted bound- ing boxes deviating substantially from ground-truth. Fur- ther granular analysis revealed that these general-purpose models lack recognition and classification capabilities for UI elements, particularly rare ones. Even their performance on the most common elements like text (OCR Grounding) still remained low, aligning with recent findings by other researchers (Li et al. 2025). In contrast, the open-source Qwen2.5-VL (7B, 32B, 72B) model, which is the founda- tion for our UI-UG model, possesses stronger capabilities in both referring and grounding tasks. However, it struggles significantly with format accuracy, especially in referring tasks(under 0.6). Among domain-specific models, Ferret-UI2 (Gemma- 2B, Llama3-8B) showed marginal superiority over the best general-purpose MLLM in UI classification tasks. Neverthe- less, its referring and grounding performance lagged far be- hind our model. We attributed this gap partly to its reliance on older datasets (e.g., RICO). OmniParser V2 achieved im- proved grounding performance compared to Ferret-UI2, yet still remained inferior to our approach (mAP:0.29 vs 0.559). We also observed that our final model UI-UG, which is re- inforcement learning enhanced, demonstrates a more signif- icant improvement compared to the SFT model, especially pronounced for the grounding tasks. UI Generation Evaluation Evaluation Protocol.We evaluated UI generation in six key categories (each score rated 0-10, totaling 60) through the following: 1) JSON",
    "our final model UI-UG, which is re- inforcement learning enhanced, demonstrates a more signif- icant improvement compared to the SFT model, especially pronounced for the grounding tasks. UI Generation Evaluation Evaluation Protocol.We evaluated UI generation in six key categories (each score rated 0-10, totaling 60) through the following: 1) JSON format compliance rate measuring DSL syntax, 2) CLIP-based visual similarity between gen- Format AccuracyVisual Structure & AlignmentColor & Aesthetic Design T extual & Content Consistency Interface & InteractivityReference SimilarityGPT-4o Claude3.7 SonnetGemini2.5 Pro Qwen2.5-VL-72BUI-UG-7B-SFT UI-UG-7BFigure 4: Generation scores for different models, with each dimension normalized. Our model UI-UG achieved quality enhancements through reinforcement learning and now ap- proaches the level of current powerful larger models. erated and reference UIs, and 3) MLLM-based scoring in four categories following Web2Code. Performance Analysis.Figure 4 presents the radar chart of scores from various MLLMs. Due to training on the UI DSL Dataset, our model achieved the highest score in gen- erative format stability. Following DPO training, our model showed a nearly 14.5% improvement in MLLM scores across four dimensions (from 36.7 to 42.02), demonstrating the effectiveness of our preference learning via DPO. Al- though its generative quality did not surpass that of closed- source models (Claude 3.7 Sonnet: 45.08, GPT-4o: 43.76, Gemini 2.5 Pro: 44.80), it performed on par with the model that produced training data (Qwen2.5-VL-72B: 42.15). Generating speed was also measured. By fine-tuning a smaller model (7B), we reduced system prompt tokens and achieved lower first token latency with faster model infer- ence. In our tests, our model deployed on two NVIDIA L20 GPUs completed a full output on average of 5.2s, while other models required 20-30s. The GPTQ / AWQ-INT4 quantized version can further reduce inference time to under 2s, en- abling real-time generation during user conversations. Ablation Study Table 2 shows the result of our additional ablation study, where theudy, where the, where the scores for each task are aggregated. First, we experimented with trainable pa- rameters for SFT, including: only LLM, Adapter+LLM, and ViT+Adapter+LLM. The Adapter+LLM strategy achieved the best-balanced performance and is adopted in the next experiments. Next, we compared models trained exclusively on UI un- derstanding data versus UI generation data. Incorporating UI understanding data enhanced both the quality and score of UI generation, while the model trained solely on understand- ing data showed no significant difference in understandingModelReferringGroundingGenerating RefScore mAP GenScore SFT L 3.685 0.489 36.65 SFT A+L 3.7310.54436.70 SFT V+A+L 3.7400.45136.94 SFT A+L(UOnly) 3.792 0.527 - SFT A+L(GOnly) - - 36.52 SFT A+L+GRPO U 3.777 0.56937.34 SFT A+L+DPO G 3.764 0.53542.53 SFT A+L+Both(Final) 3.761 0.559 42.02 Table 2: Ablation study on training strategy. Final model UI- UG used SFT(A+L) + Both (GRPO for Understanding, DPO for Generation), and achieved the most balanced excellent performance. (L: LLM, A: Adapter, V: VIT) tasks. This confirms that jointly training understanding and generation tasks improves model accuracy more effectively through their synergistic knowledge transfer. In reinforcement learning experiments, both GRPO and DPO demonstrated significantly improved performance in their respective domains (RefScore +1.2%, mAP +4.6%, GenScore +15.9%) compared to the SFT model. Our",
    "V: VIT) tasks. This confirms that jointly training understanding and generation tasks improves model accuracy more effectively through their synergistic knowledge transfer. In reinforcement learning experiments, both GRPO and DPO demonstrated significantly improved performance in their respective domains (RefScore +1.2%, mAP +4.6%, GenScore +15.9%) compared to the SFT model. Our final model, which combines SFT, GRPO, and DPO, achieved the most balanced excellent performance for all tasks (RefScore: 3.761, mAP: 0.559, GenScore: 42.02). Extended Zero-shot Evaluation Furthermore, we conducted two additional zero-shot experi- ments to validate model generalizability. First, we randomly sampled 130 RICO images to build a streamlined UI un- derstanding benchmark. Our model maintains superiority with 82% category classification accuracy versus Ferret-UI2 (Llama3-8B)’s 70%, with 0.15 detection mAP versus Ferret- UI2’s 0.08 and OmniParser V2’s 0.11. Visualization of an- notations further verifies that the performance decline in this benchmark stems from RICO’s erroneous labeling rather than model overfitting. Additionally, we evaluated captioned grounding task on ScreenSpot (Cheng et al. 2024) benchmark, despite having no such training data. However, this UI-related task showed a 24.8% decrease (from 84.7 to 63.7) in localization met- rics compared to our base model (Qwen2.5-VL-7B). We at- tribute this to the inherent opposition between captioning (describing UI) and grounding (generating UI from descrip- tions), which prevents knowledge sharing, unlike the com- plementary tasks of UI understanding and generation. More future work and investigation will focus on this issue. Conclusion We propose UI-UG, a unified MLLM that achieves state-of- the-art (SOTA) performance on modern complex UI under- standing tasks and matches the UI generation performance of larger MLLMs at significantly lower computational cost. As well, we are planning to open-source the model, eval- uation benchmarks, and UI rendering toolkits in the near fu- ture to support community development. References Anthropic. 2025. Claude 3.7 Sonnet and Claude Code. https: //www.anthropic.com/news/claude-3-7-sonnet. Accessed: 2025-02-25. Bai, J.; Bai, S.; Yang, S.; Wang, S.; Tan, S.; Wang, P.; Lin, J.; Zhou, C.; and Zhou, J. 2023. Qwen-VL: A Versatile Vision- Language Model for Understanding, Localization, Text Reading, and Beyond.arXiv preprint arXiv:2308.12966. Bai, S.; Chen, K.; Liu, X.; Wang, J.; Ge, W.; Song, S.; Dang, K.; Wang, P.; Wang, S.; Tang, J.; Zhong, H.; Zhu, Y .; Yang, M.; Li, Z.; Wan, J.; Wang, P.; Ding, W.; Fu, Z.; Xu, Y .; Ye, J.; Zhang, X.; Xie, T.; Cheng, Z.; Zhang, H.; Yang, Z.; Xu, H.; and Lin, J. 2025. Qwen2.5-VL Technical Report.arXiv preprint arXiv:2502.13923. Beltramelli, T. 2018. pix2code: Generating code from a graphical user interface screenshot. InProceedings of the ACM SIGCHI symposium on engineering interactive com- puting systems, 1–6. Bunian, S.; Li, K.; Jemmali, C.; Harteveld, C.; Fu, Y .; and Seif El-Nasr, M. S. 2021. Vins: Visual search for mobile user interface design. InProceedings of the 2021 CHI Con- ference on Human Factors in Computing Systems, 1–14. Cai, D.; Yu, S.; Wen, J.-R.; and Ma, W.-Y . 2003. Vips: a vision-based page segmentation algorithm. Chen, X.; Zhao, Z.; Chen, L.; Zhang, D.; Ji, J.; Luo, A.; Xiong, Y .; and Yu, K. 2021. Websrc: A dataset for web- based structural reading",
    "Human Factors in Computing Systems, 1–14. Cai, D.; Yu, S.; Wen, J.-R.; and Ma, W.-Y . 2003. Vips: a vision-based page segmentation algorithm. Chen, X.; Zhao, Z.; Chen, L.; Zhang, D.; Ji, J.; Luo, A.; Xiong, Y .; and Yu, K. 2021. Websrc: A dataset for web- based structural reading comprehension.arXiv preprint arXiv:2101.09465. Cheng, K.; Sun, Q.; Chu, Y .; Xu, F.; Li, Y .; Zhang, J.; and Wu, Z. 2024. Seeclick: Harnessing gui grounding for ad- vanced visual gui agents.arXiv preprint arXiv:2401.10935. Christiano, P. F.; Leike, J.; Brown, T.; Martic, M.; Legg, S.; and Amodei, D. 2017. Deep reinforcement learning from human preferences.Advances in neural information pro- cessing systems, 30. Comanici, G.; Bieber, E.; Schaekermann, M.; Pasupat, I.; Sachdeva, N.; Dhillon, I.; Blistein, M.; Ram, O.; Zhang, D.; Rosen, E.; et al. 2025. Gemini 2.5: Pushing the fron- tier with advanced reasoning, multimodality, long context, and next generation agentic capabilities.arXiv preprint arXiv:2507.06261. Deka, B.; Huang, Z.; Franzen, C.; Hibschman, J.; Afergan, D.; Li, Y .; Nichols, J.; and Kumar, R. 2017. Rico: A mobile app dataset for building data-driven design applications. In Proceedings of the 30th annual ACM symposium on user interface software and technology, 845–854. Deka, B.; Huang, Z.; and Kumar, R. 2016. ERICA: Interac- tion mining mobile apps. InProceedings of the 29th annual symposium on user interface software and technology, 767– 776. Dosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn, D.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.; Heigold, G.; Gelly, S.; et al. 2020. An image is worth 16x16 words: Transformers for image recognition at scale.arXiv preprint arXiv:2010.11929.Du, Y .; Jiang, S.; Han, Z.; Liu, X.; Ding, W.; Liang, J.; Wu, H.; Li, S.; Li, X.; Xu, Y .; et al. 2021. PP-OCRv2: Bag of Tricks for Ultra Lightweight OCR System.arXiv preprint arXiv:2109.03144. Guo, D.; Yang, D.; Zhang, H.; Song, J.; Zhang, R.; Xu, R.; Zhu, Q.; Ma, S.; Wang, P.; Bi, X.; et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.arXiv preprint arXiv:2501.12948. Hurst, A.; Lerer, A.; Goucher, A. P.; Perelman, A.; Ramesh, A.; Clark, A.; Ostrow, A.; Welihinda, A.; Hayes, A.; Rad- ford, A.; et al. 2024. Gpt-4o system card.arXiv preprint arXiv:2410.21276. Jain, N.; Han, K.; Gu, A.; Li, W.-D.; Yan, F.; Zhang, T.; Wang, S.; Solar-Lezama, A.; Sen, K.; and Stoica, I. 2024. Livecodebench: Holistic and contamination free eval- uation of large language models for code.arXiv preprint arXiv:2403.07974. Jain, V .; Agrawal, P.; Banga, S.; Kapoor, R.; and Gulyani, S. 2019. Sketch2Code: transformation of sketches to UI in real-time using deep neural network.arXiv preprint arXiv:1910.08930. Jocher, G.; Chaurasia, A.; and Qiu, J. 2023. Ultralytics YOLOv8. Lee, K.; Joshi, M.; Turc, I. R.; Hu, H.; Liu, F.; Eisensch- los, J. M.; Khandelwal, U.; Shaw, P.; Chang, M.-W.; and Toutanova, K. 2023. Pix2struct: Screenshot parsing as pre- training for visual language understanding. InInternational Conference on Machine Learning, 18893–18912. PMLR. Li, M.; Zhang, R.; Chen, J.; Gu, J.; Zhou, Y .; Dernoncourt, F.; Zhu, W.; Zhou, T.; and Sun, T. 2025. Towards visual text grounding of multimodal large language model.arXiv preprint arXiv:2504.04974. Li, Z.; You,",
    "parsing as pre- training for visual language understanding. InInternational Conference on Machine Learning, 18893–18912. PMLR. Li, M.; Zhang, R.; Chen, J.; Gu, J.; Zhou, Y .; Dernoncourt, F.; Zhu, W.; Zhou, T.; and Sun, T. 2025. Towards visual text grounding of multimodal large language model.arXiv preprint arXiv:2504.04974. Li, Z.; You, K.; Zhang, H.; Feng, D.; Agrawal, H.; Li, X.; Moorthy, M. P. S.; Nichols, J.; Yang, Y .; and Gan, Z. 2024. Ferret-ui 2: Mastering universal user interface understanding across platforms.arXiv preprint arXiv:2410.18967. Liu, Z.; Sun, Z.; Zang, Y .; Dong, X.; Cao, Y .; Duan, H.; Lin, D.; and Wang, J. 2025. Visual-rft: Visual reinforcement fine- tuning.arXiv preprint arXiv:2503.01785. Luo, R.; Wang, L.; He, W.; and Xia, X. 2025. Gui-r1: A gen- eralist r1-style vision-language action model for gui agents. arXiv preprint arXiv:2504.10458. Nguyen, T. A.; and Csallner, C. 2015. Reverse engineer- ing mobile application user interfaces with remaui (t). In 2015 30th IEEE/ACM international conference on auto- mated software engineering (ASE), 248–259. IEEE. Radford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.; Agarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.; et al. 2021. Learning transferable visual models from nat- ural language supervision. InInternational conference on machine learning, 8748–8763. PmLR. Rafailov, R.; Sharma, A.; Mitchell, E.; Manning, C. D.; Er- mon, S.; and Finn, C. 2023. Direct preference optimization: Your language model is secretly a reward model.Advances in neural information processing systems, 36: 53728–53741. Shen, H.; Liu, P.; Li, J.; Fang, C.; Ma, Y .; Liao, J.; Shen, Q.; Zhang, Z.; Zhao, K.; Zhang, Q.; et al. 2025. Vlm-r1: A sta- ble and generalizable r1-style large vision-language model. arXiv preprint arXiv:2504.07615. Song, R.; Liu, H.; Wen, J.-R.; and Ma, W.-Y . 2004. Learning block importance models for web pages. InProceedings of the 13th international conference on World Wide Web, 203– 211. Su, J.; Ahmed, M.; Lu, Y .; Pan, S.; Bo, W.; and Liu, Y . 2024. Roformer: Enhanced transformer with rotary position em- bedding.Neurocomputing, 568: 127063. Ting Zhou, X. H. X. S. K. C. H. W., Yanjie Zhao. 2024. Bridging Design and Development with Automated Declar- ative UI Code Generation. arXiv:2409.11667. Wan, J.; Song, S.; Yu, W.; Liu, Y .; Cheng, W.; Huang, F.; Bai, X.; Yao, C.; and Yang, Z. 2024a. Omniparser: A uni- fied framework for text spotting key information extraction and table recognition. InProceedings of the IEEE/CVF con- ference on computer vision and pattern recognition, 15641– 15653. Wan, Y .; Wang, C.; Dong, Y .; Wang, W.; Li, S.; Huo, Y .; and Lyu, M. R. 2024b. Automatically generating UI code from screenshot: A divide-and-conquer-based approach.arXiv preprint arXiv:2406.16386. Wang, H.; Li, X.; Huang, Z.; Wang, A.; Wang, J.; Zhang, T.; Zheng, J.; Bai, S.; Kang, Z.; Feng, J.; et al. 2025. Traceable evidence enhanced visual grounded reasoning: Evaluation and methodology.arXiv preprint arXiv:2507.07999. Wang, P.; Bai, S.; Tan, S.; Wang, S.; Fan, Z.; Bai, J.; Chen, K.; Liu, X.; Wang, J.; Ge, W.; Fan, Y .; Dang, K.; Du, M.; Ren, X.; Men, R.; Liu, D.; Zhou, C.; Zhou, J.; and Lin, J. 2024a.",
    "enhanced visual grounded reasoning: Evaluation and methodology.arXiv preprint arXiv:2507.07999. Wang, P.; Bai, S.; Tan, S.; Wang, S.; Fan, Z.; Bai, J.; Chen, K.; Liu, X.; Wang, J.; Ge, W.; Fan, Y .; Dang, K.; Du, M.; Ren, X.; Men, R.; Liu, D.; Zhou, C.; Zhou, J.; and Lin, J. 2024a. Qwen2-VL: Enhancing Vision-Language Model’s Perception of the World at Any Resolution.arXiv preprint arXiv:2409.12191. Wang, W.; Chen, Z.; Wang, W.; Cao, Y .; Liu, Y .; Gao, Z.; Zhu, J.; Zhu, X.; Lu, L.; Qiao, Y .; et al. 2024b. En- hancing the reasoning ability of multimodal large language models via mixed preference optimization.arXiv preprint arXiv:2411.10442. Wu, J.; Zhang, X.; Nichols, J.; and Bigham, J. P. 2021. Screen parsing: Towards reverse engineering of ui models from screenshots. InThe 34th Annual ACM Symposium on User Interface Software and Technology, 470–483. You, K.; Zhang, H.; Schoop, E.; Weers, F.; Swearngin, A.; Nichols, J.; Yang, Y .; and Gan, Z. 2024. Ferret-ui: Grounded mobile ui understanding with multimodal llms. InEuropean Conference on Computer Vision, 240–255. Springer. Yu, W.; Yang, Z.; Wan, J.; Song, S.; Tang, J.; Cheng, W.; Liu, Y .; and Bai, X. 2025. Omniparser v2: Structured-points- of-thought for unified visual text parsing and its general- ity to multimodal large language models.arXiv preprint arXiv:2502.16161. Yun, S.; Thushara, R.; Bhat, M.; Wang, Y .; Deng, M.; Wang, J.; Tao, T.; Li, J.; Li, H.; Nakov, P.; et al. 2024. Web2code: A large-scale webpage-to-code dataset and evaluation frame- work for multimodal llms.Advances in neural information processing systems, 37: 112134–112157.Zhang, J.; Huang, J.; Yao, H.; Liu, S.; Zhang, X.; Lu, S.; and Tao, D. 2025. R1-vl: Learning to reason with multimodal large language models via step-wise group relative policy optimization.arXiv preprint arXiv:2503.12937. Ziegler, D. M.; Stiennon, N.; Wu, J.; Brown, T. B.; Radford, A.; Amodei, D.; Christiano, P.; and Irving, G. 2019. Fine- tuning language models from human preferences.arXiv preprint arXiv:1909.08593. Appendix 1. Details for Data Preparation Definition of UI Categories.Figure 5 presents all UI element categories we designed along with their exam- ples, including a total of 19 distinct types (’Checked Box’, ’Unchecked Box’, ’Check Text View’, ’Close’, ’Drop Down’, ’Edit Text’, ’Icon’, ’Image’, ’More’, ’Nonselectable Text Button’, ’Selectable Text Button’, ’Pop Up’, ’Progress’, ’Switch’, ’Return’, ’Checked Text’, ’Unchecked Text’, ’Text’, ’Text Button’). Prompts for UI DSL generation. You are an experienced UI designer and a skilled front-end developer. Based on the user’s input, return a UI DSL that conforms to the following definition: UI DSL Definition: UI DSL is a domain-specific language in JSON format that describes UI content features and styling features. The ba- sic node attributes of UI DSL include: ”type”, ”name”, ”className”, ”params”, and ”children”. type: Indicates the node type, with optional values being ”Component” or ”Tag”. name: If type is ”Component”, this attribute is the name of a component from the ”available components” list. If type is ”Tag”, this attribute is the name of an HTML tag. className: Tailwind CSS class names. Use relative layouts (avoid fixed). Width/height can be absolute (e.g., w-[100px]) or relative (e.g., w-full, w-screen, w-[100%]). params: Represents the",
    "attribute is the name of a component from the ”available components” list. If type is ”Tag”, this attribute is the name of an HTML tag. className: Tailwind CSS class names. Use relative layouts (avoid fixed). Width/height can be absolute (e.g., w-[100px]) or relative (e.g., w-full, w-screen, w-[100%]). params: Represents the content features of the node. If type is ”Component”, the ”value” attribute in params corre- sponds to the component’s property values. If type is ”Tag”, params contains the attributes and values of the HTML tag. The params node must include a ”bindType” attribute with possible values ”Static” or ”Data”: ”Static”: The attribute is static. ”Data”: The attribute is dynamic, referencing a vari- able from the ”real data” (e.g., promotions[0].title). chil- dren: A list of child nodes. Example Node Data: json {”type”: ”Tag”, ”name”: ”a”, ”className”: ”color-gray- 500”, ”params”:{”textContent”:{”bindType”: ”Static”, ”value”: ”Test Link”}}} {”type”: ”Tag”, ”name”: ”a”, ”className”: ”color- gray-500”, ”params”:{”textContent”:{”bindType”: ”Data”, ”bindField”: ”promotions[0].title”}}} Design Style Reference: Button Styles: Base structure: inline-flex items-center justify-center font-medium transition-colors focus:outline- none focus:ring-2 focus:ring-offset-2 disabled:opacity-50 disabled:pointer-events-none Primary button: bg-blue-600 text-white hover:bg-blue-700 active:bg-blue-800 rounded- 3xl py-2 px-4 border-0 Secondary button: bg-white text- gray-700 hover:bg-gray-50 active:bg-gray-100 rounded-3xl py-2 px-4 border border-gray-300 Output Requirements: If the ”available components” list is empty, only use ”Tag” nodes (no ”Component” nodes). If components are avail- able, prioritize using them where appropriate. Use the pro- vided ”real data” to populate the UI DSL content dynam- ically—reference variables instead of hardcoding values. Ensure all dynamic ”bindField” paths correctly match the data structure. If the user provides a UI screenshot, match the generated layout to the element sizes and visual style in the image. Otherwise, default to mobile-friendly UI card di- mensions. The output must have a single root node. Do not include comments. Ensure the result is valid JSON. Avoid direct image URLs—describe image content in detail in- stead. Omit the ”params” or ”children” attributes if they are empty. To ensure aesthetic appeal, you may adjust content (add/modify/delete), but avoid emojis. Use tags and gradi- ent backgrounds to enhance design quality. The UI DSL must adhere to: ·Clear content hierarchy ·Subtle rounded corners and shadows ·Consistent spacing ·Clear interactive feedback ·Flat yet slightly dimensional design ·Strong us- ability and readability ·Modular component design User Input: Demand Description:{demand} Real Data:{mock json} Outputs for UI DSL generation. {”type”: ”Tag”, ”name”: ”div”, ”className”: ”flex flex-col items-center justify-center bg-gradient-to-r from-pink-500 to-orange-500 rounded-3xl shadow-lg w-full max-w- sm p-4”, ”children”: [{”type”: ”Tag”, ”name”: ”img”, ”className”: ”w-full h-48 object-cover rounded-2xl mb-4”, ”params”:{”src”:{”bindType”: ”Static”, ”value”: ”mockImageUrl”}}},{”type”: ”Tag”, ”name”: ”div”, ”className”: ”text-center”, ”children”: [{”type”: ”Tag”, ”name”: ”h2”, ”className”: ”text-xl font-bold text-white mb-2”, ”params”:{”textContent”:{”bindType”: ”Data”, ”bindField”: ”title”}}},{”type”: ”Tag”, ”name”: ”p”, ”className”: ”text-base text-white mb-2”, ”params”: {”textContent”:{”bindType”: ”Data”, ”bindField”: ”eventDescription”}}},{”type”: ”Tag”, ”name”: ”p”, ”className”: ”text-base text-white mb-2”, ”params”: {”textContent”:{”bindType”: ”Data”, ”bindField”: ”dat- eRange”}}},{”type”: ”Tag”, ”name”: ”p”, ”className”: ”text-base text-white mb-2”, ”params”:{”textContent”: {”bindType”: ”Data”, ”bindField”: ”location”}}},{”type”: ”Tag”, ”name”: ”p”, ”className”: ”text-xl font-bold text- white”, ”params”:{”textContent”:{”bindType”: ”Data”, ”bindField”: ”price”}}}]},{”type”: ”Tag”, ”name”: ”a”, ”className”: ”mt-4 inline-flex items-center justify-center font-medium transition-colors focus: outline-none focus: ring-2 focus: ring-offset-2 disabled: opacity-50 disabled: pointer-events-none bg-blue-600 text-white hover: bg-blue- 700 active: bg-blue-800 rounded-3xl py-2",
    "”className”: ”text-base text-white mb-2”, ”params”:{”textContent”: {”bindType”: ”Data”, ”bindField”: ”location”}}},{”type”: ”Tag”, ”name”: ”p”, ”className”: ”text-xl font-bold text- white”, ”params”:{”textContent”:{”bindType”: ”Data”, ”bindField”: ”price”}}}]},{”type”: ”Tag”, ”name”: ”a”, ”className”: ”mt-4 inline-flex items-center justify-center font-medium transition-colors focus: outline-none focus: ring-2 focus: ring-offset-2 disabled: opacity-50 disabled: pointer-events-none bg-blue-600 text-white hover: bg-blue- 700 active: bg-blue-800 rounded-3xl py-2 px-4 border-0”, ”params”:{”href”:{”bindType”: ”Static”, ”value”: ”#”}, ”textContent”:{”bindType”: ”Static”, ”value”: ”Learn More”}}}]} In addition, Figure 6 presents the rendered UI image by Checked Box Unchecked Box Check Text View Close Drop Down Edit Text Icon Image More Nonselectable Text Button Selectable Text Button Pop Up Progress Switch Return Unchecked TextChecked Text Text / Text Button E-Mail:Figure 5: Display of all our UI categories. this DSL output. 2. Prompt Templates Prompts for Creating VQA Dataset. prompt templates grounding = [ ”List all the{category}items.”, ”Please list all the{category}in the image.”, ”Identify every{category}present in this picture.”, ”List all{category}shown in the image.”, ”Enumerate each{category}in the provided photo.”, ”What are all the{category}in this image? Please enumer- ate.”, ”Provide all{category}types found in this picture.”, ”Can you list every{category}appearing in this image?”, ”Give a detailed list of every{category}in the picture.”, ”Please identify all of the{category}in the image.”, ”List every type and position of{category}displayed in the image.” ] ModelReferring Grounding Format Category OCR TextColor Format mAP AP50 AP75 GPT-4o 1 0.273 0.193 0.350 0.932 0.018 0.029 0.008 Claude 3.7 Sonnet 0.953 0.680 0.400 0.462 0.939 0.010 0.020 0.001 Gemini 2.5 Pro 1 0.400 0.157 0.514 0.959 0.013 0.021 0.004 Qwen2.5-VL-7B 0.419 0.527 0.586 0.649 0.923 0.092 0.120 0.064 Qwen2.5-VL-32B 0.317 0.754 0.841 0.447 0.918 0.183 0.258 0.107 Qwen2.5-VL-72B 0.575 0.744 0.826 0.686 0.918 0.266 0.345 0.187 Ferret-UI2 (Gemma-2B) 1 0.789 - - 0.715 0.070 0.108 0.032 Ferret-UI2 (Llama3-8B) 1 0.758 - - 0.855 0.083 0.119 0.046 OmniParser V2 - - - - - 0.290 0.382 0.198 UI-UG-7B-SFT L 0.998 0.975 0.836 0.874 1 0.489 0.508 0.47 UI-UG-7B-SFT A+L 1 0.972 0.859 0.9 1 0.544 0.567 0.527 UI-UG-7B-SFT V+A+L 0.998 0.972 0.862 0.906 1 0.451 0.466 0.435 UI-UG-7B-SFT L(UOnly) 1 0.973 0.894 0.935 1 0.538 0.561 0.516 UI-UG-7B-SFT A+L(UOnly) 1 0.968 0.892 0.932 1 0.527 0.547 0.507 UI-UG-7B-SFT V+A+L (UOnly) 1 0.972 0.906 0.953 1 0.499 0.526 0.472 UI-UG-7B-SFT A+L+GRPO U 1 0.977 0.864 0.936 1 0.569 0.598 0.552 UI-UG-7B-SFT A+L+DPO G 1 0.974 0.855 0.935 1 0.535 0.558 0.512 UI-UG-7B 1 0.974 0.854 0.933 1 0.559 0.578 0.540 Table 3: Performance comparison of different models on referring and grounding tasks. Figure 6: Rendered UI Image by our DSL specification. prompt templates referring = [ ”Describe the region{box}.”, ”Describe the categories and properties of the UI in area {box}.”, ”List the types and characteristics of UI elements in region {box}.”, ”Provide details on the UI types and their attributes within area{box}.”, ”Explain the categories and features of the UI components in region{box}.”, ”Summarize the types and properties of UI present in area{box}.” ] prompt templates ocr = [ ”Describe the text present in area{box}.”, ”List all the textual information in area{box}.”, ”Please detail any text found in area{box}.”, ”What text appears in region{box}? Please explain.”, ”Summarize the main textual content in region{box}.”",
    "”Summarize the types and properties of UI present in area{box}.” ] prompt templates ocr = [ ”Describe the text present in area{box}.”, ”List all the textual information in area{box}.”, ”Please detail any text found in area{box}.”, ”What text appears in region{box}? Please explain.”, ”Summarize the main textual content in region{box}.” ] prompt templates textcolor = [ ”Describe the text colors in region{box}.”, ”List all text colors found in area{box}.”, ”What text colors are used in region{box}? ”, ”Summarize the colors of the text in region{box}.”, ”Please state the colors of various texts in area{box}.” ] prompt templates caption = [ ”Describe this UI image.”, ”Please describe this UI image.”, ”Introduce the content of this UI image.”, ”Explain in detail the content displayed in this UI image.”, ”Please analyze the content of this UI image.” ] prompt templates generation = [ ”Design a UI card to showcase the details of {uidescription}”, ”Design a concise, clear, and visually appealing UI card to display{ui description}”, ”Design a UI card to display{ui description}”, ”Please design a UI card displaying{ui description}”, ”Please generate a UI card to guide users to the Figure 7: Improvements of performance for referring tasks for each category by reinforcement learning. {uidescription}”, ”Design a UI card to showcase the details of {uidescription}. The following is the mock data for the UI card.{mock data}”, ”Design a concise, clear, and visually appealing UI card to display{ui description}. The following is the mock data for the UI card.{mock data}”, ”Design a UI card to display{ui description}. The follow- ing is the mock data for the UI card.{mock data}”, ”Please design a UI card displaying{ui description}. The following is the mock data for the UI card.{mock data}”, ”Please generate a UI card to guide users to the {uidescription}. The following is the mock data for the UI card.{mock data}” ] Prompts for General-purpose MLLMs’ UI Understand- ing Evaluation. catstr = ”text, image, SelectabletextButton, close, popup, icon, checkedText, NonselectabletextButton, unchecked- Text, uncheckedBox, checkTextView, switch, progress, textButton, dropDown, editText, return, more, checked- Box” For referring. Your task is to give description of the given region in the image from four perspectives. First, you should decide the category of the given region, the reference categories are ”cat str”. Second, you should describe the text content in the given region. Third, you should output the color the text you extract in the second step in the hexadecimal represen- tation such as #414141. You should output the answer in json format like: ”cate- gory”: ”the category you decide”, ”text”: ”the text you ex- tract”, ”text color”: [”the color you decide”] and do not re- turn any other content.For grounding. Your task is to detect all the given ui elements in the im- age. First, you should output the category of all the ui ele- ment you detect, you can refer to the following categories: ”cat str”. Second, you should output the bounding box of all the ui element you detect in the format of [x min, y min, xmax, y min].’ Please return the answer in json format like: [{”type”:{ui element}, ”box”:",
    "ui ele- ment you detect, you can refer to the following categories: ”cat str”. Second, you should output the bounding box of all the ui element you detect in the format of [x min, y min, xmax, y min].’ Please return the answer in json format like: [{”type”:{ui element}, ”box”: [x min,y min,x max,y max]”},...]. (optional: Scale the coordinates between 0 and 1.) **Note**: Please do not create new ui elements and do not return any other content.’ Prompts for Web2Code UI Generation Evaluation. You are an advanced AI model equipped with OCR and im- age understanding capabilities, capable of analyzing visual elements in detail. Your task is to assess two UI images and output a score be- tween 0 and 10 for each of the following questions. If the answer to a question is a definite YES, output a score of 10, signifying perfect similarity. Conversely, a definite NO should yield a score of 0, indicating no similarity. For an- swers that fall in between, assign a score accordingly, where a higher number indicates a greater degree of similarity. Only provide the numerical score for each question, with- out any additional text. Example contexts are provided for clarity. Examples provides the idea, but you can output any number in 0-10 range accordingly. Only output a comma separated list containing 4 numbers. DO NOT give score of 10 for any category unless otherwise the two images are identical. 1.Visual structure and alignment(Score: 0-10): Does the placement of elements like images, buttons, and text boxes aligned similarly on both images? Do the sizes and aspect ratios of images, buttons, and text boxes appear consistent across both images? Do both UI images exhibit a similar GPT4-o Qwen2.5 -VL-72B UI-UG OmniParser V2 Ferret-UI 2 Figure 8: Visual comparison of different models for grounding task for densely arranged UIs. level of visual harmony and balance in their design? (e.g., A score of 10 for identical visual structure and alignment, 5 for similar but not exact consistent, and 0 for completely different structure.) 2.Color match and Aesthetic Resemblance(Score: 0-10): How closely do the color schemes of the two UI images align in terms of background and text colors? Evaluate the similarity in hues, saturation, and overall color aesthetics. Is the overall aesthetic appeal (modern, minimalistic, tra- ditional, etc.) similar on both UI images? (e.g., A score of 10 for perfectly matching color schemes and identical aes- thetics, including identical hues and saturation levels, 6 for similar color palettes and styles with minor variations, and 0 for starkly different color schemes and aesthetics that create entirely different visual impacts.) 3.Textual and Content Consistency(Score: 0-10): Do the font type, size, style, and weight of two UI images similar? Do the words and sentences match between the two UI im- ages? (e.g., A score of 10 for complete uniformity in font type, size, style, weight across both UI images and identicaltext, 5 for consistency in font type and size but variations in style or weight, and 0 for wide disparities in font type, size, style, or weight, leading to a",
    "ages? (e.g., A score of 10 for complete uniformity in font type, size, style, weight across both UI images and identicaltext, 5 for consistency in font type and size but variations in style or weight, and 0 for wide disparities in font type, size, style, or weight, leading to a distinctly different textual ap- pearance and content.) 4.User Interface Consistency(Score: 0-10): Do the user interface elements (like menus, buttons, and forms) on both UI images share a similar design language and appearance? (e.g., A score of 10 for identical UI elements, 6 for slight de- sign variations, and 0 for completely different UI designs.) 3. More Experiment Results Results for UI Understanding Evaluation.Table 3 presents the results of referring and grounding tasks across all the models, supplemented with performance compar- isons from ablation experiments. Additionally, due to the imbalanced data distribution across different UI categories, the overall accuracy differences between models are not par- ticularly significant. We conducted a detailed breakdown of subcategories within UI-UG (as shown in Figure 7), which GPT4-o Qwen2.5 -VL-72B UI-UG OmniParser V2 Ferret-UI 2 Figure 9: Visual comparison of different models for grounding task for complex text-image mixed UIs. demonstrates that our reinforcement learning approach in- deed achieved improvements in challenging categories. Notably, we observedsignificantly lower mAP (mean Average Precision) valuesin other models compared to ours. To investigate this issue, we visualized some sample images from our test set. Especially, Figure 8 and Figure 9 respectively compare typical models (including GPT4-o, Qwen2.5-VL-72B, Ferret-UI 2, and OmniParser V2) with our UI-UG model on densely arranged UIs and complex text-image mixed UIs. Our analysis confirms that our model exhibits superior classification accuracy and detection preci- sion, particularly in: • More accurate identification of UI categories. • Tighter bounding box predictions along elements. It should be emphasized that our final AP metric averages performance acrossboth simple and complex categories. As well, given our focus onsmall element detectionin UI images, the evaluation criteria become inherently more strin-gent. These findings collectively validate that our evaluation outcomes align with theoretical expectations. In addition, we mentioned in the paper that our model still outperforms other models on the zero-shot RICO dataset, although the overall mAP is lower compared to its perfor- mance on our primary dataset. Figure 10 presents a visual comparison of grounding task results on the RICO dataset. We observe that our model’s predictions demonstrate finer granularity compared to the ground-truth annotations in the RICO dataset, with bounding boxes showing better align- ment. This visually substantiates the explanations provided in our paper. Results for UI Generation Evaluation.Figure 11 dis- plays a preview of our DPO dataset, featuring comparisons between high-scoring and low-scoring UIs. Table 4 presents the performance metrics of all general-purpose MLLMs (Multimodal Large Language Models) on the UI dataset, in- cluding detailed scores across various categories. Benefiting Ground Truth UI-UG OmniParser V2 Ferret-UI 2 Figure 10: Visual comparison of different models for grounding task in the RICO dataset. from Supervised Fine-tuning (SFT), our model achieves op- timal performance in formatting tasks. Furthermore, through Direct Preference Optimization (DPO) training, it demon-",
    "cluding detailed scores across various categories. Benefiting Ground Truth UI-UG OmniParser V2 Ferret-UI 2 Figure 10: Visual comparison of different models for grounding task in the RICO dataset. from Supervised Fine-tuning (SFT), our model achieves op- timal performance in formatting tasks. Furthermore, through Direct Preference Optimization (DPO) training, it demon- strates significant improvements in visual evaluation met- rics, reaching performance levels comparable to those of larger, more advanced general-purpose models. 4. Code & Demonstrations in the ZIP Package Codes and Data.In addition to the technical contribu- tions outlined in our paper, we have made availabletraining data samples(folder: data/ ) andmodel evaluation codes (folder: code/ ) to support reproducibility and extended anal- ysis. Real-time Rendering for UI generation.For additional visual demonstrations and dynamic examples, pleaserefer to the video(ui render demo.mp4 ) contained within the supplementary materials zip package. These multimedia re- sources provide extended illustrations of how to render UIsusing our model. UIs with high score UIs with lowscore Figure 11: Visual comparison of UIs with high score and low score (from DPO Dataset). Format Reference Visual Color & Textual Interface Total Model Accuracy Similarity Structure & Aesthetic & Content & Alignment Design Consistency Interactivity GPT-4o 8.80 8.97 6.36 6.59 6.82 6.22 43.76 Claude 3.7 Sonnet 8.80 9.02 6.90 7.11 6.49 6.76 45.08 Gemini 2.5 Pro 8.70 8.95 7.00 6.92 6.60 6.63 44.80 Qwen2.5-VL-72B 8.10 8.92 6.28 6.63 6.16 6.06 42.15 UI-UG-7B-SFT A+L(UOnly) 8.60 4.53 4.30 5.35 4.85 4.53 32.16 UI-UG-7B-SFT L 8.60 8.87 4.34 5.28 5.12 4.44 36.65 UI-UG-7B-SFT A+L 8.60 8.83 4.38 5.21 5.05 4.63 36.70 UI-UG-7B-SFT V+A+L 8.60 8.90 4.52 5.47 4.80 4.65 36.94 UI-UG-7B-SFT A+L+GRPO U 9.10 8.86 4.64 5.33 4.77 4.64 37.34 UI-UG-7B-SFT A+L+DPO G 9.50 8.92 5.70 6.29 6.47 5.65 42.53 UI-UG-7B 9.30 8.91 5.71 6.17 6.28 5.65 42.02 Table 4: Performance comparison of different models on generation tasks."
  ]
}