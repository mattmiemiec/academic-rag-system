{
  "filename": "2509.03972v1.pdf",
  "total_chunks": 7,
  "text_length": 22419,
  "chunks": [
    "Expanding Foundational Language Capabilities in Open-Source LLMs through a Korean Case Study Moreh∗ Abstract We introduce Llama-3-Motif, a language model consisting of 102 billion parame- ters, specifically designed to enhance Korean capabilities while retaining strong performance in English. Developed on the Llama 3 architecture, Llama-3-Motif employs advanced training techniques, including LlamaPro and Masked Structure Growth, to effectively scale the model without altering its core Transformer ar- chitecture. Using the MoAI platform for efficient training across hyperscale GPU clusters, we optimized Llama-3-Motif using a carefully curated dataset that main- tains a balanced ratio of Korean and English data. Llama-3-Motif shows decent performance on Korean-specific benchmarks, outperforming existing models and achieving results comparable to GPT-4. 1 Introduction The rapid advancement of large language models (LLMs) has significantly transformed the field of natural language processing. However, there are still notable performance gaps for languages like Korean, primarily due to the scarcity of high-quality datasets and the computational challenges involved in scaling models efficiently. To address these issues, we present Llama-3-Motif , a 102 billion-parameter language model designed to enhance the proficiency in Korean while maintaining robust performance in English. Llama-3-Motif builds on the Llama 3 [ 5] pre-trained model, incorpo- rating advanced training methodologies. Specifically, we employ techniques such as LlamaPro [15] for depth expansion and Masked Structure Growth (MSG) [ 16] for width expansion, allowing for scalable model growth without altering the core Transformer architecture [ 14]. These approaches aim to close the performance gap in Korean language tasks while preserving the overall linguistic capabilities of the model. A key component of our development process is the use of MoAI Platform2, an advanced AI infrastructure designed to streamline the training of large-scale models. This platform manages thousands of GPU clusters, offering features such as automatic parallelization, GPU virtualization, and dynamic GPU allocation. Using these capabilities, the MoAI platform accelerates experiments and optimizations, enabling extensive tasks such as hyperparameter tuning and adjustments for alignment without the burden of manual GPU management. This efficiency allows us to concentrate on refining Llama-3-Motif’s architecture and performance. We curate a comprehensive high-quality dataset of approximately 194 billion tokens, carefully balanced between Korean and English content to improve language proficiency. Through rigorous data collection and filtering processes, we ensure the dataset’s relevance and quality, enabling the model to tackle complex, domain-specific tasks in Korean. Following pretraining, we fine-tune and align the model using advanced techniques such as Noisy Embedding Instruction Fine Tuning (NEFTune) [ 7] and Kahneman-Tversky Optimization (KTO) [ 6], significantly improving its efficacy and generalization performance. ∗A detailed contributor list can be found in the appendix of this paper. 2https://moreh.io/productarXiv:2509.03972v1 [cs.CL] 4 Sep 2025 Evaluations using benchmarks such as the Korean Multitask Language Understanding (KMMLU) [ 13] metric and the KorMedMCQA [ 9] data set demonstrate that Llama-3-Motif surpasses existing Korean specialized models by 9- 40% / 4.9-16.6 points, while also achieving competitive results compared to GPT-4 [1]. Our contributions include the following. •Dataset Curation : Developing a high-quality, balanced Korean and English dataset to enhance bilingual language proficiency. •Model Scaling : Proposing an efficient methodology to scale",
    "surpasses existing Korean specialized models by 9- 40% / 4.9-16.6 points, while also achieving competitive results compared to GPT-4 [1]. Our contributions include the following. •Dataset Curation : Developing a high-quality, balanced Korean and English dataset to enhance bilingual language proficiency. •Model Scaling : Proposing an efficient methodology to scale a language model from 70 billion to 102 billion parameters through progressive training techniques. •Advanced Fine-Tuning Techniques : Utilizing NEFTune and KTO in post-training to enhance the model’s performance. •Performance Evaluation : Demonstrating Llama-3-Motif’s decent performance on special- ized benchmarks for Korean language tasks. Through this work, we aim to close the performance gap in Korean language models while providing valuable insights into efficient model scaling and the use of advanced AI infrastructure for less- resourced languages. 2 Model architecture and scaling Llama-3-Motif utilizes a standard dense Transformer architecture [ 14], with the Llama 3 70B [ 5] pretrained model as its base. A key contributor to our performance improvements is the increased scale of the model. The expansion of our model was achieved through a progressive training approach. This method- ology facilitates the efficient scaling of the existing model by leveraging its current weights while maintaining the original Transformer architecture and enhancing performance. The adoption of this progressive training strategy represents a crucial design decision aimed at improving pre-trained Transformer models. In this context, we explored two primary options: expanding the model’s depth and its width. The \"depth expansion\" refers to increasing the number of layers, while \"width expansion\" involves enlarging the hidden size and intermediate size of various components, such as RMS normalization layers, feedforward layers, embedding layers, and attention layers. To determine the most effective approach for expansion, we conducted four preliminary experiments using Qwen 1.8B [ 3] as the base model. In these trials, we focused exclusively on increasing the depth by 50%, without implementing any width expansion. The experiments employed several methodological approaches, including: •Initialization of a 2.7 billion parameter model, expanded from Qwen 1.8B, based on a normal distribution derived from the base model’s parameters. • Staged expansion [12] • Implementation of LlamaPro [15] • Depth-up scaling [8] The selection of LlamaPro for depth expansion was based on our preliminary results. For width expansion, we employed MSG. The number of layers was increased by 20%, while the head dimension was preserved. Adjustments were made to the hidden and intermediate sizes to accommodate these changes. Table 1 presents the architecture of Llama-3-Motif. Thus, the expanded model was initialized by implementing LlamaPro’s depth expansion on the base model, followed by the width expansion facilitated by MSG which utilizes a masking mechanism to maintain the capabilities of the initial model by neutralizing the effects of the newly introduced neurons. This approach allows for a gradual emphasis on the importance of these neurons throughout the training process. Initially, the newly added parameters are masked, and they are progressively unmasked at each training step for every layer. 2 Table 1: Overview of Llama-3-Motif 102B Llama-3-Motif-102B Layers 96 Model Dimension 9,216 FFN Dimension 30,720 Attention Heads 72 Key/Value Heads 8 V ocabulary",
    "of these neurons throughout the training process. Initially, the newly added parameters are masked, and they are progressively unmasked at each training step for every layer. 2 Table 1: Overview of Llama-3-Motif 102B Llama-3-Motif-102B Layers 96 Model Dimension 9,216 FFN Dimension 30,720 Attention Heads 72 Key/Value Heads 8 V ocabulary Size 128,000 3 Infrastructure We employed the MoAI Platform( https://moreh.io/product ) to train Llama-3-Motif. The MoAI Platform is an AI infrastructure specifically designed to facilitate the development of large- scale deep learning models by managing thousands of GPU clusters required for both training and inference. Key features of the platform include automatic parallelization, GPU virtualization, and dynamic GPU allocation, all of which collectively enhance the efficiency of the model development process. In our work with Llama-3-Motif, we conducted extensive experiments that included hyperparameter tuning, alignment adjustment methods, exploration of scaling techniques, and NEFT embedding [ 7], using MoAI Platform with hundreds of AMD MI250 GPUs. These iterative experiments were essential for identifying the optimal scaling and training techniques for the better model. In particular, the platform’s automatic parallelization feature proved invaluable, enabling us to efficiently conduct multiple experiments without much considerations on manual configuration of the GPU infrastructure or concerns regarding efficient/effective resource allocation. By treating numerous GPUs as a single virtual device, we were able to concentrate solely on refining the training methods for the Llama- 3-Motif model, thereby significantly streamlining the development process and accelerating our progress toward better model performance. 4 Pre-training To tailor the model with expanded parameters for specific capabilities, additional pre-training is nec- essary. In particular, we identify the target domain data as a critical factor in enhancing performance on downstream tasks. The continual pre-training dataset utilized for our model consists of an extensive corpus of approx- imately 194 billion tokens. This dataset was meticulously curated to achieve a balance between Korean and English language proficiency, with a strong emphasis on Korean data to support our primary objective of enhancing the model’s proficiency in Korean. 4.1 Data composition The dataset maintained a carefully calibrated ratio of approximately 9:1 between Korean and English content. This distribution was strategically designed to achieve two primary objectives: •To prioritize the enhancement of the model’s Korean language capabilities through compre- hensive exposure to a diverse range of Korean texts. •To concurrently preserve the model’s proficiency in English, ensuring its effectiveness in bilingual or English only contexts. 4.2 Data collection The majority of the Korean dataset was sourced from web-crawled documents. We aggregated 194 billion tokens of data from a wide range of sources, including news articles, blog posts, and professional documents such as patents, academic papers, and publicly accessible research reports. Raw data was extracted from various formats, including web pages and different document types. 3 To isolate relevant content from these diverse formats, we applied sophisticated text extraction techniques, followed by advanced filtering strategies to refine and cleanse the dataset. 4.3 Data filtering A comprehensive data processing, filtering, and deduplication pipeline was applied to ensure the dataset’s quality and relevance. This rigorous procedure led to a considerable reduction in the",
    "these diverse formats, we applied sophisticated text extraction techniques, followed by advanced filtering strategies to refine and cleanse the dataset. 4.3 Data filtering A comprehensive data processing, filtering, and deduplication pipeline was applied to ensure the dataset’s quality and relevance. This rigorous procedure led to a considerable reduction in the number of samples, specifically by 83.59%. However, the overall volume of text decreased by a relatively smaller margin of 40.13%. This difference suggests that the filtering process was particularly effective in removing shorter and less informative samples, while retaining more substantial and valuable content. 5 Post-training In machine learning, the quality of the datasets plays a critical role in improving the accuracy, efficiency, and generalizability of the models. However, the collection of fine-grained datasets remains a significant challenge, particularly for non-English languages. To overcome this limitation and develop a high-quality, fine-grained Korean-specific dataset, we implemented a comprehensive data cleaning process on open-source instruction datasets. This process used an internal methodology, LLM as a Judge [18] and was further supported by extensive evaluations by human validaters. In the Supervised Fine-Tuning (SFT) process, we used NEFTune [ 7] to optimize the performance of SFT with noisy embeddings using NEFT-alpha of 8. Preference optimization (PO) plays a vital role in facilitating controllable enhancements in large- language models. However, current PO methods have limitations that make them inadequate for application to our Korean language model. Although Proximal Policy Optimization (PPO) [ 11] has shown promising results in various contexts, its considerable memory demands and the additional training costs associated with the policy model make it suboptimal for certain training scenarios. Likewise, Direct Policy Optimization (DPO) [ 10] not only requires more memory resources than SFT, but also incurs additional costs related to paired data collection. As a result, DPO may be less practical for real-world applications that rely on hard-to-collect language datasets, such as those in Korean. To reduce the costs involved in collecting fine-grained, language-specific pairwise datasets, we utilized Kahneman-Tversky Optimization (KTO) [ 6] as an alternative to DPO and PPO, which presumably maintains comparable performance. KTO uses unpaired preference data with a binary signal to learn whether an output is desirable or undesirable for an input, unlike DPO that learns them in pairs, which makes data collection more feasible. To ensure comparable performances between the aforementioned methods, we conducted evaluations using both LLM-as-a-Judge and human assessment. The results of this comprehensive evaluations confirmed the effectiveness of KTO, leading to its adoption as the human alignment approach. From our experiments, we observed that naïve hyperparameter tuning with KTO resulted in a notable increase in toxicity. To mitigate this issue, we cached the policy model logits across multiple rounds of hyperparameter tuning, a crucial step in maximizing computational efficiency during preference alignment while conducting several iterations. The training parameters for KTO were configured as follows: a batch size of 128, a learning rate of 1e-6, a NEFT alpha of 0, and KTO lambda values set to Desired: 1.375 and Undesired: 1. 6 Model evaluation We use the Korean Multitask Language Understanding (KMMLU) [ 13] and KorMedMCQA",
    "The training parameters for KTO were configured as follows: a batch size of 128, a learning rate of 1e-6, a NEFT alpha of 0, and KTO lambda values set to Desired: 1.375 and Undesired: 1. 6 Model evaluation We use the Korean Multitask Language Understanding (KMMLU) [ 13] and KorMedMCQA [ 9] benchmarks to evaluate Llama-3-Motif’s proficiency in Korean. These benchmarks are widely recognized for assessing the performance of Korean language models, each offering distinct insights into comprehension across various domains. KMMLU assesses the general knowledge and reasoning skills of the model in a broad spectrum of Korean subjects, including the humanities, social sciences and natural sciences. KorMedMCQA evaluates the model’s understanding of specialized knowledge within the medical domain through multiple-choice questions in Korean. 4 Table 2: KMMLU evaluation Model KMMLU-direct score (5-shot) Exaone-3.0+44.5 [2] Solar-10.7B 41.65 [17] Llama-3-70B-Instruct 54.5† Llama-3.1-70B-Instruct 52.1† Qwen1.5-72B 52.6‡ Qwen1.5-110B 57.45‡ Qwen2-72B-Instruct 64.1† GPT-4-0125-preview 59.95† GPT-4o-2024-05-13 64.11‡ Gemini Pro 50.18 [13] Hyperclova X-Large+53.4 [17] Llama-3-Motif-102B+64.74 †: Community report [4], ‡: Measured by the authors. +: Specialized in Korean Using these benchmarks, we can thoroughly assess Llama-3-Motif’s capabilities in both general language comprehension in Korean anddomain-specific expertise , particularly in professional and technical contexts. Moreover, as we are developing a medical consultation service powered by Llama-3-Motif, the inclusion of KorMedMCQA is especially relevant, aligning with our focus on assessing the model’s proficiency in the medical field. 6.1 KMMLU The KMMLU metric is a comprehensive benchmark designed to evaluate the knowledge acquisition capabilities of language models, particularly those trained on Korean data. KMMLU consists of a diverse set of 35,030 questions across 45 distinct subject areas, sourced from various Korean standard- ized assessments, including the Public Service Aptitude Test (PSAT), professional certification exams, and the College Scholastic Ability Test (CSAT). These questions cover a wide range of difficulty levels, from high school to expert, allowing for a nuanced assessment of the model’s performance. We conducted our evaluation using the 5-shot approach whose result is summarized in Table 2. The strong performance of our model on the KMMLU benchmark can be largely attributed to the composition of our training dataset. In addition to conventional sources such as blog posts and news articles, the dataset included a substantial proportion of specialized documents, such as domestic academic papers, research reports, and patents. The diverse and professionally-oriented nature of this training corpus has significantly improved the model’s ability to exhibit comprehensive knowledge across the diverse domains. 6.2 KorMedMCQA KorMedMCQA is a multiple-choice question-answer dataset based on the Korean medical licensing exams, consisting of questions for doctors, nurses, and pharmacists from 2012 to the present. The data was collected by crawling publicly available questions provided by the Korea Health Personnel Licensing Examination Institute. For the Doctor exam, the dataset includes subjects such as Healthcare Laws and Regulations, General Medicine, and Specialized Medicine, covering a wide range of medical topics. This dataset reflects the regulations, standards, and practices of the Korean healthcare system and serves as a valuable benchmark for assessing the understanding and capabilities of medical AI models within the context of Korean healthcare.",
    "Laws and Regulations, General Medicine, and Specialized Medicine, covering a wide range of medical topics. This dataset reflects the regulations, standards, and practices of the Korean healthcare system and serves as a valuable benchmark for assessing the understanding and capabilities of medical AI models within the context of Korean healthcare. We evaluated various models using 5-shot prompting, calculating both subject-specific scores and overall average scores, and compared the results, as presented in Table 3. 5 Table 3: KorMedMCQA evaluation Model Doctor (5-shot) Average (5-shot) Llama-3.1-70B-Instruct‡71.58 79.06 Qwen-1.5-110B-Instruct‡48.42 65.32 GPT-4-base-0125†76.49 83.06 Llama-3-Motif-102B 77.19 83.34 †: Report from [9], ‡: Measured by the authors. 7 Conclusion In this report, we presented Llama-3-Motif, an advanced language model specifically designed to improve Korean language processing. Llama-3-Motif has demonstrated significant improvements in both general linguistic understanding and specialized domain knowledge, outperforming the capabilities of GPT-4’s base model. This work represents a step toward reducing the performance gap for languages with limited resources. It also provides valuable insights that can be leveraged in the development of similar models for other under-resourced languages. References [1]Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 , 2023. [2]Soyoung An, Kyunghoon Bae, Eunbi Choi, Stanley Jungkyu Choi, Yemuk Choi, Seokhee Hong, Yeonjung Hong, Junwon Hwang, Hyojin Jeon, Gerrard Jeongwon Jo, et al. Exaone 3.0 7.8 b instruction tuned language model. arXiv e-prints , pages arXiv–2408, 2024. [3]Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609 , 2023. [4]https://kr.linkedin.com/posts/jdh3577_llama31-gemma2-qwen2-%EB%93%B1-%EC%B5%9C% EA%B7%BC%EC%97%90%EB%8A%94-%ED%95%9C%EA%B5%AD%EC%96%B4%EB%8F%84-%EC%B6%A9%EB% B6%84%ED%9E%88-%EC%9E%98%ED%95%98%EB%8A%94-activity-7222200121843752961-aMwi? utm_source=li_share&utm_content=feedcontent&utm_medium=g_dt_web&utm_campaign= copy . Accessed: 2024-09-28. [5]Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783 , 2024. [6]Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Kto: Model alignment as prospect theoretic optimization. arXiv preprint arXiv:2402.01306 , 2024. [7]Neel Jain, Ping-yeh Chiang, Yuxin Wen, John Kirchenbauer, Hong-Min Chu, Gowthami Somepalli, Brian R Bartoldson, Bhavya Kailkhura, Avi Schwarzschild, Aniruddha Saha, et al. Neftune: Noisy embeddings improve instruction finetuning. arXiv preprint arXiv:2310.05914 , 2023. [8]Dahyun Kim, Chanjun Park, Sanghoon Kim, Wonsung Lee, Wonho Song, Yunsu Kim, Hyeonwoo Kim, Yungi Kim, Hyeonju Lee, Jihoo Kim, et al. Solar 10.7 b: Scaling large language models with simple yet effective depth up-scaling. arXiv preprint arXiv:2312.15166 , 2023. [9]Sunjun Kweon, Byungjin Choi, Minkyu Kim, Rae Woong Park, and Edward Choi. Kormedmcqa: Multi- choice question answering benchmark for korean healthcare professional licensing examinations. arXiv preprint arXiv:2403.01469 , 2024. [10] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. Advances in Neural Information Processing Systems , 36, 2024. [11] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347 , 2017. 6 [12] Sheng Shen, Pete Walsh, Kurt Keutzer, Jesse Dodge, Matthew Peters, and",
    "language model is secretly a reward model. Advances in Neural Information Processing Systems , 36, 2024. [11] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347 , 2017. 6 [12] Sheng Shen, Pete Walsh, Kurt Keutzer, Jesse Dodge, Matthew Peters, and Iz Beltagy. Staged training for transformer language models. In International Conference on Machine Learning , pages 19893–19908. PMLR, 2022. [13] Guijin Son, Hanwool Lee, Sungdong Kim, Seungone Kim, Niklas Muennighoff, Taekyoon Choi, Cheonbok Park, Kang Min Yoo, and Stella Biderman. Kmmlu: Measuring massive multitask language understanding in korean. arXiv preprint arXiv:2402.11548 , 2024. [14] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need.(nips), 2017. arXiv preprint arXiv:1706.03762 , 10:S0140525X16001837, 2017. [15] Chengyue Wu, Yukang Gan, Yixiao Ge, Zeyu Lu, Jiahao Wang, Ye Feng, Ping Luo, and Ying Shan. Llama pro: Progressive llama with block expansion. arXiv preprint arXiv:2401.02415 , 2024. [16] Yiqun Yao, Zheng Zhang, Jing Li, and Yequan Wang. Masked structural growth for 2x faster language model pre-training. arXiv preprint arXiv:2305.02869 , 2023. [17] Kang Min Yoo, Jaegeun Han, Sookyo In, Heewon Jeon, Jisu Jeong, Jaewook Kang, Hyunwook Kim, Kyung-Min Kim, Munhyong Kim, Sungju Kim, et al. Hyperclova x technical report. arXiv preprint arXiv:2404.01954 , 2024. [18] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems , 36:46595–46623, 2023. A Appendix A.1 Contributions All authors sorted alphabetically by last name. Technical and management leadership : Gangwon Jo, Sungmin Lee, Junghwan Lim, Jiyoung Park Core contributors : Dongseok Kim, Jihwan Kim, Junhyeok Lee Contributors : Wai Ting Cheung, Dahye Choi, Kibong Choi, Jaeyeon Huh, Beomgyu Kim, Jangwoong Kim, Taehyun Kim, Haesol Lee, Jeesoo Lee, Dongpin Oh, Changseok Song, Daewon Suh 7"
  ]
}