{
  "filename": "2509.08736v1.pdf",
  "total_chunks": 21,
  "text_length": 71223,
  "chunks": [
    "ChemBOMAS: Accelerated BO in Chemistry with LLM-Enhanced Multi-Agent System Dong Han1†Zhehong AI1†Pengxiang Cai1†Shuzhou Sun1Shanya Lu2 Jianpeng Chen1Ben Gao1Lingli Ge1Weida Wang1Xiangxin Zhou1 Xihui Liu3Mao Su1Wanli Ouyang1Lei Bai1Dongzhan Zhou1 Tao Xu2∗Yuqiang Li1∗Shufei Zhang1∗ 1Shanghai Artificial Intelligence Laboratory, Shanghai, China 2Tongji University, Shanghai, China 3The University of Hong Kong, Hong Kong, China taoxu@tongji.edu.cn liyuqiang@pjlab.org.cn zhangshufei@pjlab.org.cn †These authors contributed equally to this work.∗Corresponding authors Abstract The efficiency of Bayesian optimization (BO) in chemistry is often hindered by sparse experimental data and complex reaction mechanisms. To overcome these limitations, we introduce ChemBOMAS, a new framework named LLM-Enhanced Multi-AgentSystem for acceleratingBOinchemistry. ChemBOMAS’s optimiza- tion process is enhanced by LLMs and synergistically employs two strategies: knowledge-driven coarse-grainedoptimization anddata-driven fine-grained optimization. First, in the knowledge-driven coarse-grained optimization stage, LLMs intelligently decompose the vast search space by reasoning over existing chemical knowledge to identify promising candidate regions. Subsequently, in the data-driven fine-grained optimization stage, LLMs enhance the BO process within these candidate regions by generating pseudo-data points, thereby improving data utilization efficiency and accelerating convergence.Benchmark evaluations further confirm that ChemBOMAS significantly enhances optimization effective- ness and efficiency compared to various BO algorithms. Importantly, the practical utility of ChemBOMAS was validated throughwet-lab experimentsconducted under pharmaceutical industry protocols, targeting conditional optimization for a previously unreportedand challenging chemical reaction. In the wet experiment, ChemBOMAS achieved an optimal objective value of 96%. This was substantially higher than the 15% achieved by domain experts. This real-world success, together with strong performance on benchmark evaluations, highlights ChemBOMAS as a powerful tool to accelerate chemical discovery. 1 Introduction Manual experimentation and traditional control variable methods have long underpinned chemical discovery, yet they remain labor-intensive and time-consuming, slowing the generation of new scientific insights Xie et al. [2023], Tom et al. [2024]. To address these constraints, automated or self-driving laboratories integrate robotic execution with AI algorithms, delivering high throughput, precision, and efficiency Seifrid et al. [2022], Chen et al. [2024], Ai et al. [2024a]. Within these experimental platforms, Bayesian Optimization (BO) algorithms are widely recognized as a crucial decision-making tool for experiment design Guo et al. [2023], Abolhasani and Kumacheva [2023], Chen et al. [2023], Ai et al. [2024b]. BO enables efficient navigation of complex experimental variable spaces and converges toward optimal reaction conditions or material compositions by integrating prior Preprint. Under review.arXiv:2509.08736v1 [cs.LG] 10 Sep 2025 data, constructing probabilistic surrogate models, quantifying uncertainty, and iteratively selecting the most informative subsequent experimentsShields et al. [2021]. Despite BO achieving remarkable success in complex scientific domains, particularly chemistry, it still contends with two major obstacles: (I) the scarcity and high cost of experimental observations during the early optimization stages, and (II) the multitude of reaction parameters that inflate the search into high-dimensional design spaces Shahriari et al. [2015], Wang et al. [2023]. The two obstacles exacerbate the limitations of vanilla BO, such as the \"cold start\" problem and the \"curse of dimensionality\", frequently leading to slow convergence Guo et al. [2023]. Without effective acceleration strategies, the protracted optimization process may yield only marginal improvements, which could cause researchers to abandon the search before discovering the optimal conditions. Several strategies have been",
    "such as the \"cold start\" problem and the \"curse of dimensionality\", frequently leading to slow convergence Guo et al. [2023]. Without effective acceleration strategies, the protracted optimization process may yield only marginal improvements, which could cause researchers to abandon the search before discovering the optimal conditions. Several strategies have been proposed to accelerate BO, including search space partitioning Wang et al. [2020], specialized encoding embeddings Tripp et al. [2020], Moriconi et al. [2020], Nayebi et al. [2019], pseudo-data generation Yin et al. [2023], and transfer across similar tasks Swersky et al. [2013]. However, when these acceleration strategies are applied to the intricate chemical reactions, two critical shortcomings emerge. First, most approaches employ a single acceleration technique, which may prove insufficient for the chemical optimization problems with multiple demands, such as exploration of diverse reaction parameters combinations while enriching sparse early-stage data. Second, current acceleration methods are predominantly data-driven. Because reaction pathways differ widely in their underlying kinetics and thermodynamics, a purely statistical BO framework frequently expends resources in chemically implausible regions of the search space, missing opportunities to leverage mechanistic insight that could guide the search more efficiently. To overcome these limitations, we proposeChemBOMAS, an LLM-EnhancedMulti-AgentSystem specifically designed for acceleratedBayesianOptimization in chemistry. ChemBOMAS introduces a novel approach where Large Language Models (LLMs) enhance a synergistic optimization process that combinesknowledge-driven coarse-grainedoptimization withdata-driven fine-grainedopti- mization. The system operates using a multi-agent framework where agents collaborate dynamically. In the coarse-grained optimization stage, an LLM-powered agent reasons over existing chemical knowledge (e.g., literature, databases) to intelligently decompose the vast search space and identify promising candidate regions, effectively pruning chemically irrelevant areas. Subsequently, in the fine-grained optimization stage within these candidate regions, another LLM-enhanced workflow generates informative pseudo-data points. These pseudo-points augment the data available to the BO agent, significantly improving data utilization efficiency and accelerating convergence towards the optimum. The effectiveness of ChemBOMAS was rigorously evaluated. We conducted extensive experiments on four benchmark chemical reaction datasets (Suzuki coupling, Buchwald-Hartwig coupling, Tandem arylation, and Enantio selectivity), demonstrating consistent improvements in optimization efficiency and results compared to various baseline methods. Furthermore, ablation studies confirmed the contribution of each component within our multi-agent framework. Crucially, the practical utility and real-world applicability of ChemBOMAS were validated throughwet-labexperiments conducted under pharmaceutical industry protocols. These experiments targeted conditional optimization for apreviously unreportedand challenging chemical reaction, where ChemBOMAS achieved outstanding results, significantly surpassing the performance achieved by domain experts. Our main contributions are summarized as follows: 1.We propose ChemBOMAS, a novel LLM-enhanced multi-agent framework that synergistically integrates knowledge-driven and data-driven strategies for efficient chemical optimization, enabling dynamic collaboration between specialized agents. 2.We introduce an LLM-powered knowledge integration approach for coarse-grained optimization, where LLMs analyze domain knowledge to guide search space decomposition and identify high- potential regions. 3.We propose a fine-grained optimization stage in which an LLM-enhanced workflow synthesizes chemically plausible pseudo-experiments within the high-potential regions, augmenting the sparse experimental observations and accelerating convergence toward the global optimum. 4.We demonstrate the effectiveness and efficiency of ChemBOMAS through comprehensive exper- iments on diverse benchmark datasets and, critically, through successfulwet-lab validationon a novel and challenging chemical",
    "which an LLM-enhanced workflow synthesizes chemically plausible pseudo-experiments within the high-potential regions, augmenting the sparse experimental observations and accelerating convergence toward the global optimum. 4.We demonstrate the effectiveness and efficiency of ChemBOMAS through comprehensive exper- iments on diverse benchmark datasets and, critically, through successfulwet-lab validationon a novel and challenging chemical reaction, outperforming domain experts. 2 2 Related Work Large Language Models (LLMs) offer synergistic potential with Bayesian Optimization (BO) to address traditional BO limitations (e.g., sample inefficiency, cold starts) by providing prior knowl- edge Souza et al. [2021], enhancing surrogate models Liu et al. [2024], Nguyen et al. [2024], Ramos et al. [2023], automating acquisition function design Austin et al. [2024], and enabling optimiza- tion in novel problem representations. Prior work has explored LLM-driven BO improvements in warm-starting, surrogate modeling, candidate generation, acquisition function design, and search space understanding. However, directly replacing core BO modules with LLMs introduces significant challenges. LLM \"hallucinations\" can mislead optimization, compromising reliability. Furthermore, the direct suit- ability of LLMs as surrogates or acquisition functions is limited by concerns regarding uncertainty quantification, theoretical guarantees, computational cost, efficiency in low-data regimes, adapt- ability to specific numerical tasks, and interpretability. On another front, some techniques such as LA-MCTS Wang et al. [2020] was proposed, which employ tree structures to decompose the search space Wang et al. [2023, 2024a, 2019]. Some works propose hierarchical Bayesian opti- mization Moriconi et al. [2020], Reker et al. [2020]. These approaches offer valuable strategies for managing and navigating complex optimization landscapes. Unlike previous works, we focus on robustly integrating LLM knowledge to enhance BO, leveraging their strengths as auxiliary tools while mitigating weaknesses such as hallucinations, to achieve this synergy over substitution. 3 Background The discovery and optimization of chemical reactions are pivotal for progress in diverse fields, including pharmaceuticals, materials science, and sustainable energy solutions. Fundamentally, this endeavor can be framed as a classicblack-box optimization problem. The objective is to identify an optimal set of reaction parameters X∗from a multi-dimensional search space Xthat maximizes a desiredreaction performance, which is quantified by an objective function h(X) . X=X 1× X 2× ··· × X ndefines the multi-dimensional search space and {Xi}n i=1represent the spaces of experimental conditions such as temperature, pressure, catalyst type, solvent, reactant concentrations, and reaction time. Sometimes it also involves theselection or modification of reactants(e.g., substituting a functional group). Desired reaction performance is typically quantified by metrics such as yield (the amount of desired product obtained from a chemical reaction), selectivity (the ratio of desired product to undesired byproducts), or a composite objective function combining multiple criteria. Formally, this optimization problem is expressed as finding X∗= arg maxh(X) , whereX∈ X represents the reaction parameters and the objective function h:X →R maps these parameters to the experimental performance. 4 ChemoBOMAS To address the identified challenges, we present ChemBOMAS, a pioneering two-stage multi-agent optimization framework designed to enhance the efficiency of reaction condition optimization. As depicted in Figure 1, the framework is systematically structured into two distinct stages: coarse optimization through knowledge-driven search space decomposition and data-driven fine-grained optimization. By seamlessly integrating these",
    "the identified challenges, we present ChemBOMAS, a pioneering two-stage multi-agent optimization framework designed to enhance the efficiency of reaction condition optimization. As depicted in Figure 1, the framework is systematically structured into two distinct stages: coarse optimization through knowledge-driven search space decomposition and data-driven fine-grained optimization. By seamlessly integrating these phases, ChemBOMAS offers a comprehensive and effective approach to navigating the complexities inherent in chemical reaction optimization. The following sections provide a comprehensive exposition of the two stages, detailing their methodologies and underlying principles. 4.1 Knowledge-driven search space decomposition This stage leverages the literature comprehension and reasoning capabilities of large language models to systematically analyze relevant literature and physicochemical data, assessing the significance of various reaction parameters on the outcome and identifying key physicochemical properties that influence reaction performance. Based on the importance of reaction parameters and the corresponding physicochemical characteristics, a hierarchical tree structure is employed to partition 3 Figure 1: ChemBOMAS: A two-stage, coarse-to-fine optimization framework for identifying optimal reaction conditions.Stage 1 (Coarse Optimization)implements a knowledge-driven decomposition of the search space, utilizing prior chemical insights extracted from literature through large language models, applying the UCB algorithm to identify promising candidate subspaces.Stage 2 (Fine Optimization)then conducts data-driven Bayesian Optimization within these selected subspaces, accelerated by pseudo-points generated from fine-tuned LLMs, to determine optimal reaction condi- tions. the condition space, facilitating a coarse-grained search for optimal reaction conditions and enhancing the efficiency of experimental exploration. Specifically, the large language model (LLM) first conducts a systematic search of chemical literature relevant to the target reaction, extracting and synthesizing key information to rank the potential significance of various reaction parameters, including reactants, catalysts, ligands, solvents, and bases. Furthermore, the LLM identifies critical physicochemical properties, such as electronic and steric effects, that influence the experimental outcomes. Subsequently, it retrieves specific physicochemical properties for all conditions within the reaction space from a physicochemical property database and performs clustering based on these properties. A hierarchical optimization tree is then constructed based on the importance ranking of reaction parameters and the clustering results based on their physicochemical property. The root node of this tree represents the complete parameter space (All possible combinations of reaction parameters) for the reaction to be optimized and the other nodes of the tree are the subsets of the whole parameter space. The hierarchical structure of the tree is built layer by layer according to the importance ranking of the parameters (the more important condition variable corresponds to the layer with a higher level). At each layer, the parent node from the previous layer is split into several child nodes based on the physicochemical property of the corresponding condition variable of this layer. This splitting process is performed recursively until all considered parameters are incorporated into the tree structure, ultimately forming a decision tree that guides the optimization direction. To facilitate a deeper understanding of the entire process, a detailed illustrative example is provided in the supplementary materials for further clarification. Once the optimization tree is constructed, it can be systematically traversed layer by layer to identify and sample high-value subspaces of reaction conditions.",
    "that guides the optimization direction. To facilitate a deeper understanding of the entire process, a detailed illustrative example is provided in the supplementary materials for further clarification. Once the optimization tree is constructed, it can be systematically traversed layer by layer to identify and sample high-value subspaces of reaction conditions. It facilitates the reduction of the optimization search space, thereby enhancing efficiency in the refinement process. Specifically, we employ the Upper Confidence Bound (UCB) algorithm to search this tree. Starting from the root node, we iteratively select the subsequent child nodes at each hierarchical level based on the UCB strategy, conducting in-depth exploration until reaching the final leaf node. For each child node i of the current parent node, we calculate its UCB value: UCB i=¯Ri+Cp×logq log(N parent ) ni, where ¯Ri 4 is the average experimental return (e.g., average yield or selectivity) of child node i, representing the ‘exploitation’ term. It is calculated by dividing the total return Qiobtained from this node by its number of visits ni.Nparent is the total number of times the current parent node has been visited. ni is the number of times child node i has been visited. Cpis an exploration constant used to balance the relative importance of the exploitation and exploration terms. At each hierarchical level, the child node exhibiting the highest UCB value is selected for the subsequent exploration step. This formula ensures that while the algorithm tends to select child nodes that have performed well historically, it also gives an opportunity for exploration to less-visited child nodes, thus avoiding premature convergence to a suboptimal solution. As the selection process recursively traverses a path within the tree until a leaf node is reached, the final subset of chemical reaction conditions is established. Subsequently, Bayesian optimization is adopted to sample reaction conditions and corresponding experiments (or, in simulations, query results in datasets) are conducted to evaluate the actual impact of these reaction conditions, obtaining key performance indicators such as reaction yield and selectivity. This evaluation value, denoted asRexp, serves as a reward signal. This signal is then backpropagated to update the statistics of all nodes along the path from the currently evaluated leaf node to the root node. The specific update rules are as follows: 1.The visit countn ifor each nodeion the path is updated byn i←n i+ 1. 2.The cumulative returnQ ifor each nodeion the path is increased byR exp:Qi←Q i+Rexp. 3.Through these updates, the average return of the node ¯Ri=Q i/niis also adjusted accordingly, thereby influencing the UCB value calculation and node selection in subsequent iterations. The entire process consists of three key steps: (1) trajectory sampling on the tree to obtain the final subset of reaction conditions, (2) Bayesian optimization within this subset to sample reaction conditions and conduct experiments to acquire the reward signal, and (3) backpropagation to update the Upper Confidence Bound (UCB) values of the nodes. Through continuous iterations of these steps, the value estimation of nodes or chemical condition subsets becomes increasingly precise, thereby enhancing the guidance for optimal condition selection. 4.2 Data-driven Bayesian Optimization The knowledge-guided",
    "acquire the reward signal, and (3) backpropagation to update the Upper Confidence Bound (UCB) values of the nodes. Through continuous iterations of these steps, the value estimation of nodes or chemical condition subsets becomes increasingly precise, thereby enhancing the guidance for optimal condition selection. 4.2 Data-driven Bayesian Optimization The knowledge-guided optimization tree constructed can effectively decompose the search space and capture potential correlations under specific conditions (e.g., certain ligands are not used with strong bases). However, relying solely on knowledge makes it difficult to accurately predict reaction performance, and obtaining specific reaction variable recommendations still requires intrinsic infor- mation of experimental data. Given the high cost and lengthy cycles of real experiments, as well as the cold-start problem of traditional Bayesian Optimization (BO), the Large Language Model (LLM) is used to learn from historical experimental data and mine underlying complex patterns to provide a higher quality prior for BO’s surrogate model, thereby accelerating the optimization process. The LLM exhibit good compatibility in handling data with different representations, can be pre-trained using massive amounts of unlabeled data and non-domain-specific data, and support diverse task types. Particularly noteworthy is their potential for extrapolation, which offers possibilities for exploring unknown reaction spaces. Therefore, we apply LLMs to the task of predicting the reaction performance. To achieve reaction performance prediction, we first construct an LLM-based regression model. The model directly takes SMILES (Simplified Molecular Input Line Entry Specification) strings as input. SMILES is a linear notation that encodes the topological structures of molecules, including atoms, chemical bonds, rings, and branches, into one-dimensional ASCII text, facilitating efficient computer processing, storage, and widespread application in structure retrieval and analysis within chemoinformatics and molecular modeling research. We selected LLaMa 3.1 Grattafiori et al. [2024] as the base architecture. We employ the Pistachio dataset for pre-training. The Pistachio dataset (a proprietary dataset acquired from a commercial vendor specializing in chemical data) primarily integrates various patent data (e.g., from USPTO). Considering that the Pistachio dataset may suffer from issues such as missing entries, difficulty in tracing sources, and significant noise in key performance data like reaction yields, we do not directly incorporate reaction performance information during the pre-training phase. Instead, we 5 design a conditional prediction task: given the reactants and products (i.e., the reaction equation), the model’s objective is to predict the corresponding reaction conditions. This process utilizes a Causal Language Modeling (CLM) loss, which involves predicting the next token in the sequence of reaction conditions: Lpretrain =Ex∼D chem\u0002Pt= 1Tlogp(x t|x<t)\u0003 , where tdenotes the token index, xtrepresents the t-th token in the condition sequence, and Tis the sequence length. To enhance chemical awareness, we augment input sequences with functional group annotations using the RDKit toolkit Landrum et al. [2019], enabling explicit modeling of reaction-critical substructures. Given that our research primarily focuses on predicting continuous variables like yield and selectivity, we enhance a pre-trained LLM by integrating a regression head. Through prompt engineering, we guide the LLM, which takes an input X(e.g., prompted reaction conditions), to generate token sequences pertinent to reaction performance. The hidden state of the last token",
    "primarily focuses on predicting continuous variables like yield and selectivity, we enhance a pre-trained LLM by integrating a regression head. Through prompt engineering, we guide the LLM, which takes an input X(e.g., prompted reaction conditions), to generate token sequences pertinent to reaction performance. The hidden state of the last token from this LLM output, which we can denote as LLM(X)Tafter processing the input, is then fed into an additionally incorporated projection layer. This layer, for which we adopted a Multilayer Perceptron (MLP), f, outputs the specific reaction performance prediction, aiming for the ground truth valuey. During the subsequent fine-tuning stage, we optimize the entire architecture using a small amount of labeled data, Dlabeled , consisting of (X, y) pairs. This process employs two distinct parameter update strategies: The pre-trained LLM componentis fine-tuned using Low-Rank Adaptation (LoRA) technology Hu et al. [2022]. This introduces a set of low-rank adaptable parameters, ϕLoRA, to the LLM’s original weights. This approach effectively alleviates catastrophic forgetting Li et al. [2025], ensuring the LLM retains pre-trained knowledge while adapting its representations, now denoted LLM ϕLoRA(X), to the new task. The MLP, now more precisely fθMLP, is trained with its full set of parameters θMLPto accurately map the final token representation LLMT ϕLoRA(X)from the adapted LLM to the target outputy. The overall fine-tuning objective is to minimize an L2 loss function with an L2 regularization term on the MLP’s parametersθ MLPcontrolled by a hyperparameterλ. The complete fine-tuning loss is: Lfinetune =1 |Dlabeled|X (X,y)∈D labeled fθMLP(LLMT ϕLoRA(X))−y 2+λ|θ MLP|2(1) This formulation jointly optimizes the LoRA parameters ϕLoRA of the LLM and the full parameters θMLPof the MLP to enhance predictive accuracy on target variables like yield and selectivity. The trained LLM prediction model can be used to generate pseudo-points, providing data prior for BO’s surrogate model. By performing inference over the entire search space (or the decomposed subspaces), we can obtain pseudo-labels (i.e., predicted reaction performance) for all combinations of reaction parameters. These pseudo-points are then used to initialize the BO surrogate model. Although this step involves a considerable amount of inference computation, it is still negligible compared to the high costs and time investment of chemical experiments. Notably, performing BO in decomposed subspaces also helps reduce overall computational complexity. As BO iterates and real experimental data accumulates, we will gradually remove pseudo-points to increase the weight of real data in model updates, thereby mitigating the potential impact of pseudo-data noise. The removal of pseudo-points follows two core principles: Data Similarity (Local Removal):We utilize the embedding of the LLM’s last token to calculate the cosine similarity between data points. If the similarity between a pseudo-point and a newly acquired real feedback point (i.e., actual experimental result) exceeds a predefined threshold, these pseudo-points are removed from the pseudo-dataset. Observed Performance (Global Removal):Considering that as iteration progresses, the observed optimal performance tends to improve, the model should be encouraged to explore more broadly rather than over-relying on existing predictions. Therefore, based on the predicted performance values of the pseudo-points, we randomly discard a certain proportion of pseudo-data, starting from those with high",
    "Removal):Considering that as iteration progresses, the observed optimal performance tends to improve, the model should be encouraged to explore more broadly rather than over-relying on existing predictions. Therefore, based on the predicted performance values of the pseudo-points, we randomly discard a certain proportion of pseudo-data, starting from those with high predicted performance downwards. These generated pseudo-points can also provide further support for the construction of the knowledge- guided optimization tree. By adjusting the temperature parameter of the LLM during prediction generation, we can guide the model to produce different rankings and classifications of reaction condition combinations’ performance, thereby generating multiple candidate optimization tree struc- 6 tures. Using a large number of pseudo-points, we can quantitatively evaluate the rationality of these candidate trees. Ideally, reaction varibales combinations within the same tree node should exhibit similar performance characteristics. Therefore, we calculate the variance of the predicted performance of all pseudo-points within each node for every candidate tree. The tree structure that minimizes the sum (or average) of intra-node variances is selected as the optimal knowledge-guided optimization tree. 5 Experiment To rigorously assess the effectiveness and delineate the superiority of our approach, we conducted a comprehensive validation process. This involved extensive computational (dry lab) simulations utilizing multiple benchmarks, alongside a previously unreported wet laboratory experiment. The computational simulations proceeded in several stages. First, we conduct a direct comparison of our method against a traditional BO algorithm on a long-iteration chemical optimization task. Then, to assess our method’s robustness in mitigating the ‘cold start’ phenomenon inherent in BO, it was benchmarked against three representative BO techniques across four distinct benchmarks, with a specific focus on early-stage performance. Finally, an ablation study was conducted to examine the contributions of individual components of our proposed system to the overall optimization performance. For wet laboratory validation, our approach was applied to a complex chemical reaction optimization task designed by a pharmaceutical company. Its performance was then benchmarked against the results obtained by experienced human chemical experts. 5.1 Benchmark evaluations 5.1.1 Experimental Setup and Benchmarks This subsection details the setup for the computational evaluations, including ChemBOMAS con- figurations and benchmark dataset characteristics, crucial for understanding the subsequent compar- ative analyses and ablation study. The configurations for the baseline algorithms (Traditional BO, LLAMBO Liu et al. [2024], LA-MCTS Wang et al. [2020]) are provided in Supplementary Material For the ChemBOMAS framework, general LLM-driven tasks, such as knowledge extraction and reasoning, utilized the GPT-4O API and GPT-O1 API. A critical measure to prevent data leakage involvedmanually curating the literature databaseto ensure no publications directly corresponding to the benchmark datasets were included. Furthermore, prompts were meticulously engineered to compel the LLM to derive insights and predictionsbased solely on the provided literature corpus, thereby minimizing reliance on its pre-trained knowledge about the specific benchmark reactions. The exploration constant ( Cp) for the UCB algorithm in the coarse-grained optimization stage was set to 10, consistent with the LA-MCTS configuration. All other Bayesian Optimization parameters for the fine-grained stage were identical to those of the baseline method. The data module was constructed by fine-tuning an 8B",
    "reactions. The exploration constant ( Cp) for the UCB algorithm in the coarse-grained optimization stage was set to 10, consistent with the LA-MCTS configuration. All other Bayesian Optimization parameters for the fine-grained stage were identical to those of the baseline method. The data module was constructed by fine-tuning an 8B parameter LLaMa 3.1 model. Our proposed method was evaluated using high-throughput screening reaction datasets curated from the literature. While these datasets are of moderate size, containing up to approximately 6,000 reactions, they possess a critical attribute for rigorous evaluation:completeness within the defined variable space. Specifically, for every combi- nation of reaction parameters proposed by the optimization framework, a corresponding experimental result is available. This unique completeness enables a deterministic and quantitative assessment of our method’s recommendations. Furthermore, the integrity and consistency of these datasets are assured as all data were generated on automated experimental platforms, thereby minimizing human- induced error and experimental bias. The reliability of these benchmarks is further underscored by the adoption of several of these datasets inmultiple prior studieson reaction optimization Shields et al. [2021], Wang et al. [2024b]. 5.1.2 Benchmark Results Long iterationAs shown in figure 2A, our method achieved the best objective value that exceeded the traditional BO baseline by 30% in the initial round and by 17% over the entire optimization campaign. Notably, ChemBOMAS reached convergence by the 4th experimental round, representing over a 50% improvement in optimization efficiency compared to the traditional BO, which had not 7 012340102030405060708090100 ChemBOMAS Remove Data ModuleRemove Knowledge ModuleRemove both Ablation StudyC ���������������Figure 2: Experiment results of long iteration optimization and ablation study.(A)Single run comparison of ‘Best Value Found (%)’ per ‘Iteration Round’ for ‘Baseline’ and ‘ChemBOMAS’, showing observations and maximum values.(B)Mean maximum value and standard deviation from 10 repeated runs for ‘Baseline’ and ‘ChemBOMAS’.(C)Ablation study showing ‘Best Value Found (%)’ for ‘ChemBOMAS’ and variants with removed modules. yet converged by the 9th round. Moreover, Figure 2Billustrates that ChemBOMAS consistently outperforms the baseline, as confirmed by ten independent repetitions of the optimization campaign. Short iterationAs shown in Figure S1 in supplementary material, our benchmark evaluations revealed three key findings. First, ChemBOMAS consistently outperformed all baselines in initial performance across the four chemical reaction optimization tasks. Second, it pinpointed the optimal objective value for each task within just four iterations. Third, in the Suzuki coupling task, LLAMBO reached an optimal target value of 96.3% in the first iteration, which was slightly higher than the initial ChemBOMAS value of 92.56%, possibly due to the abundant relevant literature that guides LLAMBO in selecting high-value candidates at the early stage. However, at the end of the optimization campaign, ChemBOMAS matched LLAMBO’s optimal objective value of 96.3%. Ablation studyThe ablation study results, presented in Figure 2C, demonstrate the superior optimization performance of the complete ChemBOMAS framework. The complete ChemBOMAS outperformed three distinct ablated versions: (i) without the Knowledge Module, (ii) without the Data Module, and (iii) without either module. A key observation is that the absence of the Data Module led to better initialization performance than the absence of the Knowledge Module. This suggests a",
    "framework. The complete ChemBOMAS outperformed three distinct ablated versions: (i) without the Knowledge Module, (ii) without the Data Module, and (iii) without either module. A key observation is that the absence of the Data Module led to better initialization performance than the absence of the Knowledge Module. This suggests a more significant contribution of the Knowledge Module to alleviating the cold-start problem in BO. Nevertheless, the standard BO, devoid of the two specialized modules, exhibited the lowest optimization performance among all tested configurations. 5.2 Wet Experiments To further validate the efficacy and applicability of our proposed method, an algorithm-drivenwet lab- oratory experimentwas conducted. Guided by ChemBOMAS, this study aimed to maximize product yield in a challenging chemical reaction optimization—the palladium-catalyzed cross-coupling of boronic esters with aryl chlorides. This demanding optimization task, originating from a pharmaceu- tical enterprise, was governed by four stringent practical constraints:(1)apreviously-unreported chemical reaction, resulting in the complete absence of reference data;(2)a six-dimensional process parameter space, reportedlyexceeding seventy timesthe scale of those in comparable published studies , thus posing a considerable exploration challenge;(3)a cost-saving imperative requiringa tenfold reductionin catalyst loading relative to conventional levels, substantially hampering product formation; and(4)a restriction on approximately60 experimental runsto curtail labor intensity. Detailed experiment settings could be found in the Supplementary Material. 5.2.1 Experiment Results As shown in Figure 3, during the wet experiment task, ChemBOMAS successfully identified the optimal reaction condition with a yield of 96%, markedly outperforming the 15% yield achieved by a chemist employing the traditional control variable method. Additionally, three noteworthy phenomena emerged. First, in the initial round, ChemBOMAS had attained a maximum product yield of 90%, surpassing the target threshold of 75%. Second, the optimal reaction condition yielding 96% was discovered in the early stage of the optimization process, specifically in the second iteration. Third, as the optimization progressed, ChemBOMAS increasingly recommended reaction conditions 8 0 1 2 3 4020406080100 High-Value Observation Low-Value Observation Maximum Value via ChemBOMAS Maximum Value via Experts Target Threshold Iteration RoundBest Value Found (%) Human Expert����������������Figure 3:Wet laboratory experiment result. Comparison of ‘Best Value Found (%)’ over ‘Iteration Rounds’, showing individual high and low-value observations. Lines indicate maximum values achieved via ChemBOMAS, human experts, and a target threshold. with yields exceeding the 75% target threshold, indicating a continuous refinement of the surrogate model. The number of high-yielding conditions ( ≥75%) identified in rounds one through five was one, two, three, five, and five, respectively. The strong initialization performance, rapid convergence, and progressive model improvement collectively demonstrate the effectiveness and efficiency of ChemBOMAS in accelerating chemical reaction optimization. Additionally, to validate the robustness of ChemBOMAS’s initialization performance, the initial-round sampling was repeated ten times with the fixed experimental configurations, as detailed in Supplementary Materials. In the ten repeated initialization tests using ChemBOMAS, each run consistently identified at least two reaction conditions with yields exceeding 60%. Moreover, reaction conditions achieving yields above 80% appeared in 70% of the validation tests, totaling 11 such high-yield conditions across all trials. These results demonstrate that ChemBOMAS reliably mitigates the “cold-start” problem inherent to BO optimization. 6 Limitations While ChemBOMAS demonstrates",
    "at least two reaction conditions with yields exceeding 60%. Moreover, reaction conditions achieving yields above 80% appeared in 70% of the validation tests, totaling 11 such high-yield conditions across all trials. These results demonstrate that ChemBOMAS reliably mitigates the “cold-start” problem inherent to BO optimization. 6 Limitations While ChemBOMAS demonstrates significant advancements in accelerating Bayesian Optimization for chemical reactions, several limitations warrant discussion and offer avenues for future research: Dependence on LLM Performance and Knowledge Base: The efficacy of the knowledge-driven coarse-grained optimization heavily relies on the LLM’s ability to accurately interpret chemical literature, rank variable importance, and identify key physicochemical properties. Errors, biases in the LLM’s reasoning, or the quality of available chemical literature and databases for a specific reaction class could lead to suboptimal initial search space decomposition, potentially hindering the overall optimization efficiency. Scalability to Extremely High-Dimensional Spaces: The current wet-lab validation involved a six-dimensional parameter space. While this is significant, some chemical optimization problems, particularly in materials discovery or complex biological systems, can involve even higher dimension- ality. The effectiveness of the UCB-guided tree search and the computational cost of LLM inference for pseudo-data generation across all subspaces might face challenges in such scenarios, although the paper notes that LLM inference is negligible compared to wet-lab experiments. 7 Conclusion ChemBOMAS presents an LLM-enhanced multi-agent framework designed to accelerate Bayesian Optimization in the context of chemical reactions. Through a synergistic combination of knowledge- driven coarse-grained search space decomposition and data-driven fine-grained optimization, this approach seeks to mitigate common challenges like data scarcity and complex reaction mechanisms. Results from benchmark evaluations, along with encouraging outcomes from wet-lab validation on a demanding, previously unreported industrial reaction—where ChemBOMAS showed improved performance compared to domain expert methods—suggest its potential for practical application. 9 ChemBOMAS offers a promising direction for facilitating chemical discovery and enhancing the optimization of complex chemical processes. References Yunchao Xie, Kianoosh Sattari, Chi Zhang, and Jian Lin. Toward autonomous laboratories: Conver- gence of artificial intelligence and experimental automation.Progress in Materials Science, 132: 101043, 2023. Gary Tom, Stefan P Schmid, Sterling G Baird, Yang Cao, Kourosh Darvish, Han Hao, Stanley Lo, Sergio Pablo-García, Ella M Rajaonson, Marta Skreta, et al. Self-driving laboratories for chemistry and materials science.Chemical Reviews, 124(16):9633–9732, 2024. Martin Seifrid, Robert Pollice, Andres Aguilar-Granda, Zamyla Morgan Chan, Kazuhiro Hotta, Cher Tian Ser, Jenya Vestfrid, Tony C Wu, and Alan Aspuru-Guzik. Autonomous chemical experiments: Challenges and perspectives on establishing a self-driving lab.Accounts of Chemical Research, 55(17):2454–2466, 2022. Yangguan Chen, Longhan Zhang, Zhehong Ai, Yifan Long, Ji Qi, Pengxiao Bao, and Jing Jiang. Robot-assisted optimized array design for accurate multi-component gas quantification.Chemical Engineering Journal, 496:154225, 2024. Zhehong Ai, Longhan Zhang, Yangguan Chen, Yu Meng, Yifan Long, Julin Xiao, Yao Yang, Wei Guo, Yueming Wang, and Jing Jiang. Customizable colorimetric sensor array via a high-throughput robot for mitigation of humidity interference in gas sensing.ACS sensors, 9(8):4143–4153, 2024a. Jeff Guo, Bojana Rankovi ´c, and Philippe Schwaller. Bayesian optimization for chemical reactions. Chimia, 77(1/2):31–38, 2023. Milad Abolhasani and Eugenia Kumacheva. The rise of self-driving labs in chemical and materials sciences.Nature Synthesis, 2(6):483–492, 2023. Yangguan",
    "via a high-throughput robot for mitigation of humidity interference in gas sensing.ACS sensors, 9(8):4143–4153, 2024a. Jeff Guo, Bojana Rankovi ´c, and Philippe Schwaller. Bayesian optimization for chemical reactions. Chimia, 77(1/2):31–38, 2023. Milad Abolhasani and Eugenia Kumacheva. The rise of self-driving labs in chemical and materials sciences.Nature Synthesis, 2(6):483–492, 2023. Yangguan Chen, Longhan Zhang, Zhehong Ai, Yifan Long, Temesgen Muruts Weldengus, Xubin Zheng, Di Wang, Haowen Wang, Yiteng Zhai, Yuqing Huang, et al. Robot-accelerated development of a colorimetric co2 sensing array with wide ranges and high sensitivity via multi-target bayesian optimizations.Sensors and Actuators B: Chemical, 390:133942, 2023. Zhehong Ai, Longhan Zhang, Yangguan Chen, Yifan Long, Boyuan Li, Qingyu Dong, Yueming Wang, and Jing Jiang. On-demand optimization of colorimetric gas sensors using a knowledge-aware algorithm-driven robotic experimental platform.ACS sensors, 9(2):745–752, 2024b. Benjamin J Shields, Jason Stevens, Jun Li, Marvin Parasram, Farhan Damani, Jesus I Martinez Alvarado, Jacob M Janey, Ryan P Adams, and Abigail G Doyle. Bayesian reaction optimization as a tool for chemical synthesis.Nature, 590(7844):89–96, 2021. Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P Adams, and Nando De Freitas. Taking the human out of the loop: A review of bayesian optimization.Proceedings of the IEEE, 104(1): 148–175, 2015. Xilu Wang, Yaochu Jin, Sebastian Schmitt, and Markus Olhofer. Recent advances in bayesian optimization.ACM Computing Surveys, 55(13s):1–36, 2023. Linnan Wang, Rodrigo Fonseca, and Yuandong Tian. Learning search space partition for black-box optimization using monte carlo tree search.Advances in Neural Information Processing Systems, 33:19511–19522, 2020. Austin Tripp, Erik Daxberger, and José Miguel Hernández-Lobato. Sample-efficient optimization in the latent space of deep generative models via weighted retraining.Advances in Neural Information Processing Systems, 33:11259–11272, 2020. Riccardo Moriconi, Marc Peter Deisenroth, and KS Sesh Kumar. High-dimensional bayesian optimization using low-dimensional feature spaces.Machine Learning, 109:1925–1943, 2020. 10 Amin Nayebi, Alexander Munteanu, and Matthias Poloczek. A framework for bayesian optimization in embedded subspaces. InInternational Conference on Machine Learning, pages 4752–4761. PMLR, 2019. Yuxuan Yin, Yu Wang, and Peng Li. High-dimensional bayesian optimization via semi-supervised learning with optimized unlabeled data sampling.arXiv preprint arXiv:2305.02614, 2023. Kevin Swersky, Jasper Snoek, and Ryan P Adams. Multi-task bayesian optimization.Advances in neural information processing systems, 26, 2013. Artur L. F. Souza, Luigi Nardi, Leonardo B. Oliveira, Kunle Olukotun, Marius Lindauer, and Frank Hutter. Bayesian optimization with a prior for the optimum. In Nuria Oliver, Fernando Pérez- Cruz, Stefan Kramer, Jesse Read, and José Antonio Lozano, editors,Machine Learning and Knowledge Discovery in Databases. Research Track - European Conference, ECML PKDD 2021, Bilbao, Spain, September 13-17, 2021, Proceedings, Part III, volume 12977 ofLecture Notes in Computer Science, pages 265–296. Springer, 2021. doi: 10.1007/978-3-030-86523-8\\_17. URL https://doi.org/10.1007/978-3-030-86523-8_17. Tennison Liu, Nicolás Astorga, Nabeel Seedat, and Mihaela van der Schaar. Large language models to enhance bayesian optimization.arXiv preprint arXiv:2402.03921, 2024. Tung Nguyen, Qiuyi Zhang, Bangding Yang, Chansoo Lee, Jörg Bornschein, Yingjie Miao, Sagi Perel, Yutian Chen, and Xingyou Song. Predicting from strings: Language model embeddings for bayesian optimization.CoRR, abs/2410.10190, 2024. doi: 10.48550/ARXIV .2410.10190. URL https://doi.org/10.48550/arXiv.2410.10190. Mayk Caldas Ramos, Shane S Michtavy, Marc D Porosoff, and Andrew D White. Bayesian optimiza- tion of catalysts with in-context learning.arXiv preprint arXiv:2304.05341, 2023. David Eric Austin, Anton Korikov,",
    "Yutian Chen, and Xingyou Song. Predicting from strings: Language model embeddings for bayesian optimization.CoRR, abs/2410.10190, 2024. doi: 10.48550/ARXIV .2410.10190. URL https://doi.org/10.48550/arXiv.2410.10190. Mayk Caldas Ramos, Shane S Michtavy, Marc D Porosoff, and Andrew D White. Bayesian optimiza- tion of catalysts with in-context learning.arXiv preprint arXiv:2304.05341, 2023. David Eric Austin, Anton Korikov, Armin Toroghi, and Scott Sanner. Bayesian optimization with llm-based acquisition functions for natural language preference elicitation. In Tommaso Di Noia, Pasquale Lops, Thorsten Joachims, Katrien Verbert, Pablo Castells, Zhenhua Dong, and Ben London, editors,Proceedings of the 18th ACM Conference on Recommender Systems, RecSys 2024, Bari, Italy, October 14-18, 2024, pages 74–83. ACM, 2024. doi: 10.1145/3640457.3688142. URLhttps://doi.org/10.1145/3640457.3688142. Shukuan Wang, Ke Xue, Lei Song, Xiaobin Huang, and Chao Qian. Monte carlo tree search based space transfer for black-box optimization.arXiv preprint arXiv:2412.07186, 2024a. Linnan Wang, Saining Xie, Teng Li, Rodrigo Fonseca, and Yuandong Tian. Sample-efficient neural architecture search by learning action space.arXiv preprint arXiv:1906.06832, 2019. Daniel Reker, Emily A Hoyt, Gonçalo JL Bernardes, and Tiago Rodrigues. Adaptive optimization of chemical reactions with minimal experimental information.Cell Reports Physical Science, 1(11), 2020. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, et al. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783. Greg Landrum et al. Rdkit: Open-source cheminformatics, 2019. URL https://www.rdkit.org/ . Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. InThe Tenth Interna- tional Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URLhttps://openreview.net/forum?id=nZeVKeeFYf9. Xinlong Li, Weijieying Ren, Wei Qin, Lei Wang, Tianxiang Zhao, and Richang Hong. Analyzing and reducing catastrophic forgetting in parameter efficient tuning. InICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1–5, 2025. doi: 10.1109/ICASSP49660.2025.10889361. Jason Y Wang, Jason M Stevens, Stavros K Kariofillis, Mai-Jan Tom, Dung L Golden, Jun Li, Jose E Tabora, Marvin Parasram, Benjamin J Shields, David N Primer, et al. Identifying general reaction conditions by bandit optimization.Nature, 626(8001):1025–1033, 2024b. 11 Damith Perera, Joseph W Tucker, Shalini Brahmbhatt, Christopher J Helal, Ashley Chong, William Farrell, Paul Richardson, and Neal W Sach. A platform for automated nanomole-scale reaction screening and micromole-scale synthesis in flow.Science, 359(6374):429–434, 2018. Derek T Ahneman, Jesús G Estrada, Shishi Lin, Spencer D Dreher, and Abigail G Doyle. Predicting reaction performance in c–n cross-coupling using machine learning.Science, 360(6385):186–190, 2018. Jia Qiu, Jiancong Xie, Shimin Su, Yadong Gao, Han Meng, Yuedong Yang, and Kuangbiao Liao. Se- lective functionalization of hindered meta-c–h bond of o-alkylaryl ketones promoted by automation and deep learning.Chem, 8(12):3275–3287, 2022. Andrew F Zahrt, Jeremy J Henle, Brennan T Rose, Yang Wang, William T Darrow, and Scott E Denmark. Prediction of higher-selectivity catalysts by computer-driven workflow and machine learning.Science, 363(6424):eaau5631, 2019. Connor J. Taylor, Alexander Pomberger, Kobi C. Felton, Rachel Grainger, Magda Barecka, Thomas W. Chamberlain, Richard A. Bourne, Christopher N. Johnson, and Alexei A. Lapkin. A brief introduc- tion to chemical reaction optimization.Chemical Reviews, 123(6):3089–3126, 2023. OpenAI et al. Gpt-4o system card, 2024. URLhttps://arxiv.org/abs/2410.21276. David Eriksson, Michael Pearce, Jacob Gardner, Ryan D Turner, and Matthias Poloczek. Scalable global optimization via local",
    "Barecka, Thomas W. Chamberlain, Richard A. Bourne, Christopher N. Johnson, and Alexei A. Lapkin. A brief introduc- tion to chemical reaction optimization.Chemical Reviews, 123(6):3089–3126, 2023. OpenAI et al. Gpt-4o system card, 2024. URLhttps://arxiv.org/abs/2410.21276. David Eriksson, Michael Pearce, Jacob Gardner, Ryan D Turner, and Matthias Poloczek. Scalable global optimization via local bayesian optimization.Advances in neural information processing systems, 32, 2019. 12 A Illustrative Example of LLM Assisted Construction of a Reaction Optimization Tree In the following illustrative example, we demonstrate how an LLM can assist in constructing an optimization tree for the reaction A + B →C in two steps, enabling efficient optimization of reaction conditions (e.g., catalyst, ligand, solvent, base), given that reactants A and B are fixed. In the first step, an LLM is used to infer the possible reaction type for A + B →C. Based on the inferred reaction type and specific optimization objectives (e.g., improving yield or selectivity), relevant scientific literature is retrieved. Literature acquisition can be done through manual downloads or by using publisher-provided APIs (noting that not all APIs are openly accessible). The collected literature is then used to construct a vector database to support the subsequent retrieval process. Using the information from the literature in the vector database, the LLM is queried via analyzing literature to determine the relative importance of different reaction conditions (variables) on the reaction objects, generating a ranked list. For instance, the LLM might determine the order of influence as: Catalyst > Ligand > Solvent > Base. Further queries to the LLM identify the key physicochemical properties within each category that significantly influence the chemical reaction performance. For example, within the ligand category, the LLM may highlight \"steric and electronic effects\" as crucial physicochemical properties. Subsequently, detailed information regarding the key physicochemical properties of each ligand candidate is retrieved from online databases, after which the LLM clusters these ligand candidates into subsets based on similarities in \"steric and electronic effects\". In the second step, the optimization tree is constructed based on the variable importance ranking and clustering results. The first level of the tree corresponds to the most important variable—the catalyst. At the first level, several child nodes can be established, representing different subsets of catalyst candidates clustered by property similarity. The second level of the tree corresponds to the next most important variable—the ligand. Under each catalyst subset node at the first level, additional child nodes branch out, representing various subsets of ligand candidates categorized by their physicochemical properties. This process continues iteratively, layer by layer, incorporating additional variables (e.g., solvent, base) until the complete optimization tree is constructed. B Update on ChemBOMAS During Optimization After receiving the observation feedback on each round of the experiment, ChemBOMAS would update. First, the data module would be retrained with the prior and newly acquired data points, and then infer the unsampled data points to generate pseudo-labels. Second, the optimization tree would recount the visit number and value of each node to refine the identified hot regions. Third, with the updated observations, pseudo-labels, and refined hot regions, the BO module would recommend next-round",
    "newly acquired data points, and then infer the unsampled data points to generate pseudo-labels. Second, the optimization tree would recount the visit number and value of each node to refine the identified hot regions. Third, with the updated observations, pseudo-labels, and refined hot regions, the BO module would recommend next-round reaction conditions, targeting potentially higher object values. C Benchmark Detail This section provides further details on the benchmark datasets used for evaluating ChemBOMAS. These datasets represent common chemical reaction optimization tasks and have been previously utilized in the literature for benchmarking machine learning-driven optimization approaches. The key characteristics of these datasets are summarized in Table 1-3. Dataset Name Total Number Reaction variables Objective Suzuki coupling Perera et al. [2018] 5760 4 (Reactants, Ligand, Base, Solvent) yield Buchwald-Hartwig coupling Ahneman et al. [2018] 3955 4 (Ligand, Base, Additive, Solvent) yield Tandem arylation Qiu et al. [2022] 1032 3 (Reactants, Solvent) yield Enantioselectivity(Catalyst, Reactants,) Zahrt et al. [2019] 1076 3 (Reactants, Base) selectivity Table 1: Datasets for chemical reaction optimization benchmarks. The \"Reaction variables\" column lists the types of discrete chemical choices or continuous variables explored within each dataset. D Wet Experiment Detail Protocol General Procedure for Reaction OptimizationFor the wet experiment involving palladium- catalyzed coupling of boronic esters with aryl chlorides, first, an oven-dried 10 mL Schlenk tube fitted 13 Rules Catalyst Pd Usage 0.002 mmol Ligand P Usage 0.008 mmol Base Usage 0.3 mmol Reactant 1 Usage 0.2 mmol Reactant 2 Usage 0.25 mmol Solvent Usage 2 mL Reaction Time 24 h Table 2: Fixed experimental variables and reagent quantities for the palladium-catalyzed Suzuki coupling reactions. Variable Dimensionality Variable Number Subset Number Division Basis Catalyst Type 5 2 Valence Ligand Type 14 6 Steric and Electronic Effect Base Type 14 3 Basicity Solvent Type 11 3 Polarity Water usage 3 3 Solubility Temperature 3 3 Activation Energy Table 3:Variable Space, Experimental variable space detailing the six process parameters for the ChemBOMAS-guided optimization of the palladium-catalyzed cross-coupling reaction. Each variable’s total number of options, predefined subsets, and division criteria are presented. with a Teflon-coated magnetic stir bar was charged inside an N2-filled glovebox with Pd-catalyst (0.002 mmol), Phosphine ligand (0.008 mmol), and base (0.30 mmol, 1.5 equiv). Then, the tube was sealed with a septum, removed from the glovebox, and placed under a positive flow of N2. The Mixture of organic solvent and water (2 mL) was introduced via a syringe. Next, pinacol boronic ester 2 (Reactant 1, 0.20 mmol, 1 equiv) and Aryl chloride 1 (Reactant 1, 0.25 mmol, 1.25 equiv) were added sequentially by syringe. The tube was capped tightly, placed in a pre-heated aluminum heating block maintained at 80 °C, 100 °C, or 120 °C, and the mixture was stirred (approximately 1500 rpm) for 24 hours. After cooling to room temperature, the mixture was diluted with ethyl acetate (3 mL) and quenched with water (3 mL). Finally, GC yields were determined directly from the crude mixture against the n-dodecane standard. ChemBOMAS ConfigurationSome configurations of ChemBOMAS described in the Experiment Section of the main text were adjusted for the wet experiment",
    "temperature, the mixture was diluted with ethyl acetate (3 mL) and quenched with water (3 mL). Finally, GC yields were determined directly from the crude mixture against the n-dodecane standard. ChemBOMAS ConfigurationSome configurations of ChemBOMAS described in the Experiment Section of the main text were adjusted for the wet experiment task. First, in the Knowledge Module, the additional process parameters (here, water usage and temperature) were divided into multiple subsets automatically by the LLM using RAG, and these subsets were grouped by the similarity of physical properties, which is the same as the category variables. For instance, temperature conditions were categorized into three distinct subsets corresponding to low, intermediate, and high activation energy levels. Moreover, during the Bayesian Optimization (BO), considering the relatively high experimental throughput, multiple acquisition functions (here, EI and UCB) were applied to generate fourteen samples per round. Apart from the aforementioned adjustments, all other configurations within ChemBOMAS remained consistent with those used in the dry-lab experiments. Sample in The Initial RoundThe initial experiment was only designed by Knowledge module due to the lack of prior data. Specifically, after the Knowledge Module partitioned the variables into subsets, a sampling function that can select variables from different subsets evenly was applied to generate fourteen diverse reaction conditions. The generated reaction conditions were then sent to the experiment operators for actual observation, which facilitated providing data to inform the experimental design in the next round. Sample in The Iterated RoundAs illustrated in Section B of the Supplementary Material, after receiving the observation feedback on each round of the wet experiment, all ChemBOMAS modules would update based on the feedback from each round of the wet-lab experiments. Following the update of ChemBOMAS, the BO module would recommend fourteen reaction conditions with potentially higher yields for the subsequent round. 14 E Additional Results Analysis E.1 More Benchmark Results SuzukiBest Value Found (%)0255075100 Iteration RoundInitilize1234 ChemLLMBO Bayesian Optimization LA-MCTS LLAMBOTandemBest Value Found (%)017.53552.570 Iteration Round01234 ChemLLMBO Bayesian Optimization LA-MCTS LLAMBOBuchwaldBest Value Found (%)0255075100 Iteration Round01234 ChemLLMBO Bayesian Optimization LA-MCTS LLAMBOEnantioselectivityBest Value Found (∆∆G) 01234 Iteration Round01234 ChemLLMBO Bayesian Optimization LA-MCTS LLAMBO Figure 4: Experiment results of short iteration optimization on 5 iterations. As described in the main text, for comparison with baseline methods(here, traditional BO, LA-MCTS, LLAMBO), we conducted evaluations on four datasets. The configuration of baseline methods is detailed in Section F of the Supplementary Material. The results are shown in the Figure 4. Given the relatively small search space (see Table 1), even random selection may sample reasonably good variable combinations after numerous iterations. Consequently, we constricted our experiments to five rounds to evaluate the initialization performance of each algorithm during the early optimization stage. E.2 Additional Results of Data Modules’s Prediction Model Figure 5:Comparison of initial candidate points selected by only Data Module versus a random sampling baseline for Bayesian Optimization on Suzuki reaction test instances.Green dashed line: expected maximum from five random samples. Blue dashed line: actual maximum value for the given test combination. Red crosses: five initial sampling points generated by the Data Module-based BO. The figure highlights the",
    "Data Module versus a random sampling baseline for Bayesian Optimization on Suzuki reaction test instances.Green dashed line: expected maximum from five random samples. Blue dashed line: actual maximum value for the given test combination. Red crosses: five initial sampling points generated by the Data Module-based BO. The figure highlights the effectiveness of the Data Module within the ChemBOMAS framework in identifying promising initial points. This study evaluated the effectiveness of a prediction model (the Data Module)-based BO initialization strategy through experiments on the Suzuki reaction dataset (see Table 1). The experiment dataset 15 comprised 15 types of Suzuki reactions within distinct reactant combinations, but all reaction types shared a variable space for reaction conditions. The Suzuki reaction dataset was partitioned into training and test datasets according to the types of reactant combinations: four types of reactant combinations were used to train an auxiliary predictive model to guide BO initialization, while the remaining 11 types of reactions constituted the test set to assess the performance of BO with different initialization strategies. The dataset division strategy was designed to mimic real-world scenarios, specifically predictions for previously unseen reactant combinations. Figure 5 illustrates the comparative initialization performance of our method and random sampling. The experimental results indicate that although the auxiliary predictive model exhibited a relatively low Root Mean Square Error (RMSE), the BO initialized by the predictive model captured the candidate points with high target values across different Suzuki reaction types in most test instances. Furthermore, compared to random sampling initialization (indicated by green lines in the figure), the proposed ChemBOMAS initialization strategy (indicated by red crosses in the figure) consistently provided superior initial candidate points for BO. F Configuration of Baseline Methods Traditional BOWe implemented a traditional BO baseline with the BoTorch framework, using a Gaussian Process (GP) surrogate model with a Matérn kernel, a constant prior, and a qLogEI acquisition function ( η= 0.001). The input of BO, namely the reaction condition variables, was encoded with simple one -hot vectors; we did not employ more elaborate encodings because previous studies report only marginal benefits Taylor et al. [2023], Shields et al. [2021]. Furthermore, the batch size of each iteration, including the initial round, was fixed at 5, and the initial population was generated by random sampling. LLAMBOThe LLAMBO implementation was based on the default configurations provided in the original authors’ publicly available codebase Liu et al. [2024]. A notable deviation from the original study was necessitated by the deprecation of the LLM API used therein. Consequently, we integrated a more recent and advanced LLM API, specifically GPT-4O OpenAI et al. [2024], for all LLM-dependent operations within the LLAMBO framework. LA-MCTSFor LA-MCTS, we utilized the open-source code from the original publication Wang et al. [2020], with hyperparameters adapted for optimal performance on chemical datasets. The algorithm was initialized with 5 randomly selected samples. The Support Vector Machine (SVM) component within LA-MCTS employed a Radial Basis Function (RBF) kernel. The splitting threshold for the MCTS was adjusted according to the dataset size, specifically set to 50 for larger datasets and 20 for smaller ones,",
    "datasets. The algorithm was initialized with 5 randomly selected samples. The Support Vector Machine (SVM) component within LA-MCTS employed a Radial Basis Function (RBF) kernel. The splitting threshold for the MCTS was adjusted according to the dataset size, specifically set to 50 for larger datasets and 20 for smaller ones, to ensure appropriate tree expansion. The exploration constant, Cp, was configured to 10. Sampling was performed using the TuRBO algorithm Eriksson et al. [2019], with its internal hyperparameters retained at their default values as specified in its original implementation. 16 G Detail of the Prompts As outlined in the main text, our methodology leverages large language models to support several crit- ical tasks in reaction optimization, such as analyzing literature, assessing parameter significance, and understanding physicochemical properties to inform the construction of a hierarchical optimization tree. This appendix section presents a detailed overview of the specific prompts designed to guide the LLM in executing these crucial Tasks. G.1 Prompts of Knowledge Module The Knowledge Module, as described in Section A of the Supplementary Material, employs the LLM to systematically analyze chemical literature and physicochemical data. This involves ranking the impact of various reaction parameters and classifying components based on their physicochemical properties. Impact Sort PromptThe Impact Sort prompt is designed to instruct the LLM to evaluate and rank the relative importance of different reaction parameters (e.g., reactant, catalyst, ligand, solvent, base) on the outcome of a specific target reaction. The parameters ranking is derived from the LLM’s synthesis of information from relevant chemical literature and its general chemical knowledge. An example of the prompt used to elicit the parameter significance ranking is shown below: Prompt for Parameter Impact Ranking Based on Provided Literature.Objective: Analyze the provided scientific literature excerpts related to the Suzuki-Miyaura cross-coupling reaction. Your task is to rank the following reaction parameters in descending order of their reported impact or significance on the yield. Crucial Instruction:Your analysis, conclusions, and the resulting rankingMUST be based solely and exclusively on the information presented within the provided literature excerpts below. Do not use any external knowledge, pre-existing understanding, or general chemical intuition you might possess. Focus strictly on what the given texts state or imply regarding the influence of each parameter. Reaction Parameters to Rank: [Type of Solvent] [Type of Base] ... Provided Literature: [LITERATURE_1] [LITERATURE_2] ... While toolkits exist that allow LLMs to actively retrieve literature from sources such as arXiv and Google Scholar, thereby enabling automated construction of a vector library for retrieval-augmented generation (RAG), these automated retrieval approaches face significant limitations. Many publishers impose paywalls or lack accessible APIs, and automated retrieval poses potential risks of data leakage during the evaluation process. Therefore, we opted to manually curate and provide the literature to ensure data integrity and rationality. Variable Candidates Classification PromptThe prompt guides the LLM to identify key physico- chemical properties of each variable and cluster variable candidates based on their similarity in the physicochemical properties. Below is an example of the prompt for variable candidates classification. Prompt for Variable Candidates Classification Based on Physicochemical DataObjective: Classify the provided",
    "Classification PromptThe prompt guides the LLM to identify key physico- chemical properties of each variable and cluster variable candidates based on their similarity in the physicochemical properties. Below is an example of the prompt for variable candidates classification. Prompt for Variable Candidates Classification Based on Physicochemical DataObjective: Classify the provided list of candidate chemical substances according to the [Speci- fied_physicochemical_Properties]. Your primary method for classification must be the utilization 17 of quantitative data that would typically be found in a comprehensive physicochemical property database. Crucial Instructions: Prioritize Quantitative Data:For each substance and property, you should first attempt to classify it based on specific, measurable, quantitative values (e.g., pKa for basicity/acidity, dielectric constant for polarity, boiling point for volatility, specific functional group counts). Minimize General Knowledge/Intuition:Avoid relying on your general, unquantified chemical knowledge or intuition. If a quantitative value from the \"database\" directly supports a classification, state that. If a direct value isn’t typically used for a category but strong structural indicators (which could be quantified, e.g., number of H-bond donors) point to it, explain this as an inference based on data-like principles. Adhere to Provided Categories:Classify substances strictly into the categories provided for each property. If a substance doesn’t clearly fit or straddles categories based on (assumed) data, note this ambiguity. Candidate Substances to Classify: [CANDIDATE_SUBSTANCES_LIST] Available Tools: [GoogleScholarToolkit], [ArxivToolkit], [PubMedToolkit], [PubChemToolkit], [Wikipedi- aToolkit] G.2 Prompt of Data Module As detailed in the main text, our pre-training phase employs a conditional prediction task. Given the reactants and products, the model’s objective is to predict the corresponding reaction conditions. This process utilizes a Causal Language Modeling (CLM) loss, where the model learns to predict the next token in the sequence of reaction conditions. To provide concrete examples of the input format for this task, this appendix section (Section G.2) presents a selection of prompts utilized during the pre-training phase. These prompts typically consist of the reactants, products, and the target reaction condition sequence that the model is trained to predict. Furthermore, in line with the methodology described in the main text, these input sequences are augmented with functional group annotations (generated via RDKit) to enhance the model’s chemical awareness; the augmentation of the prompt is also reflected in the examples provided below. Prompt of Condition Prediction PretrainingFor the condition prediction pre-training, the input prompts are structured to provide the model with comprehensive reaction information. Typically, a prompt is formatted as: [Reactants_SMILES]; [Products_SMILES]; [Reaction Type];[Target_Reaction_Conditions]. Prior to constructing these prompts, the SMILES strings for both reactants and products are canonicalized using RDKit. This normalization step ensures a stan- dardized and consistent representation of molecular structures, which is vital for robust model training. The model then processes this complete sequence, aiming to predict the [Target_Reaction_Conditions] segment token by token, guided by the Causal Language Modeling objective and conditioned on the preceding reaction type, reactants, and products. To further clarify the input structure for this prediction task, the following examples demonstrate the format used: title=Condition Prediction Pre-training Prompts \"reaction\": \"Here is a chemical reaction. Reactants are: C1=CC=CC=2C3=CC=CC=C3N(C12)CC#C,BrC#CCCCCO. Product is: C1=CC=CC=2C3=CC=CC=C3N(C12)CC#CC#CCCCCO. Reaction type is Cadiot-Chodkiewicz coupling.\", \"condition\":",
    "objective and conditioned on the preceding reaction type, reactants, and products. To further clarify the input structure for this prediction task, the following examples demonstrate the format used: title=Condition Prediction Pre-training Prompts \"reaction\": \"Here is a chemical reaction. Reactants are: C1=CC=CC=2C3=CC=CC=C3N(C12)CC#C,BrC#CCCCCO. Product is: C1=CC=CC=2C3=CC=CC=C3N(C12)CC#CC#CCCCCO. Reaction type is Cadiot-Chodkiewicz coupling.\", \"condition\": \"The reaction conditions of this reaction are: 18 Solvent: O,CN(C=O)C,CN(C=O)C. Catalyst: Cl[Cu]. Atmosphere: N#N. Additive: C(C)N,[Na]Cl,Cl.NO. \", \"reaction_type\": \"Cadiot-Chodkiewicz coupling\", Prompt of Functional Groups InformationWe utilize RDKit to extract functional group informa- tion from the SMILES representation of the reactants. This information includes the type of functional groups present in each reagent and their transformations during the reaction. By incorporating this data, we enhance the input to the LLM, providing it with richer chemical context. Below is an example of the functional group information from the reactants in the Suzuki coupling dataset to augment the prompt. title=An Example of functional group information Functional groups information: Reactant: [CCc1cccc(CC)c1.Clc1ccc2ncccc2c1]. Functional Group: Chlorine([R]Cl). Count: 1. Reactant: [Cc1ccc2c(cnn2C2CCCCO2)c1B(O)O]. Functional Group: Alcohol([R]O). Count: 2. Reactant: [Cc1ccc2c(cnn2C2CCCCO2)c1B(O)O]. Functional Group: Ether([R]O[R]). Count: 1. Reactant: [Cc1ccc2c(cnn2C2CCCCO2)c1B(O)O]. Functional Group: BoricAcid([R]B(O)O). Count: 1. Product: [Cc1ccc2c(cnn2C2CCCCO2)c1-c1ccc2ncccc2c1]. Functional Group: Alcohol([R]O). Count: 2. Product: [Cc1ccc2c(cnn2C2CCCCO2)c1-c1ccc2ncccc2c1]. Functional Group: Ether([R]O[R]). Count: 1. New Functional Groups: None. Lost Functional Groups: Functional Group: Chlorine([R]Cl). Count: 1. Lost: 1. Functional Group: BoricAcid([R]B(O)O). Count: 1. Lost: 1. H Example of Knowledge Module Results H.1 Example of Impact Sort Result This section presents an \"Impact Sort\" result generated by an LLM. The ranking task given to the LLM was to analyze provided scientific literature excerpts concerning the Suzuki-Miyaura cross-coupling reaction. Based solely on these excerpts, the LLM was asked to rank reaction variables in descending order of their reported influence or significance on the reaction yield. The LLM’s response, detailing the ranking and justifications, is as follows:Ranking of Reaction Variables by Impact on Yield (Most to Least Impactful): 1. Catalyst 2. Ligand 3. Base 4. Solvent 5. Temperature Justification Based on Provided Literature Excerpts: 1.Ligand:The provided texts place enormous emphasis on the role of the ligand in determining reaction success and yield. ... 2.Catalyst:The catalyst, typically a palladium species, is fundamental to the reaction. Its nature, concentration, and activity are repeatedly linked to the reaction outcome. ... H.2 Example of Variable Candidates Classification This section provides an example of the classification of various base candidates conducted by an LLM. The clustering task was to classify a list of candidate bases according to their pH in solution, based on provided literature information and pKa values. The LLM’s response, detailing the classification and justifications, is as follows: Summary of Classifications based on pH: 1. NaOCH 3Basic 19 2. t-BuOK: Strongly Basic 3. Li 2CO 3: Basic 4. KOAc: Basic 5. CsOAc: Basic 6. NaOPh: Basic 7. NaOTf: Neutral ... Justification of some substance NaOCH 3(Sodium methoxide): The conjugate acid of methoxide ( OCH− 3) is methanol ( CH 3OH). The pKa of methanol is approximately 15.5. Since methanol is a very weak acid, its conjugate base, methoxide, is a strong base. Therefore, NaOCH 3in solution will be Basic. t-BuOK (Potassium tert-butoxide): The conjugate acid",
    "3(Sodium methoxide): The conjugate acid of methoxide ( OCH− 3) is methanol ( CH 3OH). The pKa of methanol is approximately 15.5. Since methanol is a very weak acid, its conjugate base, methoxide, is a strong base. Therefore, NaOCH 3in solution will be Basic. t-BuOK (Potassium tert-butoxide): The conjugate acid of tert-butoxide ( t-BuO−) is tert-butanol (t-BuOH). The pKa of tert-butanol is approximately 16.54 to 19. As tert-butanol is a very weak acid, its conjugate base, tert-butoxide, is a very strong base. Thus, t-BuOK in solution will be Strongly Basic. ... H.3 Example of Search Space Decomposition Result Here, we present an example of the search space decomposition on the Buchwald-Hartwig coupling dataset. This search space decomposition is generated by the Knowledge Mod- ule. As specifically emphasized in the prompt, its generation process relies entirely on knowledge sourced from the literature. For brevity, only a portion of the decomposition is displayed. An example of decomposition \"Buchwald\": \"Solvent\": \"CS(C)=O\": \"iupac\": \"methylsulfinylmethane\", \"cid\": 679, \"subset\": 0 , \"Base\": \"CN1CCCN2CCCN=C12\": \"iupac\": \"1-methyl-2,3,4,6,7,8-hexahydropyrimido[1,2-a]pyrimidine\", \"cid\": 123583, \"subset\": 0 , \"CN(C)C(=NC(C)(C)C)N(C)C\": \"iupac\": \"2-tert-butyl-1,1,3,3-tetramethylguanidine\", \"cid\": 3571581, \"subset\": 0 , \"CCN=P(N=P(N(C)C)(N(C)C)N(C)C)(N(C)C)N(C)C\": \"iu- pac\": \"N-[dimethylamino-ethylimino-[[tris(dimethylamino)-lambda5-phosphanylidene]amino]- lambda5-phosphanyl]-N-methylmethanamine\", \"cid\": 3393106, \"subset\": 1 , \"Ligand\": \"CC(C)c1cc(C(C)C)c(-c2ccccc2P(C(C)(C)C)C(C)(C)C)c(C(C)C)c1\": \"iupac\": \"ditert- butyl-[2-[2,4,6-tri(propan-2-yl)phenyl]phenyl]phosphane\", \"cid\": 11618717, \"subset\": 0 , \"CC(C)c1cc(C(C)C)c(-c2ccccc2P(C2CCCCC2)C2CCCCC2)c(C(C)C)c1\": \"iupac\": \"dicyclohexyl-[2-[2,4,6-tri(propan-2-yl)phenyl]phenyl]phosphane\", \"cid\": 11155794, \"subset\": 1 , \"COc1ccc(OC)c(P([C@]23C[C@H]4C[C@H](C[C@H](C4)C2)C3)[C@]23C[C@H]4C[C@H](C[C@H](C4)C2)C3)c1- c1c(C(C)C)cc(C(C)C)cc1C(C)C\": \"iupac\": \"bis(1-adamantyl)-[3,6-dimethoxy-2- [2,4,6-tri(propan-2-yl)phenyl]phenyl]phosphane\", \"cid\": 60144828, \"subset\": 0 , \"COc1ccc(OC)c(P(C(C)(C)C)C(C)(C)C)c1-c1c(C(C)C)cc(C(C)C)cc1C(C)C\": \"iupac\": \"ditert- butyl-[3,6-dimethoxy-2-[2,4,6-tri(propan-2-yl)phenyl]phenyl]phosphane\", \"cid\": 44233348, \"subset\": 0 20"
  ]
}